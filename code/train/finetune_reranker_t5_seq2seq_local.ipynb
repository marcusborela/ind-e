{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcN_5-RDWeqV"
      },
      "source": [
        "# Finetuning Ranker"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gratidão aos colegas Thiago Soares Laitz e Hugo (hugo@maritaca.ai) pelo apoio e código base fornecidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPNvc27A6WFj"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAoPEGKQpLAN",
        "outputId": "aee616ba-17b0-4ca3-b79c-8913b6135b62"
      },
      "source": [
        "pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWn-7w2WstVF",
        "outputId": "187954d7-71a0-4c8b-c23a-0351405d8984"
      },
      "source": [
        "pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XEO_Or8ul18",
        "outputId": "b3bbe372-e92c-4e18-ea3f-7ccb4571bed6"
      },
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umVQVnSBxnKT",
        "outputId": "f8629fba-4010-42c4-ceda-826280815d6c"
      },
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lld08iM0uzuL",
        "outputId": "3546aa0a-17d8-4211-8781-119438858ad7"
      },
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neptune rastro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import neptune.new as neptune "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0.16.18'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "neptune.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['NEPTUNE_ALLOW_SELF_SIGNED_CERTIFICATE'] = 'TRUE'\n",
        "os.environ['NEPTUNE_PROJECT'] = 'marcusborela/IA386DD'\n",
        "os.environ['NEPTUNE_API_TOKEN'] = getpass.getpass('Informe NEPTUNE_API_TOKEN')\n",
        "\n",
        "tag_contexto_rastro = 'INDIR_PTT5'\n",
        "neptune_version = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def converte_optimizer_state_dict(parm_optimizer)-> dict:\n",
        "  \"\"\"\n",
        "    Recebe um objeto \"parm_optimizer\" que é do tipo \"torch.optim.Optimizer\" e retorna um dicionário \n",
        "    com informações sobre o otimizador.\n",
        "\n",
        "    O dicionário de retorno é gerado a partir do estado do otimizador que é extraído da propriedade\n",
        "    \"state_dict()\" do objeto \"parm_optimizer\", seu primeiro grupo de parâmetros do otimizador.\n",
        "  \"\"\"\n",
        "  # return str(hparam['optimizer'])\n",
        "  return parm_optimizer.state_dict()['param_groups'][0]\n",
        "if neptune_version == 0:\n",
        "  import neptune.new as neptune  \n",
        "  class NeptuneRastroRun():\n",
        "      se_geracao_rastro = True \n",
        "      neptune_project = \"\"\n",
        "      tag_contexto_rastro = \"\"\n",
        "      neptune_api_token = \"\"\n",
        "\n",
        "      def __init__(self, parm_params:dict,  parm_lista_tag:list = None):\n",
        "        # print(f\"NeptuneRastroRun.init: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_params `{parm_params} \")\n",
        "        if self.__class__.se_geracao_rastro:      \n",
        "          self.run_neptune = neptune.init(project=self.__class__.neptune_project, api_token=self.__class__.neptune_api_token, capture_hardware_metrics=True)\n",
        "          self.run_neptune['sys/name'] = self.__class__.tag_contexto_rastro\n",
        "          vparams = copy.deepcopy(parm_params)\n",
        "          if \"optimizer\" in vparams:\n",
        "            vparams[\"optimizer\"] = converte_optimizer_state_dict(vparams[\"optimizer\"])\n",
        "          if 'criterion'  in vparams:\n",
        "            vparams[\"criterion\"] = str(vparams[\"criterion\"])\n",
        "          if 'scheduler'  in vparams:\n",
        "            vparams[\"scheduler\"] = str(type(vparams[\"scheduler\"]))\n",
        "          if 'device' in vparams:\n",
        "            vparams['device'] = str(vparams[\"device\"])\n",
        "          self.device = vparams[\"device\"]\n",
        "          for tag in parm_lista_tag:\n",
        "            self.run_neptune['sys/tags'].add(tag)\n",
        "          self.run_neptune['parameters'] = vparams\n",
        "          self.tmpDir = tempfile.mkdtemp()\n",
        "\n",
        "      @property\n",
        "      def run():\n",
        "        return self.run_neptune\n",
        "\n",
        "      @classmethod\n",
        "      def ativa_geracao_rastro(cls):\n",
        "        cls.se_geracao_rastro = True      \n",
        "\n",
        "      @classmethod\n",
        "      def def_contexto(cls):\n",
        "        cls.se_geracao_rastro = True      \n",
        "\n",
        "      @classmethod\n",
        "      def desativa_geracao_rastro(cls):\n",
        "        cls.se_geracao_rastro = False      \n",
        "\n",
        "      @classmethod\n",
        "      def retorna_status_geracao_rastro(cls):\n",
        "        return cls.se_geracao_rastro      \n",
        "\n",
        "      @classmethod\n",
        "      def retorna_tag_contexto_rastro(cls):\n",
        "        return cls.tag_contexto_rastro \n",
        "\n",
        "      @classmethod\n",
        "      def inicia_contexto(cls, neptune_project, tag_contexto_rastro, neptune_api_token):\n",
        "        assert '.' not in tag_contexto_rastro, \"NeptuneRastroRun.init(): tag_contexto_rastro não pode possuir ponto, pois será usado para gravar nome de arquivo\"      \n",
        "        cls.neptune_api_token = neptune_api_token\n",
        "        cls.tag_contexto_rastro = tag_contexto_rastro\n",
        "        cls.neptune_project = neptune_project\n",
        "\n",
        "      def salva_metrica(self, parm_metricas={}):\n",
        "        #print(f\"NeptuneRastroRun.salva_metrica: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_metricas:{parm_metricas} \")\n",
        "        if self.__class__.se_geracao_rastro:\n",
        "          for metrica, valor in parm_metricas.items(): \n",
        "            self.run_neptune[metrica].log(valor)\n",
        "  \n",
        "      def gera_grafico_modelo(self, loader_train, model):\n",
        "        if self.__class__.se_geracao_rastro: \n",
        "          # efetuar um forward \n",
        "          \"\"\"\n",
        "          se dataloader devolver x e y:\n",
        "          \"\"\"\n",
        "          x_, y_ = next(iter(loader_train))\n",
        "          x_ = x_.to(self.device)\n",
        "          outputs = model(x_)\n",
        "          \"\"\"\n",
        "          # se dataloader devolver dict:\n",
        "          dados_ = next(iter(loader_train))\n",
        "          outputs = model(dados_['x'].to(self.device))\n",
        "          #outputs = model(x_['input_ids'].to(self.device), x_['attention_mask'].to(self.device))\n",
        "          \"\"\"\n",
        "          nome_arquivo = os.path.join(self.tmpDir, \"modelo \"+ self.__class__.tag_contexto_rastro + time.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
        "          make_dot(outputs, params=dict(model.named_parameters()), show_attrs=True, show_saved=True).render(nome_arquivo, format=\"png\")\n",
        "          self.run_neptune[\"parameters/model_graph\"].upload(nome_arquivo+'.png')\n",
        "          self.run_neptune['parameters/model'] = re.sub('<bound method Module.state_dict of ', '',str(model.state_dict))      \n",
        "\n",
        "\n",
        "\n",
        "      def stop(self):\n",
        "        if self.__class__.se_geracao_rastro:         \n",
        "          self.run_neptune.stop()\n",
        "\n",
        "if neptune_version == 1:\n",
        "  import neptune\n",
        "  class NeptuneRastroRun():\n",
        "      \"\"\"\n",
        "        Classe para geração de rastro de experimento utilizando a ferramenta Neptune.\n",
        "\n",
        "        Busca implementar o rastro proposto em [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664),\n",
        "        autores Marcus Vinícius Borela de Castro e Remis Balaniuk, com o apoio da [solução Neptune](https://app.neptune.ai/)\n",
        "\n",
        "        Attributes:\n",
        "        -----------\n",
        "        se_geracao_rastro : bool\n",
        "            Indica se deve ser gerado rastro de experimento. \n",
        "        neptune_project : str\n",
        "            Nome do projeto criado no Neptune. \n",
        "        tag_contexto_rastro : str\n",
        "            Nome da tag utilizada para identificar o experimento.\n",
        "        neptune_api_token : str\n",
        "            Token utilizado para autenticação na API do Neptune. \n",
        "        run_neptune : object\n",
        "            Objeto que representa o experimento no Neptune.\n",
        "        device : str\n",
        "            Dispositivo utilizado para o treinamento do modelo.\n",
        "        tmpDir : str\n",
        "          Diretório temporário utilizado para salvar gráfico do modelo.          \n",
        "      \"\"\"\n",
        "      se_geracao_rastro = True \n",
        "      neptune_project = \"\"\n",
        "      tag_contexto_rastro = \"\"\n",
        "      neptune_api_token = \"\"\n",
        "\n",
        "      def __init__(self, parm_params:dict,  parm_lista_tag:list = None):\n",
        "        \"\"\"\n",
        "          Método construtor da classe NeptuneRastroRun.\n",
        "          \n",
        "          Args:\n",
        "          - parm_params: dicionário contendo os parâmetros do modelo.\n",
        "          - parm_lista_tag: lista contendo tags adicionais para o experimento.\n",
        "        \"\"\"      \n",
        "        # print(f\"NeptuneRastroRun.init: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_params `{parm_params} \")\n",
        "        if self.__class__.se_geracao_rastro:      \n",
        "          self.run_neptune = neptune.init_run(project=self.__class__.neptune_project, api_token=self.__class__.neptune_api_token, capture_hardware_metrics=True)\n",
        "          self.run_neptune['sys/name'] = self.__class__.tag_contexto_rastro\n",
        "          vparams = copy.deepcopy(parm_params)\n",
        "          if \"optimizer\" in vparams:\n",
        "            vparams[\"optimizer\"] = converte_optimizer_state_dict(vparams[\"optimizer\"])\n",
        "          if 'criterion'  in vparams:\n",
        "            vparams[\"criterion\"] = str(vparams[\"criterion\"])\n",
        "          if 'scheduler'  in vparams:\n",
        "            vparams[\"scheduler\"] = str(type(vparams[\"scheduler\"]))\n",
        "          if 'device' in vparams:\n",
        "            vparams['device'] = str(vparams[\"device\"])\n",
        "          self.device = vparams[\"device\"]\n",
        "          for tag in parm_lista_tag:\n",
        "            self.run_neptune['sys/tags'].add(tag)\n",
        "          self.run_neptune['parameters'] = vparams\n",
        "          # self.tmpDir = tempfile.mkdtemp()\n",
        "\n",
        "      @property\n",
        "      def run():\n",
        "        \"\"\"\n",
        "        Retorna a instância do objeto run_neptune.\n",
        "        \"\"\"      \n",
        "        return self.run_neptune\n",
        "\n",
        "      @classmethod\n",
        "      def ativa_geracao_rastro(cls):\n",
        "        \"\"\"\n",
        "        Ativa a geração de rastro.\n",
        "        \"\"\"      \n",
        "        cls.se_geracao_rastro = True      \n",
        "\n",
        "      @classmethod\n",
        "      def def_contexto(cls):\n",
        "        \"\"\"\n",
        "        Define o contexto para a geração de rastro.\n",
        "        \"\"\"      \n",
        "        cls.se_geracao_rastro = True      \n",
        "\n",
        "      @classmethod\n",
        "      def desativa_geracao_rastro(cls):\n",
        "        \"\"\"\n",
        "        Desativa a geração de rastro.\n",
        "        \"\"\"      \n",
        "        cls.se_geracao_rastro = False      \n",
        "\n",
        "      @classmethod\n",
        "      def retorna_status_geracao_rastro(cls):\n",
        "        \"\"\"\n",
        "          Retorna o status da geração de rastro.\n",
        "          \n",
        "          Returns:\n",
        "          - True se a geração de rastro está ativada, False caso contrário.\n",
        "        \"\"\"      \n",
        "        return cls.se_geracao_rastro      \n",
        "\n",
        "      @classmethod\n",
        "      def retorna_tag_contexto_rastro(cls):\n",
        "        \"\"\"\n",
        "          Retorna a tag do contexto de rastro.\n",
        "        \"\"\"      \n",
        "        return cls.tag_contexto_rastro \n",
        "\n",
        "      @classmethod\n",
        "      def inicia_contexto(cls, neptune_project, tag_contexto_rastro, neptune_api_token):\n",
        "        \"\"\"\n",
        "        Inicia o contexto de execução no Neptune.\n",
        "\n",
        "        Args:\n",
        "            neptune_project (str): Nome do projeto no Neptune.\n",
        "            tag_contexto_rastro (str): Tag que identifica o contexto de execução no Neptune.\n",
        "            neptune_api_token (str): Token de acesso à API do Neptune.\n",
        "\n",
        "        Raises:\n",
        "            AssertionError: Caso a tag_contexto_rastro possua um ponto (.), \n",
        "              o que pode gerar erros na gravação de arquivo.\n",
        "        \"\"\"      \n",
        "        assert '.' not in tag_contexto_rastro, \"NeptuneRastroRun.init(): tag_contexto_rastro não pode possuir ponto, pois será usado para gravar nome de arquivo\"      \n",
        "        cls.neptune_api_token = neptune_api_token\n",
        "        cls.tag_contexto_rastro = tag_contexto_rastro\n",
        "        cls.neptune_project = neptune_project\n",
        "\n",
        "      def salva_metrica(self, parm_metricas={}):\n",
        "        \"\"\"\n",
        "          Salva as métricas no Neptune Run caso a geração de rastro esteja ativa.\n",
        "\n",
        "          Parameters\n",
        "          ----------\n",
        "          parm_metricas: dict\n",
        "              Dicionário contendo as métricas a serem salvas. As chaves devem ser os nomes das métricas e os valores devem ser\n",
        "              os valores das métricas.\n",
        "        \"\"\"\n",
        "        #print(f\"NeptuneRastroRun.salva_metrica: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_metricas:{parm_metricas} \")\n",
        "        if self.__class__.se_geracao_rastro:\n",
        "          for metrica, valor in parm_metricas.items(): \n",
        "            self.run_neptune[metrica].append(valor)\n",
        "  \n",
        "      def gera_grafico_modelo(self, loader_train, model):\n",
        "        \"\"\"\n",
        "          Gera um gráfico do modelo e o envia para o Neptune. \n",
        "          Para gerar o gráfico, um forward pass é realizado em um batch de exemplos \n",
        "          de treino e o resultado é renderizado como um gráfico de nós conectados. \n",
        "          O gráfico é salvo em um arquivo .png e enviado para o Neptune como um arquivo anexo.\n",
        "\n",
        "          Args:\n",
        "              loader_train (torch.utils.data.DataLoader): DataLoader do conjunto de treinamento.\n",
        "              model (torch.nn.Module): Modelo a ser visualizado.\n",
        "          \n",
        "          Pendente:\n",
        "            Evolui para usar from io import StringIO (buffer = io.StringIO()) ao invés de tempdir \n",
        "        \"\"\"    \n",
        "        return\n",
        "\n",
        "        \"\"\"\n",
        "        falta ajustar make_dot\n",
        "        if self.__class__.se_geracao_rastro: \n",
        "          # efetuar um forward \n",
        "          batch = next(iter(loader_train))\n",
        "          # falta generalizar linha abaixo. Criar função que recebe modelo e batch como parâmetro?\n",
        "          outputs = model(input_ids=batch['input_ids'].to(hparam['device']), attention_mask=batch['attention_mask'].to(hparam['device']), token_type_ids=batch['token_type_ids'].to(hparam['device']), labels=batch['labels'].to(hparam['device']))\n",
        "          nome_arquivo = os.path.join(self.tmpDir, \"modelo \"+ self.__class__.tag_contexto_rastro + time.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
        "          make_dot(outputs, params=dict(model.named_parameters()), show_attrs=True, show_saved=True).render(nome_arquivo, format=\"png\")\n",
        "          self.run_neptune[\"parameters/model_graph\"].upload(nome_arquivo+'.png')\n",
        "          self.run_neptune['parameters/model'] = re.sub('<bound method Module.state_dict of ', '',str(model.state_dict))      \n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "      def stop(self):\n",
        "        \"\"\"\n",
        "          Para a execução do objeto Neptune. Todos os experimentos do Neptune são sincronizados com o servidor, e nenhum outro \n",
        "          experimento poderá ser adicionado a este objeto após a chamada a este método.\n",
        "        \"\"\"\n",
        "        if self.__class__.se_geracao_rastro:         \n",
        "          self.run_neptune.stop()\n",
        "\n",
        "### Definindo parâmetros para o rastro\n",
        "\n",
        "\n",
        "NeptuneRastroRun.inicia_contexto(os.environ['NEPTUNE_PROJECT'], tag_contexto_rastro,  os.environ['NEPTUNE_API_TOKEN'])\n",
        "#NeptuneRastroRun.desativa_geracao_rastro()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers.integrations import NeptuneCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhMngMsq6ZIV"
      },
      "source": [
        "# Infra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD6YYInN6G1u"
      },
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hUm-kyDBEHEL"
      },
      "outputs": [],
      "source": [
        "PATH_LOCAL_DATA = '../..'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b3f4qXjB6HMm"
      },
      "outputs": [],
      "source": [
        "PATH_TRAIN_MODEL_LOCAL = f\"{PATH_LOCAL_DATA}/model/train/ptt5-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3KdgJPyrEBzr"
      },
      "outputs": [],
      "source": [
        "# path_data = '/content/drive/MyDrive/treinamento/202301_IA368DD/indir/data/train_data_juris_tcu_index_bm25.csv'\n",
        "\n",
        "# PATH_TRAIN_DATA_ZIP = f\"{PATH_LOCAL_DATA}/data/train_data_juris_tcu_index.zip\"\n",
        "PATH_TRAIN_DATA = f\"{PATH_LOCAL_DATA}/data/train_juris_tcu_index/train_data_juris_tcu_index.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrF4PpSvlrcU",
        "outputId": "d385afdd-3dfc-4874-80e0-d397880266fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.exists(PATH_TRAIN_MODEL_LOCAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrF4PpSvlrcU",
        "outputId": "d385afdd-3dfc-4874-80e0-d397880266fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.exists(PATH_TRAIN_DATA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNOEKGtZ55uU"
      },
      "source": [
        "## Função de verificação de memória"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2C0W0RMyx2yu"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_pq59lAHke",
        "outputId": "ce6828a6-8ce4-405f-f534-ebaf820f9d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Jul  5 19:28:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.116.03   Driver Version: 525.116.03   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
            "|  0%   48C    P8    26W / 370W |     58MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1223      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell                9MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9XgIWvkkH-kn"
      },
      "outputs": [],
      "source": [
        "def mostra_memoria(lista_mem=['cpu']):\n",
        "  \"\"\"\n",
        "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
        "\n",
        "  Parâmetros:\n",
        "  -----------\n",
        "  lista_mem : list, opcional\n",
        "      Lista com strings 'cpu' e/ou 'gpu'.\n",
        "      'cpu' - exibe informações de memória da CPU.\n",
        "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
        "      O valor padrão é ['cpu'].\n",
        "\n",
        "  Saída:\n",
        "  -------\n",
        "  A função não retorna nada, apenas exibe as informações na tela.\n",
        "\n",
        "  Exemplo de uso:\n",
        "  ---------------\n",
        "  Para exibir informações de memória da CPU:\n",
        "      mostra_memoria(['cpu'])\n",
        "\n",
        "  Para exibir informações de memória da CPU e GPU:\n",
        "      mostra_memoria(['cpu', 'gpu'])\n",
        "\n",
        "  Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "  \"\"\"\n",
        "  if 'cpu' in lista_mem:\n",
        "    vm = virtual_memory()\n",
        "    ram={}\n",
        "    ram['total']=round(vm.total / 1e9,2)\n",
        "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
        "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
        "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
        "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
        "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
        "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
        "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
        "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
        "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
        "    print('/nGPU')\n",
        "    gpu_info = !nvidia-smi\n",
        "  if 'gpu' in lista_mem:\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dri9iiMAvCT",
        "outputId": "174c2c6f-da21-4936-9fcc-4ecd0151966a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 67.35\n",
            " available 55.33\n",
            " used 10.97\n",
            " free 19.13\n",
            " cached 35.27\n",
            " buffers 1.97\n",
            "/nGPU\n",
            "Wed Jul  5 19:28:13 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.116.03   Driver Version: 525.116.03   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
            "|  0%   49C    P8    37W / 370W |     58MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1223      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell                9MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpPSgRGQ5wJv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aGDjEcJ_bawK"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MV8a69JaiEn9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TfI-DLjRbTzF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CB1vojvvlt0v"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f2b7k0tOuv_L"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZYqAt-NimJH5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    MT5Tokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import Dataset\n",
        "from dataclasses import dataclass, field\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v2gtkEPhA0t"
      },
      "source": [
        "## Preparando para debug e display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wQ5pmlOHxHhk"
      },
      "outputs": [],
      "source": [
        "def config_display():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de display do Pandas.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configurando formato saída Pandas\n",
        "  # define o número máximo de colunas que serão exibidas\n",
        "  pd.options.display.max_columns = None\n",
        "\n",
        "  # define a largura máxima de uma linha\n",
        "  pd.options.display.width = 1000\n",
        "\n",
        "  # define o número máximo de linhas que serão exibidas\n",
        "  pd.options.display.max_rows = 100\n",
        "\n",
        "  # define o número máximo de caracteres por coluna\n",
        "  pd.options.display.max_colwidth = 50\n",
        "\n",
        "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
        "  pd.options.display.show_dimensions = True\n",
        "\n",
        "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
        "  pd.options.display.precision = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "b2tDy72ATNHs"
      },
      "outputs": [],
      "source": [
        "def config_debug():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
        "  transformers e datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define opções de impressão de tensores para o modo científico\n",
        "  torch.set_printoptions(sci_mode=True)\n",
        "  \"\"\"\n",
        "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
        "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345,\n",
        "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores\n",
        "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite\n",
        "    uma melhor compreensão dos números envolvidos.\n",
        "  \"\"\"\n",
        "\n",
        "  # Habilita detecção de anomalias no autograd do PyTorch\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "  \"\"\"\n",
        "    Permite identificar operações que podem causar problemas de estabilidade numérica,\n",
        "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada,\n",
        "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores\n",
        "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch\n",
        "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido\n",
        "    antes que se torne um problema maior.\n",
        "\n",
        "    É importante notar que a detecção de anomalias pode ter um impacto significativo\n",
        "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
        "    ela deve ser usada com cautela e apenas para depuração.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
        "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "  \"\"\"\n",
        "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada.\n",
        "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado\n",
        "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
        "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona,\n",
        "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração\n",
        "    e removê-lo após a solução do problema.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define o nível de verbosity do pacote transformers para info\n",
        "  # transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers\n",
        "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
        "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
        "\n",
        "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa\n",
        "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
        "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
        "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a\n",
        "    necessidade de cada tarefa.\n",
        "\n",
        "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e\n",
        "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
        "\n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_warning()\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Define o modo verbose do xmode, que é utilizado no debug\n",
        "  # %xmode Verbose\n",
        "\n",
        "  \"\"\"\n",
        "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
        "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
        "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais\n",
        "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
        "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
        "\n",
        "    Caso queira desabilitar o modo verbose e utilizar o modo plain,\n",
        "    comentar a linha acima e descomentar a linha abaixo:\n",
        "    %xmode Plain\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "    Dica:\n",
        "    1.  pdb (Python Debugger)\n",
        "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro\n",
        "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
        "\n",
        "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações\n",
        "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug\n",
        "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
        "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
        "\n",
        "\n",
        "    2. ipdb\n",
        "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
        "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
        "\n",
        "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do\n",
        "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha,\n",
        "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute\n",
        "      comandos para investigar o comportamento.\n",
        "\n",
        "      Durante a depuração, você pode usar comandos:\n",
        "        next (para executar a próxima linha de código),\n",
        "        step (para entrar em uma função chamada na próxima linha de código)\n",
        "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
        "\n",
        "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
        "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
        "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Tb4aqtcExR84"
      },
      "outputs": [],
      "source": [
        "config_display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-5Bq4043fkfh"
      },
      "outputs": [],
      "source": [
        "config_debug()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GYGL4MV_yhQ",
        "outputId": "005fc71c-7b47-4acb-baa8-db818de19b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: /home/borela/fontes/ind-ir/code/train\n"
          ]
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "print(\"Current directory:\", current_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPlzXHONq9BC"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fEod2-Fjl6fB"
      },
      "outputs": [],
      "source": [
        "TOKEN_FALSE = '▁não'\n",
        "TOKEN_TRUE = '▁sim'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wc5FqYFQrNrE"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'unicamp-dl/ptt5-base-pt-msmarco-100k-v2'\n",
        "# 'unicamp-dl/mt5-3B-mmarco-en-pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMrTcbJN55ue"
      },
      "source": [
        "# Carga dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "r0igqAcT55ue"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(PATH_TRAIN_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGRYa-JT55ue",
        "outputId": "1c815909-6770-4143-9f85-fcce23e28fe2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(402738, 7)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape\n",
        "# lim 100(111852, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ3S90BBTrKJ"
      },
      "source": [
        "Verificando correção do arquivo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm7ss2BfTlOu",
        "outputId": "4d2c9497-e1c8-4a82-af23-3ba55bc4b79d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUERY_ID      0\n",
            "DOC_ID        0\n",
            "RELEVANCE     0\n",
            "SCORE         0\n",
            "TYPE          0\n",
            "DOC_TEXT      0\n",
            "QUERY_TEXT    0\n",
            "Length: 7, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "kxK8j_na55uh",
        "outputId": "48c933cd-ea22-4df7-c448-a2f3dd194fa4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QUERY_TEXT</th>\n",
              "      <th>DOC_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>402738.0000000</td>\n",
              "      <td>402738.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>322.8252313</td>\n",
              "      <td>830.6957451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>165.8299958</td>\n",
              "      <td>398.1844365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>41.0000000</td>\n",
              "      <td>86.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>217.0000000</td>\n",
              "      <td>572.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>294.0000000</td>\n",
              "      <td>759.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>391.0000000</td>\n",
              "      <td>1020.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4212.0000000</td>\n",
              "      <td>3739.0000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           QUERY_TEXT        DOC_TEXT\n",
              "count  402738.0000000  402738.0000000\n",
              "mean      322.8252313     830.6957451\n",
              "std       165.8299958     398.1844365\n",
              "min        41.0000000      86.0000000\n",
              "25%       217.0000000     572.0000000\n",
              "50%       294.0000000     759.0000000\n",
              "75%       391.0000000    1020.0000000\n",
              "max      4212.0000000    3739.0000000\n",
              "\n",
              "[8 rows x 2 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[['QUERY_TEXT','DOC_TEXT']].applymap(len).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBvMP8ma55uh"
      },
      "source": [
        "Para cada positivo, tem 5 negativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH49RPhY55uh",
        "outputId": "68651bd7-dabd-4cce-dfae-b90cbe863842"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    402738.0000000\n",
              "mean          0.1666667\n",
              "std           0.3726785\n",
              "min           0.0000000\n",
              "25%           0.0000000\n",
              "50%           0.0000000\n",
              "75%           0.0000000\n",
              "max           1.0000000\n",
              "Name: RELEVANCE, Length: 8, dtype: float64"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['RELEVANCE'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "9hx3q_TOy6QB",
        "outputId": "81137dcc-1021-4000-c161-395660ce6a6d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QUERY_ID</th>\n",
              "      <th>DOC_ID</th>\n",
              "      <th>RELEVANCE</th>\n",
              "      <th>SCORE</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>DOC_TEXT</th>\n",
              "      <th>QUERY_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151655</td>\n",
              "      <td>1943</td>\n",
              "      <td>1</td>\n",
              "      <td>0.897</td>\n",
              "      <td>TEMA</td>\n",
              "      <td>O termo é \"Agente público\".\\nAgente público te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>151655</td>\n",
              "      <td>15441</td>\n",
              "      <td>0</td>\n",
              "      <td>0.732</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>151655</td>\n",
              "      <td>6373</td>\n",
              "      <td>0</td>\n",
              "      <td>0.717</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Exercício financeiro anterior\".\\nEx...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151655</td>\n",
              "      <td>6973</td>\n",
              "      <td>0</td>\n",
              "      <td>0.680</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"CJF\".\\nCJF é classificado como uma ...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>151655</td>\n",
              "      <td>7201</td>\n",
              "      <td>0</td>\n",
              "      <td>0.751</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Embratur\".\\nEmbratur é classificado...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   QUERY_ID  DOC_ID  RELEVANCE  SCORE                               TYPE                                           DOC_TEXT                                         QUERY_TEXT\n",
              "0    151655    1943          1  0.897                               TEMA  O termo é \"Agente público\".\\nAgente público te...  O dever de observância à hierarquia militar nã...\n",
              "1    151655   15441          0  0.732  relevant:TEMA, not relevant:TOTAL  O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...  O dever de observância à hierarquia militar nã...\n",
              "2    151655    6373          0  0.717  relevant:TEMA, not relevant:TOTAL  O termo é \"Exercício financeiro anterior\".\\nEx...  O dever de observância à hierarquia militar nã...\n",
              "3    151655    6973          0  0.680  relevant:TEMA, not relevant:TOTAL  O termo é \"CJF\".\\nCJF é classificado como uma ...  O dever de observância à hierarquia militar nã...\n",
              "4    151655    7201          0  0.751  relevant:TEMA, not relevant:TOTAL  O termo é \"Embratur\".\\nEmbratur é classificado...  O dever de observância à hierarquia militar nã...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "wN-H7_d54tPY"
      },
      "outputs": [],
      "source": [
        "df[\"label\"] = [TOKEN_FALSE if relevance == 0 else TOKEN_TRUE for relevance in df[\"RELEVANCE\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "aEHO3ZjL42tA",
        "outputId": "4c00de02-3d7d-4c6d-e585-9bb7d7c9087f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QUERY_ID</th>\n",
              "      <th>DOC_ID</th>\n",
              "      <th>RELEVANCE</th>\n",
              "      <th>SCORE</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>DOC_TEXT</th>\n",
              "      <th>QUERY_TEXT</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151655</td>\n",
              "      <td>1943</td>\n",
              "      <td>1</td>\n",
              "      <td>0.897</td>\n",
              "      <td>TEMA</td>\n",
              "      <td>O termo é \"Agente público\".\\nAgente público te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁sim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>151655</td>\n",
              "      <td>15441</td>\n",
              "      <td>0</td>\n",
              "      <td>0.732</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>151655</td>\n",
              "      <td>6373</td>\n",
              "      <td>0</td>\n",
              "      <td>0.717</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Exercício financeiro anterior\".\\nEx...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151655</td>\n",
              "      <td>6973</td>\n",
              "      <td>0</td>\n",
              "      <td>0.680</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"CJF\".\\nCJF é classificado como uma ...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>151655</td>\n",
              "      <td>7201</td>\n",
              "      <td>0</td>\n",
              "      <td>0.751</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Embratur\".\\nEmbratur é classificado...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   QUERY_ID  DOC_ID  RELEVANCE  SCORE                               TYPE                                           DOC_TEXT                                         QUERY_TEXT label\n",
              "0    151655    1943          1  0.897                               TEMA  O termo é \"Agente público\".\\nAgente público te...  O dever de observância à hierarquia militar nã...  ▁sim\n",
              "1    151655   15441          0  0.732  relevant:TEMA, not relevant:TOTAL  O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "2    151655    6373          0  0.717  relevant:TEMA, not relevant:TOTAL  O termo é \"Exercício financeiro anterior\".\\nEx...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "3    151655    6973          0  0.680  relevant:TEMA, not relevant:TOTAL  O termo é \"CJF\".\\nCJF é classificado como uma ...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "4    151655    7201          0  0.751  relevant:TEMA, not relevant:TOTAL  O termo é \"Embratur\".\\nEmbratur é classificado...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YwZra1Tu2j92"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'DOC_TEXT': 'text', 'QUERY_TEXT':'query'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "L6VTWWEI23LA",
        "outputId": "990a54d7-acbf-4633-e5ea-008dfc668e39"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QUERY_ID</th>\n",
              "      <th>DOC_ID</th>\n",
              "      <th>RELEVANCE</th>\n",
              "      <th>SCORE</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>text</th>\n",
              "      <th>query</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151655</td>\n",
              "      <td>1943</td>\n",
              "      <td>1</td>\n",
              "      <td>0.897</td>\n",
              "      <td>TEMA</td>\n",
              "      <td>O termo é \"Agente público\".\\nAgente público te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁sim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>151655</td>\n",
              "      <td>15441</td>\n",
              "      <td>0</td>\n",
              "      <td>0.732</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>151655</td>\n",
              "      <td>6373</td>\n",
              "      <td>0</td>\n",
              "      <td>0.717</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Exercício financeiro anterior\".\\nEx...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151655</td>\n",
              "      <td>6973</td>\n",
              "      <td>0</td>\n",
              "      <td>0.680</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"CJF\".\\nCJF é classificado como uma ...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>151655</td>\n",
              "      <td>7201</td>\n",
              "      <td>0</td>\n",
              "      <td>0.751</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Embratur\".\\nEmbratur é classificado...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   QUERY_ID  DOC_ID  RELEVANCE  SCORE                               TYPE                                               text                                              query label\n",
              "0    151655    1943          1  0.897                               TEMA  O termo é \"Agente público\".\\nAgente público te...  O dever de observância à hierarquia militar nã...  ▁sim\n",
              "1    151655   15441          0  0.732  relevant:TEMA, not relevant:TOTAL  O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "2    151655    6373          0  0.717  relevant:TEMA, not relevant:TOTAL  O termo é \"Exercício financeiro anterior\".\\nEx...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "3    151655    6973          0  0.680  relevant:TEMA, not relevant:TOTAL  O termo é \"CJF\".\\nCJF é classificado como uma ...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "4    151655    7201          0  0.751  relevant:TEMA, not relevant:TOTAL  O termo é \"Embratur\".\\nEmbratur é classificado...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "byw95g1KzUP7"
      },
      "outputs": [],
      "source": [
        "df = df[['query', 'text', 'label']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF4iu3DMztPX",
        "outputId": "eb038f3c-16b2-4d9f-a137-42f741b02d60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(402738, 3)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itVdwg9tz8Rs",
        "outputId": "632d7cae-e688-4d32-8ae7-1f11933d7df5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "# ... código anterior ...\n",
        "\n",
        "# Liberar memória utilizando gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUDBUOWmyzrf"
      },
      "source": [
        "# Separating evaluation data and prepare dataset tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wH4Tx1cGy3HR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "s7qRU31pzNnT"
      },
      "outputs": [],
      "source": [
        "train_df, valid_df = train_test_split(df, test_size=0.01,\n",
        "                                      stratify=df['label'].values, random_state=123)\n",
        "# Definir os argumentos de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVFFx-bhzNj4",
        "outputId": "d5207702-eb8e-47b8-e5cd-269d0287c773"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((398710, 3), (4028, 3))"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.shape, valid_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "G6kwqr6Cz0X9",
        "outputId": "0c235ee1-cbb2-4b7d-a088-8de7865d7395"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>270246</th>\n",
              "      <td>Verificado sobrepreço em contrato de obra públ...</td>\n",
              "      <td>O termo é \"Fatura\".\\nFatura tem nota de escopo...</td>\n",
              "      <td>▁sim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106266</th>\n",
              "      <td>Faculta-se aos médicos do poder judiciário o e...</td>\n",
              "      <td>O termo é \"Médico\".\\nMédico tem definição: \"Aq...</td>\n",
              "      <td>▁sim</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    query                                               text label\n",
              "270246  Verificado sobrepreço em contrato de obra públ...  O termo é \"Fatura\".\\nFatura tem nota de escopo...  ▁sim\n",
              "106266  Faculta-se aos médicos do poder judiciário o e...  O termo é \"Médico\".\\nMédico tem definição: \"Aq...  ▁sim\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_df[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egkeJ5Lxz7O_",
        "outputId": "ff633ddd-6d46-4158-9fd1-1298b91f9023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array(['▁não', '▁sim'], dtype=object), array([332258,  66452])) \n",
            " (array(['▁não', '▁sim'], dtype=object), array([3357,  671]))\n"
          ]
        }
      ],
      "source": [
        "print(np.unique(train_df['label'], return_counts=True), '\\n', np.unique(valid_df['label'], return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RyDkS1BHz7M3"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df[[\"query\", \"text\", \"label\"]].reset_index(drop=True))\n",
        "valid_dataset = Dataset.from_pandas(valid_df[[\"query\", \"text\", \"label\"]].reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyhn6oozz7JZ",
        "outputId": "05ee54ad-2616-43e2-d86e-d955b9bfdeae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(398710, 4028)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset), len(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBMIhR4-1AsW",
        "outputId": "1d08ec30-04ec-4398-8f22-a0bf86f84686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'Verificado sobrepreço em contrato de obra pública, a Administração deve promover ajuste do valor apurado nas faturas vincendas. Não existindo saldo financeiro, deve providenciar a instauração da competente tomada de contas especial.',\n",
              " 'text': 'O termo é \"Fatura\".\\nFatura tem nota de escopo: \"É um documento onde está registrado um valor em debito com prazo de quitação da dívida seja parcelado ou à vista.\".\\nFatura tem termo relacionado: \"Duplicata\", \"Nota fiscal eletrônica\", \"Nota fiscal\" e \"Cartão de crédito\".\\nFatura tem tradução em espanhol: \"Factura\".\\nFatura tem tradução em inglês: \"Invoice\".',\n",
              " 'label': '▁sim'}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "zZVBeVB_0Onw"
      },
      "outputs": [],
      "source": [
        "del df, train_df, valid_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh-EzCpA0cqZ",
        "outputId": "70239a6d-0ab3-40aa-b7fb-01d96bdfe7a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ-rHK17f_42"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "0ULDoMS7rkXT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)okenizer_config.json: 100%|██████████| 1.98k/1.98k [00:00<00:00, 554kB/s]\n",
            "Downloading spiece.model: 100%|██████████| 756k/756k [00:00<00:00, 793kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 1.00MB/s]\n"
          ]
        }
      ],
      "source": [
        "# tokenizer = MT5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7JUJQQuW7bCA"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    queries_documents = [f\"Query: {query} Document: {text} Relevant:\" for query, text in zip(batch[\"query\"], batch[\"text\"])]\n",
        "    print(f\"Chamado tokenize len(queries_documents): {len(queries_documents)}\")\n",
        "    tokenized = tokenizer(\n",
        "        queries_documents,\n",
        "        padding=True, # \"max_length\",\n",
        "        truncation=True,\n",
        "        # return_tensors=\"pt\",\n",
        "        max_length= 512\n",
        "    )\n",
        "    # tokenized[\"labels\"] = [[label] for label in batch[\"label\"]]\n",
        "    # tokenized['label'] = [[token_false, token_true][int(pairs[\"label\"][i])]\n",
        "    tokenized[\"labels\"] = tokenizer(batch['label'])['input_ids']\n",
        "    # tokenized[\"labels\"] = [tokenizer.get_vocab()[token] for token in batch['label']]\n",
        "    # tokenized[\"labels\"] = [token_id_true if label == 'true' else token_id_false for label in batch[\"label\"]]\n",
        "    return tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108,
          "referenced_widgets": [
            "8363e87c191c4e27a55624d76592522e",
            "9ddd0ce214044ff4985779bc45a78075",
            "6ad3f253125f410bbedb811f3313b312",
            "80f9f8c628fc4eeeaead5bf16c880b94",
            "492c664c2bf84d98bd5372d20ed8f8d2",
            "4b258b1461cb4ebd95674bbe963da89b",
            "4d153ac235b643d9b9fb1f502f220845",
            "614aa774c3b44e02b377967e4ceea596",
            "cc8cb0b3f3ba4174bd7f4e3d5a6f8f46",
            "36a988374e504d9a997e8e48955d6bd2",
            "be972ca97d034c5fb5b616299d8dd29c"
          ]
        },
        "id": "YOaxhRfM3css",
        "outputId": "a061da2f-c993-4a46-92d1-4768017b44b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   0%|          | 0/4028 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  50%|████▉     | 2000/4028 [00:00<00:00, 3247.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 28\n",
            "CPU times: user 3.68 s, sys: 138 ms, total: 3.82 s\n",
            "Wall time: 1.06 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# valid_dataset.set_transform(tokenize)\n",
        "valid_dataset = valid_dataset.map(\n",
        "        tokenize,\n",
        "        remove_columns=('query', 'text', 'label'),\n",
        "        batched=True,\n",
        "        desc='Tokenizing',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7dec4e3e109040308c245ca52fc29e93",
            "4e5ea796f1384a3984a8f52720d3c445",
            "5c546b9f682444be932b17b2ff4c3409",
            "520b60a897d342648199509590bcb3c7",
            "f50b231ed5fa423e81400f13d72f77bf",
            "bc2c9da3baaa4d2b92aaf7a4ab74035e",
            "b849d25c533f4618afa11c027f3feaad",
            "6ce6ab38f4d4400cb7ba03d59d7a7b21",
            "1d1db4f6d75944ddbe4284da885e8038",
            "ee3f86de98ba49fda4fe387569bd85d8",
            "4e6e72fe001744c4830d22150b8e2b84"
          ]
        },
        "id": "S49luVi-5bFG",
        "outputId": "0ded5508-104b-4db0-c781-d9e1ecdd3b20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   0%|          | 1000/398710 [00:00<00:59, 6739.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   1%|          | 3000/398710 [00:00<00:56, 7036.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   1%|▏         | 5000/398710 [00:00<00:57, 6825.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   2%|▏         | 7000/398710 [00:01<00:58, 6728.33 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   2%|▏         | 9000/398710 [00:01<01:03, 6145.39 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   3%|▎         | 11000/398710 [00:01<00:59, 6563.53 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   3%|▎         | 13000/398710 [00:01<00:57, 6707.86 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   4%|▍         | 15000/398710 [00:02<00:55, 6853.75 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   4%|▍         | 17000/398710 [00:02<00:55, 6904.80 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   5%|▍         | 19000/398710 [00:02<00:59, 6405.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   5%|▌         | 21000/398710 [00:03<00:56, 6718.08 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   6%|▌         | 23000/398710 [00:03<00:55, 6810.92 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   6%|▋         | 25000/398710 [00:03<00:54, 6878.63 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   7%|▋         | 27000/398710 [00:04<00:55, 6718.76 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   7%|▋         | 28000/398710 [00:04<00:54, 6801.39 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   8%|▊         | 30000/398710 [00:04<00:58, 6289.07 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   8%|▊         | 32000/398710 [00:04<00:57, 6393.56 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   9%|▊         | 34000/398710 [00:05<00:54, 6724.92 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   9%|▉         | 36000/398710 [00:05<00:53, 6782.30 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  10%|▉         | 38000/398710 [00:05<00:51, 6949.49 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  10%|█         | 40000/398710 [00:05<00:51, 6984.47 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  11%|█         | 42000/398710 [00:06<00:54, 6508.35 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  11%|█         | 44000/398710 [00:06<00:52, 6820.80 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  12%|█▏        | 46000/398710 [00:06<00:50, 7024.47 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  12%|█▏        | 48000/398710 [00:07<00:48, 7164.00 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  13%|█▎        | 50000/398710 [00:07<00:48, 7183.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  13%|█▎        | 52000/398710 [00:07<00:57, 6029.52 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  14%|█▎        | 54000/398710 [00:08<00:53, 6447.34 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  14%|█▍        | 56000/398710 [00:08<00:52, 6548.55 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  15%|█▍        | 58000/398710 [00:08<00:50, 6747.66 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  15%|█▌        | 60000/398710 [00:08<00:49, 6789.01 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  15%|█▌        | 61000/398710 [00:09<00:49, 6754.49 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  16%|█▌        | 63000/398710 [00:09<00:52, 6334.32 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  16%|█▋        | 65000/398710 [00:09<00:49, 6706.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  17%|█▋        | 67000/398710 [00:10<00:47, 6990.35 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  17%|█▋        | 69000/398710 [00:10<00:46, 7072.88 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  18%|█▊        | 71000/398710 [00:10<00:45, 7138.53 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  18%|█▊        | 73000/398710 [00:10<00:45, 7137.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  19%|█▉        | 75000/398710 [00:11<00:49, 6561.75 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  19%|█▉        | 77000/398710 [00:11<00:46, 6846.18 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  20%|█▉        | 79000/398710 [00:11<00:45, 6992.43 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  20%|██        | 81000/398710 [00:12<00:44, 7073.74 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  21%|██        | 83000/398710 [00:12<00:44, 7160.56 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  21%|██▏       | 85000/398710 [00:12<00:48, 6493.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  22%|██▏       | 87000/398710 [00:12<00:45, 6878.81 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  22%|██▏       | 89000/398710 [00:13<00:43, 7084.60 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  23%|██▎       | 91000/398710 [00:13<00:42, 7157.32 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  23%|██▎       | 93000/398710 [00:13<00:42, 7192.81 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  24%|██▍       | 95000/398710 [00:14<00:48, 6286.14 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  24%|██▍       | 97000/398710 [00:14<00:44, 6730.40 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  25%|██▍       | 99000/398710 [00:14<00:43, 6886.14 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  25%|██▌       | 101000/398710 [00:14<00:42, 6988.41 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  26%|██▌       | 103000/398710 [00:15<00:42, 7026.00 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  26%|██▋       | 105000/398710 [00:15<00:46, 6276.02 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  27%|██▋       | 107000/398710 [00:15<00:48, 6023.08 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  27%|██▋       | 109000/398710 [00:16<00:44, 6573.47 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  28%|██▊       | 111000/398710 [00:16<00:41, 6909.89 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  28%|██▊       | 113000/398710 [00:16<00:40, 7103.16 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  29%|██▉       | 115000/398710 [00:17<00:39, 7132.05 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  29%|██▉       | 117000/398710 [00:17<00:44, 6270.08 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  30%|██▉       | 119000/398710 [00:17<00:41, 6755.66 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  30%|███       | 121000/398710 [00:17<00:40, 6876.00 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  31%|███       | 123000/398710 [00:18<00:39, 7002.31 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  31%|███▏      | 125000/398710 [00:18<00:38, 7083.89 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  32%|███▏      | 127000/398710 [00:18<00:38, 7068.15 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  32%|███▏      | 129000/398710 [00:19<00:41, 6423.67 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  33%|███▎      | 131000/398710 [00:19<00:39, 6758.49 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  33%|███▎      | 133000/398710 [00:19<00:39, 6796.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  34%|███▍      | 135000/398710 [00:20<00:37, 6946.71 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  34%|███▍      | 137000/398710 [00:20<00:36, 7090.01 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  35%|███▍      | 139000/398710 [00:20<00:36, 7139.33 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  35%|███▌      | 141000/398710 [00:20<00:39, 6512.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  36%|███▌      | 143000/398710 [00:21<00:37, 6777.61 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  36%|███▋      | 145000/398710 [00:21<00:36, 6978.72 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  37%|███▋      | 147000/398710 [00:21<00:35, 7036.77 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  37%|███▋      | 149000/398710 [00:22<00:35, 7022.14 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  38%|███▊      | 151000/398710 [00:22<00:39, 6319.45 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  38%|███▊      | 153000/398710 [00:22<00:36, 6690.46 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  39%|███▉      | 155000/398710 [00:22<00:35, 6848.72 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  39%|███▉      | 157000/398710 [00:23<00:34, 7064.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  40%|███▉      | 159000/398710 [00:23<00:34, 6962.69 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  40%|████      | 160000/398710 [00:23<00:34, 6971.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  41%|████      | 162000/398710 [00:24<00:37, 6331.65 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  41%|████      | 164000/398710 [00:24<00:34, 6711.11 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  42%|████▏     | 166000/398710 [00:24<00:34, 6829.36 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  42%|████▏     | 168000/398710 [00:24<00:33, 6897.91 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  43%|████▎     | 170000/398710 [00:25<00:32, 7048.96 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  43%|████▎     | 172000/398710 [00:25<00:31, 7087.58 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  44%|████▎     | 174000/398710 [00:25<00:34, 6426.23 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  44%|████▍     | 176000/398710 [00:26<00:35, 6209.75 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  45%|████▍     | 178000/398710 [00:26<00:33, 6575.87 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  45%|████▌     | 180000/398710 [00:26<00:31, 6883.72 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  46%|████▌     | 182000/398710 [00:26<00:30, 7074.63 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  46%|████▌     | 184000/398710 [00:27<00:33, 6488.62 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  47%|████▋     | 186000/398710 [00:27<00:30, 6880.59 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  47%|████▋     | 188000/398710 [00:27<00:29, 7045.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  48%|████▊     | 190000/398710 [00:28<00:30, 6815.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  48%|████▊     | 192000/398710 [00:28<00:29, 6927.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  49%|████▊     | 194000/398710 [00:28<00:32, 6215.09 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  49%|████▉     | 196000/398710 [00:29<00:30, 6744.60 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  50%|████▉     | 198000/398710 [00:29<00:30, 6657.29 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  50%|█████     | 200000/398710 [00:29<00:28, 6902.22 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  51%|█████     | 202000/398710 [00:29<00:28, 6983.05 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  51%|█████     | 204000/398710 [00:30<00:27, 6998.85 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  52%|█████▏    | 206000/398710 [00:30<00:30, 6324.68 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  52%|█████▏    | 207000/398710 [00:30<00:29, 6590.46 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  52%|█████▏    | 209000/398710 [00:31<00:34, 5543.37 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  53%|█████▎    | 211000/398710 [00:31<00:29, 6258.78 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  53%|█████▎    | 213000/398710 [00:31<00:27, 6695.50 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  54%|█████▍    | 215000/398710 [00:31<00:26, 6932.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  54%|█████▍    | 217000/398710 [00:32<00:27, 6518.60 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  55%|█████▍    | 219000/398710 [00:32<00:26, 6855.88 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  55%|█████▌    | 221000/398710 [00:32<00:25, 7067.68 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  56%|█████▌    | 223000/398710 [00:33<00:25, 7017.22 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  56%|█████▋    | 225000/398710 [00:33<00:24, 7105.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  57%|█████▋    | 227000/398710 [00:33<00:27, 6234.10 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  57%|█████▋    | 229000/398710 [00:34<00:25, 6695.40 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  58%|█████▊    | 231000/398710 [00:34<00:24, 6898.84 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  58%|█████▊    | 233000/398710 [00:34<00:23, 6999.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  59%|█████▉    | 235000/398710 [00:34<00:22, 7145.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  59%|█████▉    | 237000/398710 [00:35<00:22, 7166.78 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  60%|█████▉    | 239000/398710 [00:35<00:25, 6319.56 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  60%|██████    | 241000/398710 [00:35<00:23, 6755.90 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  61%|██████    | 243000/398710 [00:36<00:22, 6949.57 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  61%|██████▏   | 245000/398710 [00:36<00:21, 7043.16 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  62%|██████▏   | 247000/398710 [00:36<00:21, 7156.70 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  62%|██████▏   | 249000/398710 [00:36<00:23, 6305.14 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  63%|██████▎   | 251000/398710 [00:37<00:21, 6729.24 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  63%|██████▎   | 253000/398710 [00:37<00:20, 6944.01 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  64%|██████▍   | 255000/398710 [00:37<00:20, 7092.61 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  64%|██████▍   | 257000/398710 [00:38<00:19, 7180.78 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  65%|██████▍   | 259000/398710 [00:38<00:19, 7136.68 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  65%|██████▌   | 261000/398710 [00:38<00:21, 6513.18 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  66%|██████▌   | 263000/398710 [00:38<00:19, 6856.93 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  66%|██████▋   | 265000/398710 [00:39<00:18, 7054.77 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  67%|██████▋   | 267000/398710 [00:39<00:18, 7153.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  67%|██████▋   | 269000/398710 [00:39<00:18, 7074.74 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  68%|██████▊   | 271000/398710 [00:40<00:17, 7147.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  68%|██████▊   | 273000/398710 [00:40<00:19, 6572.24 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  69%|██████▉   | 275000/398710 [00:40<00:17, 6924.55 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  69%|██████▉   | 277000/398710 [00:40<00:17, 7064.84 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  70%|██████▉   | 279000/398710 [00:41<00:16, 7091.46 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  70%|███████   | 281000/398710 [00:41<00:16, 6932.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  71%|███████   | 283000/398710 [00:41<00:18, 6155.65 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  71%|███████▏  | 285000/398710 [00:42<00:17, 6452.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  72%|███████▏  | 287000/398710 [00:42<00:16, 6684.51 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  72%|███████▏  | 289000/398710 [00:42<00:16, 6786.00 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  73%|███████▎  | 291000/398710 [00:43<00:15, 6961.35 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  73%|███████▎  | 292000/398710 [00:43<00:15, 7004.88 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  74%|███████▎  | 294000/398710 [00:43<00:16, 6376.87 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  74%|███████▍  | 296000/398710 [00:43<00:15, 6672.83 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  75%|███████▍  | 298000/398710 [00:44<00:14, 6949.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  75%|███████▌  | 300000/398710 [00:44<00:14, 7018.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  76%|███████▌  | 302000/398710 [00:44<00:13, 7047.52 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  76%|███████▌  | 304000/398710 [00:44<00:13, 7056.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  77%|███████▋  | 306000/398710 [00:45<00:14, 6416.98 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  77%|███████▋  | 308000/398710 [00:45<00:13, 6790.78 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  78%|███████▊  | 310000/398710 [00:45<00:12, 6963.70 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  78%|███████▊  | 312000/398710 [00:46<00:12, 6951.49 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  79%|███████▉  | 314000/398710 [00:46<00:12, 6937.77 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  79%|███████▉  | 316000/398710 [00:46<00:13, 6311.81 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  80%|███████▉  | 318000/398710 [00:47<00:12, 6607.48 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  80%|████████  | 320000/398710 [00:47<00:11, 6767.15 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  81%|████████  | 322000/398710 [00:47<00:11, 6952.45 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  81%|████████▏ | 324000/398710 [00:47<00:10, 6977.77 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  82%|████████▏ | 326000/398710 [00:48<00:11, 6152.07 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  82%|████████▏ | 328000/398710 [00:48<00:10, 6589.98 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  83%|████████▎ | 330000/398710 [00:48<00:10, 6804.43 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  83%|████████▎ | 332000/398710 [00:49<00:09, 6936.84 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  84%|████████▍ | 334000/398710 [00:49<00:09, 6950.52 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  84%|████████▍ | 336000/398710 [00:49<00:09, 6934.92 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  85%|████████▍ | 338000/398710 [00:50<00:09, 6252.01 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  85%|████████▌ | 340000/398710 [00:50<00:08, 6662.91 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  86%|████████▌ | 342000/398710 [00:50<00:09, 6194.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  86%|████████▋ | 344000/398710 [00:51<00:08, 6457.67 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  87%|████████▋ | 346000/398710 [00:51<00:07, 6732.96 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  87%|████████▋ | 347000/398710 [00:51<00:08, 6173.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  88%|████████▊ | 349000/398710 [00:51<00:08, 6008.09 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  88%|████████▊ | 351000/398710 [00:52<00:07, 6380.65 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  89%|████████▊ | 353000/398710 [00:52<00:06, 6585.35 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  89%|████████▉ | 355000/398710 [00:52<00:06, 6813.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  90%|████████▉ | 357000/398710 [00:53<00:06, 6778.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  90%|████████▉ | 358000/398710 [00:53<00:06, 6719.82 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  90%|█████████ | 360000/398710 [00:53<00:06, 6278.57 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  91%|█████████ | 362000/398710 [00:53<00:05, 6629.60 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  91%|█████████▏| 364000/398710 [00:54<00:05, 6788.79 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  92%|█████████▏| 366000/398710 [00:54<00:04, 6951.30 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  92%|█████████▏| 368000/398710 [00:54<00:04, 6898.70 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  93%|█████████▎| 370000/398710 [00:54<00:04, 7018.46 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  93%|█████████▎| 372000/398710 [00:55<00:04, 6483.09 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  94%|█████████▍| 374000/398710 [00:55<00:03, 6786.85 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  94%|█████████▍| 376000/398710 [00:55<00:03, 6977.84 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  95%|█████████▍| 378000/398710 [00:56<00:02, 7069.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  95%|█████████▌| 380000/398710 [00:56<00:02, 7121.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  96%|█████████▌| 382000/398710 [00:56<00:02, 6428.51 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  96%|█████████▋| 384000/398710 [00:57<00:02, 6744.67 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  97%|█████████▋| 386000/398710 [00:57<00:01, 6924.36 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  97%|█████████▋| 388000/398710 [00:57<00:01, 7023.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  98%|█████████▊| 390000/398710 [00:57<00:01, 7101.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  98%|█████████▊| 391000/398710 [00:58<00:01, 7026.50 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  99%|█████████▊| 393000/398710 [00:58<00:00, 6318.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  99%|█████████▉| 395000/398710 [00:58<00:00, 6467.16 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing: 100%|█████████▉| 397000/398710 [00:59<00:00, 6497.92 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 710\n",
            "CPU times: user 5min 27s, sys: 2.66 s, total: 5min 30s\n",
            "Wall time: 59.3 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# train_dataset.set_transform(tokenize)\n",
        "train_dataset = train_dataset.map(\n",
        "        tokenize,\n",
        "        remove_columns=('query', 'text', 'label'),\n",
        "        batched=True,\n",
        "        desc='Tokenizing',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFHMQrDr5XKD",
        "outputId": "796d3b62-4620-4e3e-b9ab-e1d701c659dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [2094, 540, 46, 1231, 6916, 81, 1044, 1164, 12, 1170, 4, 516, 1450, 3, 7, 5271, 697, 3072, 16630, 10, 941, 7, 8644, 53, 117, 1593, 1083, 6, 14591, 4575, 178, 5, 852, 18113, 17832, 6551, 3, 697, 17668, 33, 7, 14232, 1343, 93, 11, 18294, 4931, 4, 12558, 918, 5, 745, 797, 1806, 46, 28, 762, 21, 15, 3528, 1083, 64, 931, 1083, 87, 2649, 4, 19414, 46, 15, 3921, 16, 4064, 83, 141, 7578, 16, 941, 12, 4, 719, 123, 18, 4178, 4, 3366, 2424, 11, 9578, 474, 11084, 53, 52, 48, 1355, 5, 64, 931, 1083, 87, 762, 5993, 46, 15, 5787, 4152, 121, 49, 15, 1931, 121, 11408, 5441, 49, 15, 1931, 121, 11408, 27, 8, 15, 3022, 1380, 4, 8025, 64, 931, 1083, 87, 4101, 12, 1830, 46, 15, 3528, 167, 1083, 64, 931, 1083, 87, 4101, 12, 792, 46, 15, 2028, 601, 3249, 64, 294, 8985, 5572, 46, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2593, 1]}\n"
          ]
        }
      ],
      "source": [
        "print(valid_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uipa1K06isE"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hig02oOb580I"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t9-bawPr57P4",
        "outputId": "9028f41c-babd-4a9c-e0ed-73f657251698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'../../model/train/ptt5-base'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PATH_TRAIN_MODEL_LOCAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "bGJOtzX4FhnF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "num_step_alert = 200\n",
        "training_args = Seq2SeqTrainingArguments(output_dir=PATH_TRAIN_MODEL_LOCAL)\n",
        "# Needed to make the Trainer work with an on-the-fly transformation on the dataset\n",
        "# training_args.remove_unused_columns = False\n",
        "training_args.output_dir = PATH_TRAIN_MODEL_LOCAL\n",
        "training_args.warmup_steps=400 # Alterar!\n",
        "training_args.num_train_epochs=4.0 # Alterar!\n",
        "training_args.logging_steps=num_step_alert # Alterar!\n",
        "training_args.save_strategy=\"steps\"\n",
        "training_args.save_steps=num_step_alert\n",
        "training_args.save_total_limit=10\n",
        "training_args.learning_rate=5e-5\n",
        "training_args.per_device_train_batch_size=16 # t4: 8, a100-40: 32\n",
        "training_args.gradient_accumulation_steps=4 # t4: 4, a100-40: 2\n",
        "#training_args._n_gpu = 1\n",
        "# training_args.bf16 = True # se for usar a100, 3090, 4090 -> usar\n",
        "training_args.ignore_data_skip = True\n",
        "training_args.load_best_model_at_end = True\n",
        "training_args.evaluation_strategy='steps'\n",
        "training_args.eval_steps=num_step_alert\n",
        "training_args.do_eval = True\n",
        "# training_args.optim='adamw_hf' #default\n",
        "training_args.gradient_checkpointing = False # True\n",
        "# se precisar economizar gpu\n",
        "# training_args.optim='adamw_bnb_8bit'\n",
        "# training_args.gradient_checkpointing = True\n",
        "training_args.report_to=\"neptune\",\n",
        "# training_args.report_to = 'None'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFMX-y4ZkLF",
        "outputId": "df911339-88b1-4f97-d33d-4f64cd203bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mMQw6ra63Xk",
        "outputId": "98e651ba-25d0-419c-fe39-143a4db50d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=200,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=True,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../model/train/ptt5-base/runs/Jul05_19-58-20_borela-wks,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=200,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=4.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=../../model/train/ptt5-base,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=('neptune',),\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../model/train/ptt5-base,\n",
            "save_on_each_node=False,\n",
            "save_steps=200,\n",
            "save_strategy=steps,\n",
            "save_total_limit=10,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=400,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
          ]
        }
      ],
      "source": [
        "print(training_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "nBRTbzhSW7hF"
      },
      "outputs": [],
      "source": [
        "# from transformers.integrations import NeptuneCallback\n",
        "# rastro_neptune = NeptuneRastroRun(hparam, parm_lista_tag= tag_contexto_rastro)\n",
        "# neptune_callback = NeptuneCallback(run=rastro_neptune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# se local\n",
        "nome_caminho_modelo = \"/home/borela/fontes/relevar-busca/modelo/\" + MODEL_NAME\n",
        "assert os.path.exists(nome_caminho_modelo), f\"Path para {MODEL_NAME} não existe!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJt67cC8u7XO",
        "outputId": "f1825556-6a3f-48c9-a678-370de859034a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)lve/main/config.json: 100%|██████████| 635/635 [00:00<00:00, 230kB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 892M/892M [01:01<00:00, 14.6MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3.88 s, sys: 2.27 s, total: 6.15 s\n",
            "Wall time: 1min 5s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(nome_caminho_modelo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "rrK0NBIV2GjX"
      },
      "outputs": [],
      "source": [
        "trainer_cls = Seq2SeqTrainer\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "Srq3KA_ZlWPD"
      },
      "outputs": [],
      "source": [
        "# Limpa o cache da memória da GPU\n",
        "# del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWdCbovj25Sh",
        "outputId": "33a0353a-a926-4470-c532-458a6c425b73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoik0K8sX3Jg"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "XhSrqbj-tgFm",
        "outputId": "29003b19-31f0-4154-c0e9-d5cc4fcb9af3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Exception: </span>Parar aqui reinício!\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mException: \u001b[0mParar aqui reinício!\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "raise Exception('Parar aqui reinício!')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dicas para integrar com Neptune \n",
        "https://docs.neptune.ai/integrations/transformers/#__tabbed_2_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "jqrhQxd3E55t"
      },
      "outputs": [],
      "source": [
        "trainer = trainer_cls(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n",
            "\u001b[0;32m----> 1\u001b[0m \u001b[39mdel\u001b[39;00m trainer\n",
            "\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ],
      "source": [
        "del trainer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "reiniciando checkpoint 5200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "QOqMyEd_neHN",
        "outputId": "3bb99a99-33be-4315-fd83-79cbb3624050"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading model from ../../model/train/ptt5-base/checkpoint-5200.\n",
            "You are resuming training from a checkpoint trained with 4.30.2 of Transformers but your current version is 4.25.1. This is not recommended and could yield to errors or unwanted behaviors.\n",
            "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 398710\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 24920\n",
            "  Number of trainable parameters = 222903552\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 0\n",
            "  Continuing training from global step 5200\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-108\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/24920 [00:00<?, ?it/s]Didn't manage to set back the RNG states of the GPU because of the following error:\n",
            " RNG state is wrong size\n",
            "This won't yield the same results as if the training had not been interrupted.\n",
            " 22%|██▏       | 5400/24920 [14:04<22:45:30,  4.20s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0386, 'learning_rate': 3.9804241435562806e-05, 'epoch': 0.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                       \n",
            " 22%|██▏       | 5400/24920 [14:59<22:45:30,  4.20s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-5400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-5400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03830071911215782, 'eval_runtime': 55.5829, 'eval_samples_per_second': 72.468, 'eval_steps_per_second': 9.068, 'epoch': 0.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-5400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-5400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-5400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-5400/spiece.model\n",
            " 22%|██▏       | 5600/24920 [29:02<22:07:17,  4.12s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0397, 'learning_rate': 3.9396411092985316e-05, 'epoch': 0.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                       \n",
            " 22%|██▏       | 5600/24920 [29:56<22:07:17,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-5600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-5600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.039101142436265945, 'eval_runtime': 54.7467, 'eval_samples_per_second': 73.575, 'eval_steps_per_second': 9.206, 'epoch': 0.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-5600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-5600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-5600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-5600/spiece.model\n",
            " 23%|██▎       | 5800/24920 [43:35<21:40:49,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0359, 'learning_rate': 3.898858075040783e-05, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                       \n",
            " 23%|██▎       | 5800/24920 [44:30<21:40:49,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-5800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-5800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03769339248538017, 'eval_runtime': 54.5549, 'eval_samples_per_second': 73.834, 'eval_steps_per_second': 9.238, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-5800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-5800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-5800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-5800/spiece.model\n",
            " 24%|██▍       | 6000/24920 [58:09<21:29:50,  4.09s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0352, 'learning_rate': 3.858075040783034e-05, 'epoch': 0.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                       \n",
            " 24%|██▍       | 6000/24920 [59:04<21:29:50,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.037992507219314575, 'eval_runtime': 54.6378, 'eval_samples_per_second': 73.722, 'eval_steps_per_second': 9.224, 'epoch': 0.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6000/spiece.model\n",
            " 25%|██▍       | 6200/24920 [1:12:45<21:15:49,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0411, 'learning_rate': 3.817292006525285e-05, 'epoch': 0.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 25%|██▍       | 6200/24920 [1:13:40<21:15:49,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035734109580516815, 'eval_runtime': 54.4601, 'eval_samples_per_second': 73.962, 'eval_steps_per_second': 9.254, 'epoch': 0.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6200/spiece.model\n",
            " 26%|██▌       | 6400/24920 [1:27:19<21:08:14,  4.11s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0402, 'learning_rate': 3.776508972267537e-05, 'epoch': 0.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 26%|██▌       | 6400/24920 [1:28:13<21:08:14,  4.11s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03558366373181343, 'eval_runtime': 54.5072, 'eval_samples_per_second': 73.899, 'eval_steps_per_second': 9.246, 'epoch': 0.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6400/spiece.model\n",
            " 26%|██▋       | 6600/24920 [1:41:52<20:49:54,  4.09s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.036, 'learning_rate': 3.735725938009788e-05, 'epoch': 0.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 26%|██▋       | 6600/24920 [1:42:47<20:49:54,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035693056881427765, 'eval_runtime': 54.5174, 'eval_samples_per_second': 73.885, 'eval_steps_per_second': 9.245, 'epoch': 0.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6600/spiece.model\n",
            " 27%|██▋       | 6800/24920 [1:56:25<20:32:25,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0396, 'learning_rate': 3.69494290375204e-05, 'epoch': 0.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 27%|██▋       | 6800/24920 [1:57:19<20:32:25,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0364278145134449, 'eval_runtime': 54.4275, 'eval_samples_per_second': 74.007, 'eval_steps_per_second': 9.26, 'epoch': 0.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6800/spiece.model\n",
            " 28%|██▊       | 7000/24920 [2:10:59<20:18:37,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0383, 'learning_rate': 3.654159869494291e-05, 'epoch': 0.29}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 28%|██▊       | 7000/24920 [2:11:54<20:18:37,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035444226115942, 'eval_runtime': 54.4906, 'eval_samples_per_second': 73.921, 'eval_steps_per_second': 9.249, 'epoch': 0.29}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7000/spiece.model\n",
            " 29%|██▉       | 7200/24920 [2:25:32<20:06:02,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.04, 'learning_rate': 3.613376835236542e-05, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 29%|██▉       | 7200/24920 [2:26:26<20:06:02,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035171572118997574, 'eval_runtime': 54.4213, 'eval_samples_per_second': 74.015, 'eval_steps_per_second': 9.261, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-5200] due to args.save_total_limit\n",
            " 30%|██▉       | 7400/24920 [2:40:06<19:44:41,  4.06s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0389, 'learning_rate': 3.572593800978793e-05, 'epoch': 0.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 30%|██▉       | 7400/24920 [2:41:01<19:44:41,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03583730384707451, 'eval_runtime': 54.7785, 'eval_samples_per_second': 73.532, 'eval_steps_per_second': 9.201, 'epoch': 0.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-5400] due to args.save_total_limit\n",
            " 30%|███       | 7600/24920 [2:54:43<19:49:03,  4.12s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0346, 'learning_rate': 3.5318107667210443e-05, 'epoch': 0.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 30%|███       | 7600/24920 [2:55:37<19:49:03,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03525320068001747, 'eval_runtime': 54.8676, 'eval_samples_per_second': 73.413, 'eval_steps_per_second': 9.186, 'epoch': 0.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-5600] due to args.save_total_limit\n",
            " 31%|███▏      | 7800/24920 [3:09:15<19:23:44,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.039, 'learning_rate': 3.4910277324632953e-05, 'epoch': 0.42}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 31%|███▏      | 7800/24920 [3:10:10<19:23:44,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03660828247666359, 'eval_runtime': 54.6418, 'eval_samples_per_second': 73.716, 'eval_steps_per_second': 9.224, 'epoch': 0.42}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-5800] due to args.save_total_limit\n",
            " 32%|███▏      | 8000/24920 [3:23:46<19:09:10,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0363, 'learning_rate': 3.4502446982055463e-05, 'epoch': 0.45}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 32%|███▏      | 8000/24920 [3:24:41<19:09:10,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03499669209122658, 'eval_runtime': 55.0438, 'eval_samples_per_second': 73.178, 'eval_steps_per_second': 9.156, 'epoch': 0.45}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6000] due to args.save_total_limit\n",
            " 33%|███▎      | 8200/24920 [3:38:19<18:56:42,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0359, 'learning_rate': 3.409461663947798e-05, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 33%|███▎      | 8200/24920 [3:39:14<18:56:42,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035376258194446564, 'eval_runtime': 54.9293, 'eval_samples_per_second': 73.331, 'eval_steps_per_second': 9.175, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6200] due to args.save_total_limit\n",
            " 34%|███▎      | 8400/24920 [3:52:53<18:41:51,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.036, 'learning_rate': 3.368678629690049e-05, 'epoch': 0.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 34%|███▎      | 8400/24920 [3:53:47<18:41:51,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035488445311784744, 'eval_runtime': 54.5872, 'eval_samples_per_second': 73.79, 'eval_steps_per_second': 9.233, 'epoch': 0.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6400] due to args.save_total_limit\n",
            " 35%|███▍      | 8600/24920 [4:07:26<18:24:36,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0373, 'learning_rate': 3.327895595432301e-05, 'epoch': 0.55}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 35%|███▍      | 8600/24920 [4:08:20<18:24:36,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03550172969698906, 'eval_runtime': 54.471, 'eval_samples_per_second': 73.948, 'eval_steps_per_second': 9.253, 'epoch': 0.55}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6600] due to args.save_total_limit\n",
            " 35%|███▌      | 8800/24920 [4:21:58<18:16:40,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0355, 'learning_rate': 3.287112561174552e-05, 'epoch': 0.58}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 35%|███▌      | 8800/24920 [4:22:52<18:16:40,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034991305321455, 'eval_runtime': 54.6821, 'eval_samples_per_second': 73.662, 'eval_steps_per_second': 9.217, 'epoch': 0.58}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6800] due to args.save_total_limit\n",
            " 36%|███▌      | 9000/24920 [4:36:32<17:59:09,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0372, 'learning_rate': 3.246329526916803e-05, 'epoch': 0.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 36%|███▌      | 9000/24920 [4:37:26<17:59:09,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034312039613723755, 'eval_runtime': 54.6616, 'eval_samples_per_second': 73.69, 'eval_steps_per_second': 9.22, 'epoch': 0.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7000] due to args.save_total_limit\n",
            " 37%|███▋      | 9200/24920 [4:51:04<17:53:47,  4.10s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0342, 'learning_rate': 3.205546492659054e-05, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 37%|███▋      | 9200/24920 [4:51:59<17:53:47,  4.10s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035302210599184036, 'eval_runtime': 54.5633, 'eval_samples_per_second': 73.823, 'eval_steps_per_second': 9.237, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7200] due to args.save_total_limit\n",
            " 38%|███▊      | 9400/24920 [5:05:35<17:31:30,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0345, 'learning_rate': 3.1647634584013054e-05, 'epoch': 0.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 38%|███▊      | 9400/24920 [5:06:29<17:31:30,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03438310697674751, 'eval_runtime': 54.5577, 'eval_samples_per_second': 73.83, 'eval_steps_per_second': 9.238, 'epoch': 0.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7400] due to args.save_total_limit\n",
            " 39%|███▊      | 9600/24920 [5:20:06<17:17:21,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.037, 'learning_rate': 3.1239804241435564e-05, 'epoch': 0.71}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 39%|███▊      | 9600/24920 [5:21:01<17:17:21,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0335809662938118, 'eval_runtime': 54.6769, 'eval_samples_per_second': 73.669, 'eval_steps_per_second': 9.218, 'epoch': 0.71}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7600] due to args.save_total_limit\n",
            " 39%|███▉      | 9800/24920 [5:34:38<17:09:55,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0359, 'learning_rate': 3.0831973898858074e-05, 'epoch': 0.74}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 39%|███▉      | 9800/24920 [5:35:33<17:09:55,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034582629799842834, 'eval_runtime': 54.5553, 'eval_samples_per_second': 73.833, 'eval_steps_per_second': 9.238, 'epoch': 0.74}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7800] due to args.save_total_limit\n",
            " 40%|████      | 10000/24920 [5:49:09<16:52:39,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0335, 'learning_rate': 3.0424143556280587e-05, 'epoch': 0.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 40%|████      | 10000/24920 [5:50:03<16:52:39,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.039332326501607895, 'eval_runtime': 54.3935, 'eval_samples_per_second': 74.053, 'eval_steps_per_second': 9.266, 'epoch': 0.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8000] due to args.save_total_limit\n",
            " 41%|████      | 10200/24920 [6:03:39<16:37:30,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0339, 'learning_rate': 3.00163132137031e-05, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 41%|████      | 10200/24920 [6:04:33<16:37:30,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.036113590002059937, 'eval_runtime': 54.3368, 'eval_samples_per_second': 74.13, 'eval_steps_per_second': 9.275, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8200] due to args.save_total_limit\n",
            " 42%|████▏     | 10400/24920 [6:18:08<16:22:16,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0341, 'learning_rate': 2.9608482871125614e-05, 'epoch': 0.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 42%|████▏     | 10400/24920 [6:19:02<16:22:16,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033041030168533325, 'eval_runtime': 54.4255, 'eval_samples_per_second': 74.009, 'eval_steps_per_second': 9.26, 'epoch': 0.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8400] due to args.save_total_limit\n",
            " 43%|████▎     | 10600/24920 [6:32:37<16:10:12,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0381, 'learning_rate': 2.9200652528548127e-05, 'epoch': 0.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 43%|████▎     | 10600/24920 [6:33:31<16:10:12,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034012772142887115, 'eval_runtime': 54.3142, 'eval_samples_per_second': 74.161, 'eval_steps_per_second': 9.279, 'epoch': 0.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8600] due to args.save_total_limit\n",
            " 43%|████▎     | 10800/24920 [6:47:10<15:56:53,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0335, 'learning_rate': 2.8792822185970637e-05, 'epoch': 0.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 43%|████▎     | 10800/24920 [6:48:05<15:56:53,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032748669385910034, 'eval_runtime': 54.6942, 'eval_samples_per_second': 73.646, 'eval_steps_per_second': 9.215, 'epoch': 0.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8800] due to args.save_total_limit\n",
            " 44%|████▍     | 11000/24920 [7:01:42<15:45:30,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0341, 'learning_rate': 2.838499184339315e-05, 'epoch': 0.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 44%|████▍     | 11000/24920 [7:02:37<15:45:30,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034486666321754456, 'eval_runtime': 54.7649, 'eval_samples_per_second': 73.551, 'eval_steps_per_second': 9.203, 'epoch': 0.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9000] due to args.save_total_limit\n",
            " 45%|████▍     | 11200/24920 [7:16:19<15:36:55,  4.10s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0348, 'learning_rate': 2.7977161500815664e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 45%|████▍     | 11200/24920 [7:17:14<15:36:55,  4.10s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033023323863744736, 'eval_runtime': 54.5971, 'eval_samples_per_second': 73.777, 'eval_steps_per_second': 9.231, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9200] due to args.save_total_limit\n",
            " 46%|████▌     | 11400/24920 [7:30:51<15:14:23,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0337, 'learning_rate': 2.7569331158238177e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 46%|████▌     | 11400/24920 [7:31:46<15:14:23,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03410288691520691, 'eval_runtime': 54.4564, 'eval_samples_per_second': 73.967, 'eval_steps_per_second': 9.255, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9400] due to args.save_total_limit\n",
            " 47%|████▋     | 11600/24920 [7:45:20<15:00:06,  4.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0269, 'learning_rate': 2.7161500815660684e-05, 'epoch': 1.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 47%|████▋     | 11600/24920 [7:46:15<15:00:06,  4.05s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03432347625494003, 'eval_runtime': 54.4664, 'eval_samples_per_second': 73.954, 'eval_steps_per_second': 9.253, 'epoch': 1.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9600] due to args.save_total_limit\n",
            " 47%|████▋     | 11800/24920 [7:59:49<14:50:44,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0283, 'learning_rate': 2.6753670473083197e-05, 'epoch': 1.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 47%|████▋     | 11800/24920 [8:00:44<14:50:44,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03402471914887428, 'eval_runtime': 54.5164, 'eval_samples_per_second': 73.886, 'eval_steps_per_second': 9.245, 'epoch': 1.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9800] due to args.save_total_limit\n",
            " 48%|████▊     | 12000/24920 [8:14:20<14:41:43,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0289, 'learning_rate': 2.634584013050571e-05, 'epoch': 1.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 48%|████▊     | 12000/24920 [8:15:14<14:41:43,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03309272229671478, 'eval_runtime': 54.8533, 'eval_samples_per_second': 73.432, 'eval_steps_per_second': 9.188, 'epoch': 1.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10000] due to args.save_total_limit\n",
            " 49%|████▉     | 12200/24920 [8:28:50<14:23:40,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0274, 'learning_rate': 2.5938009787928224e-05, 'epoch': 1.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 49%|████▉     | 12200/24920 [8:29:45<14:23:40,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0332350879907608, 'eval_runtime': 55.0246, 'eval_samples_per_second': 73.204, 'eval_steps_per_second': 9.16, 'epoch': 1.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10200] due to args.save_total_limit\n",
            " 50%|████▉     | 12400/24920 [8:43:21<14:06:10,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0286, 'learning_rate': 2.5530179445350734e-05, 'epoch': 1.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 50%|████▉     | 12400/24920 [8:44:16<14:06:10,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03304194658994675, 'eval_runtime': 54.4162, 'eval_samples_per_second': 74.022, 'eval_steps_per_second': 9.262, 'epoch': 1.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10400] due to args.save_total_limit\n",
            " 51%|█████     | 12600/24920 [8:57:51<13:53:01,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.028, 'learning_rate': 2.5122349102773248e-05, 'epoch': 1.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 51%|█████     | 12600/24920 [8:58:45<13:53:01,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03348330035805702, 'eval_runtime': 54.3818, 'eval_samples_per_second': 74.069, 'eval_steps_per_second': 9.268, 'epoch': 1.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10600] due to args.save_total_limit\n",
            " 51%|█████▏    | 12800/24920 [9:12:19<13:39:41,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0271, 'learning_rate': 2.471451876019576e-05, 'epoch': 1.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 51%|█████▏    | 12800/24920 [9:13:13<13:39:41,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03277859091758728, 'eval_runtime': 54.3456, 'eval_samples_per_second': 74.118, 'eval_steps_per_second': 9.274, 'epoch': 1.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10800] due to args.save_total_limit\n",
            " 52%|█████▏    | 13000/24920 [9:26:47<13:24:52,  4.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0268, 'learning_rate': 2.4306688417618274e-05, 'epoch': 1.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 52%|█████▏    | 13000/24920 [9:27:41<13:24:52,  4.05s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03482929989695549, 'eval_runtime': 54.8141, 'eval_samples_per_second': 73.485, 'eval_steps_per_second': 9.195, 'epoch': 1.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11000] due to args.save_total_limit\n",
            " 53%|█████▎    | 13200/24920 [9:41:15<13:17:04,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0265, 'learning_rate': 2.3898858075040784e-05, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 53%|█████▎    | 13200/24920 [9:42:09<13:17:04,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03329039737582207, 'eval_runtime': 54.4179, 'eval_samples_per_second': 74.02, 'eval_steps_per_second': 9.262, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11200] due to args.save_total_limit\n",
            " 54%|█████▍    | 13400/24920 [9:55:43<13:03:05,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.03, 'learning_rate': 2.3491027732463298e-05, 'epoch': 1.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 54%|█████▍    | 13400/24920 [9:56:37<13:03:05,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03396589681506157, 'eval_runtime': 54.3845, 'eval_samples_per_second': 74.065, 'eval_steps_per_second': 9.267, 'epoch': 1.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11400] due to args.save_total_limit\n",
            " 55%|█████▍    | 13600/24920 [10:10:13<12:51:07,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0256, 'learning_rate': 2.3083197389885808e-05, 'epoch': 1.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 55%|█████▍    | 13600/24920 [10:11:08<12:51:07,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03387042135000229, 'eval_runtime': 54.3974, 'eval_samples_per_second': 74.048, 'eval_steps_per_second': 9.265, 'epoch': 1.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11600] due to args.save_total_limit\n",
            " 55%|█████▌    | 13800/24920 [10:24:50<12:37:54,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0284, 'learning_rate': 2.267536704730832e-05, 'epoch': 1.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 55%|█████▌    | 13800/24920 [10:25:46<12:37:54,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03275327757000923, 'eval_runtime': 55.4019, 'eval_samples_per_second': 72.705, 'eval_steps_per_second': 9.097, 'epoch': 1.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11800] due to args.save_total_limit\n",
            " 56%|█████▌    | 14000/24920 [10:39:24<12:22:00,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0291, 'learning_rate': 2.226753670473083e-05, 'epoch': 1.41}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 56%|█████▌    | 14000/24920 [10:40:19<12:22:00,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03327794745564461, 'eval_runtime': 54.6509, 'eval_samples_per_second': 73.704, 'eval_steps_per_second': 9.222, 'epoch': 1.41}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12000] due to args.save_total_limit\n",
            " 57%|█████▋    | 14200/24920 [10:53:53<12:05:35,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0289, 'learning_rate': 2.1859706362153344e-05, 'epoch': 1.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 57%|█████▋    | 14200/24920 [10:54:48<12:05:35,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033764783293008804, 'eval_runtime': 54.3898, 'eval_samples_per_second': 74.058, 'eval_steps_per_second': 9.266, 'epoch': 1.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12200] due to args.save_total_limit\n",
            " 58%|█████▊    | 14400/24920 [11:08:22<11:52:35,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0257, 'learning_rate': 2.1451876019575858e-05, 'epoch': 1.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 58%|█████▊    | 14400/24920 [11:09:16<11:52:35,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033985648304224014, 'eval_runtime': 54.2261, 'eval_samples_per_second': 74.282, 'eval_steps_per_second': 9.294, 'epoch': 1.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12400] due to args.save_total_limit\n",
            " 59%|█████▊    | 14600/24920 [11:22:58<11:42:49,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0268, 'learning_rate': 2.104404567699837e-05, 'epoch': 1.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 59%|█████▊    | 14600/24920 [11:23:52<11:42:49,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03413856402039528, 'eval_runtime': 54.5704, 'eval_samples_per_second': 73.813, 'eval_steps_per_second': 9.236, 'epoch': 1.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12600] due to args.save_total_limit\n",
            " 59%|█████▉    | 14800/24920 [11:37:25<11:22:40,  4.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0299, 'learning_rate': 2.0636215334420885e-05, 'epoch': 1.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 59%|█████▉    | 14800/24920 [11:38:19<11:22:40,  4.05s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034166935831308365, 'eval_runtime': 54.2621, 'eval_samples_per_second': 74.232, 'eval_steps_per_second': 9.288, 'epoch': 1.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12800] due to args.save_total_limit\n",
            " 60%|██████    | 15000/24920 [11:51:53<11:09:25,  4.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0264, 'learning_rate': 2.0228384991843395e-05, 'epoch': 1.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 60%|██████    | 15000/24920 [11:52:48<11:09:25,  4.05s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033432092517614365, 'eval_runtime': 54.432, 'eval_samples_per_second': 74.001, 'eval_steps_per_second': 9.259, 'epoch': 1.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13000] due to args.save_total_limit\n",
            " 61%|██████    | 15200/24920 [12:06:23<10:56:55,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0253, 'learning_rate': 1.9820554649265908e-05, 'epoch': 1.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 61%|██████    | 15200/24920 [12:07:18<10:56:55,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0337190218269825, 'eval_runtime': 54.46, 'eval_samples_per_second': 73.963, 'eval_steps_per_second': 9.254, 'epoch': 1.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13200] due to args.save_total_limit\n",
            " 62%|██████▏   | 15400/24920 [12:20:52<10:47:07,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0292, 'learning_rate': 1.9412724306688418e-05, 'epoch': 1.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 62%|██████▏   | 15400/24920 [12:21:46<10:47:07,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03311581164598465, 'eval_runtime': 54.3622, 'eval_samples_per_second': 74.096, 'eval_steps_per_second': 9.271, 'epoch': 1.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13400] due to args.save_total_limit\n",
            " 63%|██████▎   | 15600/24920 [12:35:23<10:35:42,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0258, 'learning_rate': 1.900489396411093e-05, 'epoch': 1.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 63%|██████▎   | 15600/24920 [12:36:18<10:35:42,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03332404047250748, 'eval_runtime': 54.5929, 'eval_samples_per_second': 73.783, 'eval_steps_per_second': 9.232, 'epoch': 1.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13600] due to args.save_total_limit\n",
            " 63%|██████▎   | 15800/24920 [12:49:55<10:20:55,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0292, 'learning_rate': 1.859706362153344e-05, 'epoch': 1.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 63%|██████▎   | 15800/24920 [12:50:50<10:20:55,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03233485668897629, 'eval_runtime': 54.4815, 'eval_samples_per_second': 73.933, 'eval_steps_per_second': 9.251, 'epoch': 1.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13800] due to args.save_total_limit\n",
            " 64%|██████▍   | 16000/24920 [13:04:31<10:07:22,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0262, 'learning_rate': 1.8189233278955955e-05, 'epoch': 1.73}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 64%|██████▍   | 16000/24920 [13:05:26<10:07:22,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03301709517836571, 'eval_runtime': 54.7592, 'eval_samples_per_second': 73.558, 'eval_steps_per_second': 9.204, 'epoch': 1.73}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14000] due to args.save_total_limit\n",
            " 65%|██████▌   | 16200/24920 [13:19:05<9:51:54,  4.07s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0297, 'learning_rate': 1.7781402936378465e-05, 'epoch': 1.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 65%|██████▌   | 16200/24920 [13:20:00<9:51:54,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03242029994726181, 'eval_runtime': 54.8458, 'eval_samples_per_second': 73.442, 'eval_steps_per_second': 9.189, 'epoch': 1.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14200] due to args.save_total_limit\n",
            " 66%|██████▌   | 16400/24920 [13:33:37<9:36:12,  4.06s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0264, 'learning_rate': 1.7373572593800978e-05, 'epoch': 1.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 66%|██████▌   | 16400/24920 [13:34:32<9:36:12,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0320003479719162, 'eval_runtime': 54.7364, 'eval_samples_per_second': 73.589, 'eval_steps_per_second': 9.208, 'epoch': 1.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14400] due to args.save_total_limit\n",
            " 67%|██████▋   | 16600/24920 [13:48:08<9:24:21,  4.07s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0257, 'learning_rate': 1.6965742251223495e-05, 'epoch': 1.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 67%|██████▋   | 16600/24920 [13:49:03<9:24:21,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.031586699187755585, 'eval_runtime': 54.4731, 'eval_samples_per_second': 73.945, 'eval_steps_per_second': 9.252, 'epoch': 1.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14600] due to args.save_total_limit\n",
            " 67%|██████▋   | 16800/24920 [14:02:39<9:13:00,  4.09s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0257, 'learning_rate': 1.6557911908646005e-05, 'epoch': 1.86}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 67%|██████▋   | 16800/24920 [14:03:33<9:13:00,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03272559493780136, 'eval_runtime': 54.5957, 'eval_samples_per_second': 73.779, 'eval_steps_per_second': 9.232, 'epoch': 1.86}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14800] due to args.save_total_limit\n",
            " 68%|██████▊   | 17000/24920 [14:17:11<8:59:42,  4.09s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0274, 'learning_rate': 1.6150081566068518e-05, 'epoch': 1.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 68%|██████▊   | 17000/24920 [14:18:06<8:59:42,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03192828223109245, 'eval_runtime': 54.6653, 'eval_samples_per_second': 73.685, 'eval_steps_per_second': 9.22, 'epoch': 1.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15000] due to args.save_total_limit\n",
            " 69%|██████▉   | 17191/24920 [14:31:12<8:48:49,  4.11s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: HTTPServiceUnavailable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 17192/24920 [14:31:17<8:50:05,  4.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Communication with Neptune restored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 17200/24920 [14:31:50<8:53:33,  4.15s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0255, 'learning_rate': 1.5742251223491028e-05, 'epoch': 1.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 69%|██████▉   | 17200/24920 [14:32:45<8:53:33,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033229388296604156, 'eval_runtime': 55.1296, 'eval_samples_per_second': 73.064, 'eval_steps_per_second': 9.142, 'epoch': 1.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15200] due to args.save_total_limit\n",
            " 70%|██████▉   | 17400/24920 [14:46:40<8:41:46,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0273, 'learning_rate': 1.533442088091354e-05, 'epoch': 1.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 70%|██████▉   | 17400/24920 [14:47:35<8:41:46,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032098811119794846, 'eval_runtime': 55.16, 'eval_samples_per_second': 73.024, 'eval_steps_per_second': 9.137, 'epoch': 1.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15400] due to args.save_total_limit\n",
            " 71%|███████   | 17600/24920 [15:01:31<8:28:36,  4.17s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0288, 'learning_rate': 1.4926590538336052e-05, 'epoch': 1.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 71%|███████   | 17600/24920 [15:02:26<8:28:36,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032441623508930206, 'eval_runtime': 55.211, 'eval_samples_per_second': 72.957, 'eval_steps_per_second': 9.129, 'epoch': 1.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15600] due to args.save_total_limit\n",
            " 71%|███████▏  | 17800/24920 [15:16:21<8:18:48,  4.20s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.024, 'learning_rate': 1.4518760195758565e-05, 'epoch': 2.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 71%|███████▏  | 17800/24920 [15:17:17<8:18:48,  4.20s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03280528634786606, 'eval_runtime': 55.4043, 'eval_samples_per_second': 72.702, 'eval_steps_per_second': 9.097, 'epoch': 2.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15800] due to args.save_total_limit\n",
            " 72%|███████▏  | 18000/24920 [15:31:11<8:02:44,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0231, 'learning_rate': 1.4110929853181077e-05, 'epoch': 2.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 72%|███████▏  | 18000/24920 [15:32:06<8:02:44,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-18000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-18000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032688163220882416, 'eval_runtime': 55.4264, 'eval_samples_per_second': 72.673, 'eval_steps_per_second': 9.093, 'epoch': 2.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-18000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-18000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-16000] due to args.save_total_limit\n",
            " 73%|███████▎  | 18167/24920 [15:43:45<7:45:36,  4.14s/it] "
          ]
        }
      ],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TABp6GNzneEa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySsRp88eneBS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khUa05-9e5RC"
      },
      "outputs": [],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enw1dGpIe5KO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uz-o2_H3Bfm"
      },
      "outputs": [],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jKfDvjb3BcU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCkjjXJq-aM-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CTpjx8V-aGb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcjNusQs3BZP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1qE7f2L3BVn"
      },
      "outputs": [],
      "source": [
        "EXECUÇÕES ANTERIORES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAP47M6XSVW-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhejNQYISWKJ"
      },
      "outputs": [],
      "source": [
        "print(train_metrics )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC0aZ7jrSWG3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMFVAAj7SWDH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtHXA95tSWAF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTa9O1SMSWra"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc-0ySvYFyxe"
      },
      "source": [
        "Não sei se a mudança do batch size (32x2) 64 para 32 (8x4) impactou passar por dados duas vezes!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU3xzNdiETy4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ytqhIl_ULr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av7Mc8VxETvW"
      },
      "outputs": [],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuooOEKxETr8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oumFThfiETpG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ_RROtmETl0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnK4bRPMETjn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsl2RdzcETf8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ3J3DfrFHj5"
      },
      "source": [
        "Abaixo execucao lim50: a100/40gb (erro mount drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L10lhQsHkiHl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_metrics = trainer.train(resume_from_checkpoint=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkNe4_3okiEd"
      },
      "outputs": [],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX_4MBFFkiBq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DDvR-7xS_7_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_metrics = trainer.train(resume_from_checkpoint=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1fz-U5b8bp3"
      },
      "outputs": [],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tLtOiY_cvBs"
      },
      "outputs": [],
      "source": [
        "train_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuCwnEea9Rxp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvTA3D2pc2w7"
      },
      "outputs": [],
      "source": [
        "huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk415BxBfguC"
      },
      "outputs": [],
      "source": [
        "pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEh5XDoLgX7I"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EucZwwegcGe"
      },
      "outputs": [],
      "source": [
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR3TjrK7gg8x"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(\n",
        "    model_id=\"ptt5-base-pt-msmarco-100k-v2-indir-lim100\",\n",
        "    repo_name=\"marcusborela\"\n",
        "    # use_auth_token=\"\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "e431eb1d856c426fade2a694f8536bd46c4e9c4bd47cb4afd3fb4d2c61122b03"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d1db4f6d75944ddbe4284da885e8038": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36a988374e504d9a997e8e48955d6bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492c664c2bf84d98bd5372d20ed8f8d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "4b258b1461cb4ebd95674bbe963da89b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d153ac235b643d9b9fb1f502f220845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e5ea796f1384a3984a8f52720d3c445": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc2c9da3baaa4d2b92aaf7a4ab74035e",
            "placeholder": "​",
            "style": "IPY_MODEL_b849d25c533f4618afa11c027f3feaad",
            "value": "Tokenizing: 100%"
          }
        },
        "4e6e72fe001744c4830d22150b8e2b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "520b60a897d342648199509590bcb3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee3f86de98ba49fda4fe387569bd85d8",
            "placeholder": "​",
            "style": "IPY_MODEL_4e6e72fe001744c4830d22150b8e2b84",
            "value": " 400724/400724 [06:46&lt;00:00, 880.84 examples/s]"
          }
        },
        "5c546b9f682444be932b17b2ff4c3409": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ce6ab38f4d4400cb7ba03d59d7a7b21",
            "max": 400724,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d1db4f6d75944ddbe4284da885e8038",
            "value": 400724
          }
        },
        "614aa774c3b44e02b377967e4ceea596": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad3f253125f410bbedb811f3313b312": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614aa774c3b44e02b377967e4ceea596",
            "max": 2014,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc8cb0b3f3ba4174bd7f4e3d5a6f8f46",
            "value": 2014
          }
        },
        "6ce6ab38f4d4400cb7ba03d59d7a7b21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dec4e3e109040308c245ca52fc29e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e5ea796f1384a3984a8f52720d3c445",
              "IPY_MODEL_5c546b9f682444be932b17b2ff4c3409",
              "IPY_MODEL_520b60a897d342648199509590bcb3c7"
            ],
            "layout": "IPY_MODEL_f50b231ed5fa423e81400f13d72f77bf"
          }
        },
        "80f9f8c628fc4eeeaead5bf16c880b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36a988374e504d9a997e8e48955d6bd2",
            "placeholder": "​",
            "style": "IPY_MODEL_be972ca97d034c5fb5b616299d8dd29c",
            "value": " 2000/2014 [00:01&lt;00:00, 1120.74 examples/s]"
          }
        },
        "8363e87c191c4e27a55624d76592522e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ddd0ce214044ff4985779bc45a78075",
              "IPY_MODEL_6ad3f253125f410bbedb811f3313b312",
              "IPY_MODEL_80f9f8c628fc4eeeaead5bf16c880b94"
            ],
            "layout": "IPY_MODEL_492c664c2bf84d98bd5372d20ed8f8d2"
          }
        },
        "9ddd0ce214044ff4985779bc45a78075": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b258b1461cb4ebd95674bbe963da89b",
            "placeholder": "​",
            "style": "IPY_MODEL_4d153ac235b643d9b9fb1f502f220845",
            "value": "Tokenizing:  99%"
          }
        },
        "b849d25c533f4618afa11c027f3feaad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc2c9da3baaa4d2b92aaf7a4ab74035e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be972ca97d034c5fb5b616299d8dd29c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc8cb0b3f3ba4174bd7f4e3d5a6f8f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee3f86de98ba49fda4fe387569bd85d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f50b231ed5fa423e81400f13d72f77bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
