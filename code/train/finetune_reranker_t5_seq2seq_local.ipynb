{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcN_5-RDWeqV"
      },
      "source": [
        "# Finetuning Ranker"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gratidão aos colegas Thiago Soares Laitz e Hugo (hugo@maritaca.ai) pelo apoio e código base fornecidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPNvc27A6WFj"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAoPEGKQpLAN",
        "outputId": "aee616ba-17b0-4ca3-b79c-8913b6135b62"
      },
      "source": [
        "pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWn-7w2WstVF",
        "outputId": "187954d7-71a0-4c8b-c23a-0351405d8984"
      },
      "source": [
        "pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XEO_Or8ul18",
        "outputId": "b3bbe372-e92c-4e18-ea3f-7ccb4571bed6"
      },
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umVQVnSBxnKT",
        "outputId": "f8629fba-4010-42c4-ceda-826280815d6c"
      },
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lld08iM0uzuL",
        "outputId": "3546aa0a-17d8-4211-8781-119438858ad7"
      },
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neptune rastro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import neptune.new as neptune "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0.16.18'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "neptune.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['NEPTUNE_ALLOW_SELF_SIGNED_CERTIFICATE'] = 'TRUE'\n",
        "os.environ['NEPTUNE_PROJECT'] = 'marcusborela/IA386DD'\n",
        "os.environ['NEPTUNE_API_TOKEN'] = getpass.getpass('Informe NEPTUNE_API_TOKEN')\n",
        "\n",
        "tag_contexto_rastro = 'INDIR_PTT5'\n",
        "neptune_version = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def converte_optimizer_state_dict(parm_optimizer)-> dict:\n",
        "  \"\"\"\n",
        "    Recebe um objeto \"parm_optimizer\" que é do tipo \"torch.optim.Optimizer\" e retorna um dicionário \n",
        "    com informações sobre o otimizador.\n",
        "\n",
        "    O dicionário de retorno é gerado a partir do estado do otimizador que é extraído da propriedade\n",
        "    \"state_dict()\" do objeto \"parm_optimizer\", seu primeiro grupo de parâmetros do otimizador.\n",
        "  \"\"\"\n",
        "  # return str(hparam['optimizer'])\n",
        "  return parm_optimizer.state_dict()['param_groups'][0]\n",
        "if neptune_version == 0:\n",
        "  import neptune.new as neptune  \n",
        "  class NeptuneRastroRun():\n",
        "      se_geracao_rastro = True \n",
        "      neptune_project = \"\"\n",
        "      tag_contexto_rastro = \"\"\n",
        "      neptune_api_token = \"\"\n",
        "\n",
        "      def __init__(self, parm_params:dict,  parm_lista_tag:list = None):\n",
        "        # print(f\"NeptuneRastroRun.init: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_params `{parm_params} \")\n",
        "        if self.__class__.se_geracao_rastro:      \n",
        "          self.run_neptune = neptune.init(project=self.__class__.neptune_project, api_token=self.__class__.neptune_api_token, capture_hardware_metrics=True)\n",
        "          self.run_neptune['sys/name'] = self.__class__.tag_contexto_rastro\n",
        "          vparams = copy.deepcopy(parm_params)\n",
        "          if \"optimizer\" in vparams:\n",
        "            vparams[\"optimizer\"] = converte_optimizer_state_dict(vparams[\"optimizer\"])\n",
        "          if 'criterion'  in vparams:\n",
        "            vparams[\"criterion\"] = str(vparams[\"criterion\"])\n",
        "          if 'scheduler'  in vparams:\n",
        "            vparams[\"scheduler\"] = str(type(vparams[\"scheduler\"]))\n",
        "          if 'device' in vparams:\n",
        "            vparams['device'] = str(vparams[\"device\"])\n",
        "          self.device = vparams[\"device\"]\n",
        "          for tag in parm_lista_tag:\n",
        "            self.run_neptune['sys/tags'].add(tag)\n",
        "          self.run_neptune['parameters'] = vparams\n",
        "          self.tmpDir = tempfile.mkdtemp()\n",
        "\n",
        "      @property\n",
        "      def run():\n",
        "        return self.run_neptune\n",
        "\n",
        "      @classmethod\n",
        "      def ativa_geracao_rastro(cls):\n",
        "        cls.se_geracao_rastro = True      \n",
        "\n",
        "      @classmethod\n",
        "      def def_contexto(cls):\n",
        "        cls.se_geracao_rastro = True      \n",
        "\n",
        "      @classmethod\n",
        "      def desativa_geracao_rastro(cls):\n",
        "        cls.se_geracao_rastro = False      \n",
        "\n",
        "      @classmethod\n",
        "      def retorna_status_geracao_rastro(cls):\n",
        "        return cls.se_geracao_rastro      \n",
        "\n",
        "      @classmethod\n",
        "      def retorna_tag_contexto_rastro(cls):\n",
        "        return cls.tag_contexto_rastro \n",
        "\n",
        "      @classmethod\n",
        "      def inicia_contexto(cls, neptune_project, tag_contexto_rastro, neptune_api_token):\n",
        "        assert '.' not in tag_contexto_rastro, \"NeptuneRastroRun.init(): tag_contexto_rastro não pode possuir ponto, pois será usado para gravar nome de arquivo\"      \n",
        "        cls.neptune_api_token = neptune_api_token\n",
        "        cls.tag_contexto_rastro = tag_contexto_rastro\n",
        "        cls.neptune_project = neptune_project\n",
        "\n",
        "      def salva_metrica(self, parm_metricas={}):\n",
        "        #print(f\"NeptuneRastroRun.salva_metrica: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_metricas:{parm_metricas} \")\n",
        "        if self.__class__.se_geracao_rastro:\n",
        "          for metrica, valor in parm_metricas.items(): \n",
        "            self.run_neptune[metrica].log(valor)\n",
        "  \n",
        "      def gera_grafico_modelo(self, loader_train, model):\n",
        "        if self.__class__.se_geracao_rastro: \n",
        "          # efetuar um forward \n",
        "          \"\"\"\n",
        "          se dataloader devolver x e y:\n",
        "          \"\"\"\n",
        "          x_, y_ = next(iter(loader_train))\n",
        "          x_ = x_.to(self.device)\n",
        "          outputs = model(x_)\n",
        "          \"\"\"\n",
        "          # se dataloader devolver dict:\n",
        "          dados_ = next(iter(loader_train))\n",
        "          outputs = model(dados_['x'].to(self.device))\n",
        "          #outputs = model(x_['input_ids'].to(self.device), x_['attention_mask'].to(self.device))\n",
        "          \"\"\"\n",
        "          nome_arquivo = os.path.join(self.tmpDir, \"modelo \"+ self.__class__.tag_contexto_rastro + time.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
        "          make_dot(outputs, params=dict(model.named_parameters()), show_attrs=True, show_saved=True).render(nome_arquivo, format=\"png\")\n",
        "          self.run_neptune[\"parameters/model_graph\"].upload(nome_arquivo+'.png')\n",
        "          self.run_neptune['parameters/model'] = re.sub('<bound method Module.state_dict of ', '',str(model.state_dict))      \n",
        "\n",
        "\n",
        "\n",
        "      def stop(self):\n",
        "        if self.__class__.se_geracao_rastro:         \n",
        "          self.run_neptune.stop()\n",
        "\n",
        "if neptune_version == 1:\n",
        "  import neptune\n",
        "  class NeptuneRastroRun():\n",
        "      \"\"\"\n",
        "        Classe para geração de rastro de experimento utilizando a ferramenta Neptune.\n",
        "\n",
        "        Busca implementar o rastro proposto em [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664),\n",
        "        autores Marcus Vinícius Borela de Castro e Remis Balaniuk, com o apoio da [solução Neptune](https://app.neptune.ai/)\n",
        "\n",
        "        Attributes:\n",
        "        -----------\n",
        "        se_geracao_rastro : bool\n",
        "            Indica se deve ser gerado rastro de experimento. \n",
        "        neptune_project : str\n",
        "            Nome do projeto criado no Neptune. \n",
        "        tag_contexto_rastro : str\n",
        "            Nome da tag utilizada para identificar o experimento.\n",
        "        neptune_api_token : str\n",
        "            Token utilizado para autenticação na API do Neptune. \n",
        "        run_neptune : object\n",
        "            Objeto que representa o experimento no Neptune.\n",
        "        device : str\n",
        "            Dispositivo utilizado para o treinamento do modelo.\n",
        "        tmpDir : str\n",
        "          Diretório temporário utilizado para salvar gráfico do modelo.          \n",
        "      \"\"\"\n",
        "      se_geracao_rastro = True \n",
        "      neptune_project = \"\"\n",
        "      tag_contexto_rastro = \"\"\n",
        "      neptune_api_token = \"\"\n",
        "\n",
        "      def __init__(self, parm_params:dict,  parm_lista_tag:list = None):\n",
        "        \"\"\"\n",
        "          Método construtor da classe NeptuneRastroRun.\n",
        "          \n",
        "          Args:\n",
        "          - parm_params: dicionário contendo os parâmetros do modelo.\n",
        "          - parm_lista_tag: lista contendo tags adicionais para o experimento.\n",
        "        \"\"\"      \n",
        "        # print(f\"NeptuneRastroRun.init: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_params `{parm_params} \")\n",
        "        if self.__class__.se_geracao_rastro:      \n",
        "          self.run_neptune = neptune.init_run(project=self.__class__.neptune_project, api_token=self.__class__.neptune_api_token, capture_hardware_metrics=True)\n",
        "          self.run_neptune['sys/name'] = self.__class__.tag_contexto_rastro\n",
        "          vparams = copy.deepcopy(parm_params)\n",
        "          if \"optimizer\" in vparams:\n",
        "            vparams[\"optimizer\"] = converte_optimizer_state_dict(vparams[\"optimizer\"])\n",
        "          if 'criterion'  in vparams:\n",
        "            vparams[\"criterion\"] = str(vparams[\"criterion\"])\n",
        "          if 'scheduler'  in vparams:\n",
        "            vparams[\"scheduler\"] = str(type(vparams[\"scheduler\"]))\n",
        "          if 'device' in vparams:\n",
        "            vparams['device'] = str(vparams[\"device\"])\n",
        "          self.device = vparams[\"device\"]\n",
        "          for tag in parm_lista_tag:\n",
        "            self.run_neptune['sys/tags'].add(tag)\n",
        "          self.run_neptune['parameters'] = vparams\n",
        "          # self.tmpDir = tempfile.mkdtemp()\n",
        "\n",
        "      @property\n",
        "      def run():\n",
        "        \"\"\"\n",
        "        Retorna a instância do objeto run_neptune.\n",
        "        \"\"\"      \n",
        "        return self.run_neptune\n",
        "\n",
        "      @classmethod\n",
        "      def ativa_geracao_rastro(cls):\n",
        "        \"\"\"\n",
        "        Ativa a geração de rastro.\n",
        "        \"\"\"      \n",
        "        cls.se_geracao_rastro = True      \n",
        "\n",
        "      @classmethod\n",
        "      def def_contexto(cls):\n",
        "        \"\"\"\n",
        "        Define o contexto para a geração de rastro.\n",
        "        \"\"\"      \n",
        "        cls.se_geracao_rastro = True      \n",
        "\n",
        "      @classmethod\n",
        "      def desativa_geracao_rastro(cls):\n",
        "        \"\"\"\n",
        "        Desativa a geração de rastro.\n",
        "        \"\"\"      \n",
        "        cls.se_geracao_rastro = False      \n",
        "\n",
        "      @classmethod\n",
        "      def retorna_status_geracao_rastro(cls):\n",
        "        \"\"\"\n",
        "          Retorna o status da geração de rastro.\n",
        "          \n",
        "          Returns:\n",
        "          - True se a geração de rastro está ativada, False caso contrário.\n",
        "        \"\"\"      \n",
        "        return cls.se_geracao_rastro      \n",
        "\n",
        "      @classmethod\n",
        "      def retorna_tag_contexto_rastro(cls):\n",
        "        \"\"\"\n",
        "          Retorna a tag do contexto de rastro.\n",
        "        \"\"\"      \n",
        "        return cls.tag_contexto_rastro \n",
        "\n",
        "      @classmethod\n",
        "      def inicia_contexto(cls, neptune_project, tag_contexto_rastro, neptune_api_token):\n",
        "        \"\"\"\n",
        "        Inicia o contexto de execução no Neptune.\n",
        "\n",
        "        Args:\n",
        "            neptune_project (str): Nome do projeto no Neptune.\n",
        "            tag_contexto_rastro (str): Tag que identifica o contexto de execução no Neptune.\n",
        "            neptune_api_token (str): Token de acesso à API do Neptune.\n",
        "\n",
        "        Raises:\n",
        "            AssertionError: Caso a tag_contexto_rastro possua um ponto (.), \n",
        "              o que pode gerar erros na gravação de arquivo.\n",
        "        \"\"\"      \n",
        "        assert '.' not in tag_contexto_rastro, \"NeptuneRastroRun.init(): tag_contexto_rastro não pode possuir ponto, pois será usado para gravar nome de arquivo\"      \n",
        "        cls.neptune_api_token = neptune_api_token\n",
        "        cls.tag_contexto_rastro = tag_contexto_rastro\n",
        "        cls.neptune_project = neptune_project\n",
        "\n",
        "      def salva_metrica(self, parm_metricas={}):\n",
        "        \"\"\"\n",
        "          Salva as métricas no Neptune Run caso a geração de rastro esteja ativa.\n",
        "\n",
        "          Parameters\n",
        "          ----------\n",
        "          parm_metricas: dict\n",
        "              Dicionário contendo as métricas a serem salvas. As chaves devem ser os nomes das métricas e os valores devem ser\n",
        "              os valores das métricas.\n",
        "        \"\"\"\n",
        "        #print(f\"NeptuneRastroRun.salva_metrica: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_metricas:{parm_metricas} \")\n",
        "        if self.__class__.se_geracao_rastro:\n",
        "          for metrica, valor in parm_metricas.items(): \n",
        "            self.run_neptune[metrica].append(valor)\n",
        "  \n",
        "      def gera_grafico_modelo(self, loader_train, model):\n",
        "        \"\"\"\n",
        "          Gera um gráfico do modelo e o envia para o Neptune. \n",
        "          Para gerar o gráfico, um forward pass é realizado em um batch de exemplos \n",
        "          de treino e o resultado é renderizado como um gráfico de nós conectados. \n",
        "          O gráfico é salvo em um arquivo .png e enviado para o Neptune como um arquivo anexo.\n",
        "\n",
        "          Args:\n",
        "              loader_train (torch.utils.data.DataLoader): DataLoader do conjunto de treinamento.\n",
        "              model (torch.nn.Module): Modelo a ser visualizado.\n",
        "          \n",
        "          Pendente:\n",
        "            Evolui para usar from io import StringIO (buffer = io.StringIO()) ao invés de tempdir \n",
        "        \"\"\"    \n",
        "        return\n",
        "\n",
        "        \"\"\"\n",
        "        falta ajustar make_dot\n",
        "        if self.__class__.se_geracao_rastro: \n",
        "          # efetuar um forward \n",
        "          batch = next(iter(loader_train))\n",
        "          # falta generalizar linha abaixo. Criar função que recebe modelo e batch como parâmetro?\n",
        "          outputs = model(input_ids=batch['input_ids'].to(hparam['device']), attention_mask=batch['attention_mask'].to(hparam['device']), token_type_ids=batch['token_type_ids'].to(hparam['device']), labels=batch['labels'].to(hparam['device']))\n",
        "          nome_arquivo = os.path.join(self.tmpDir, \"modelo \"+ self.__class__.tag_contexto_rastro + time.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
        "          make_dot(outputs, params=dict(model.named_parameters()), show_attrs=True, show_saved=True).render(nome_arquivo, format=\"png\")\n",
        "          self.run_neptune[\"parameters/model_graph\"].upload(nome_arquivo+'.png')\n",
        "          self.run_neptune['parameters/model'] = re.sub('<bound method Module.state_dict of ', '',str(model.state_dict))      \n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "      def stop(self):\n",
        "        \"\"\"\n",
        "          Para a execução do objeto Neptune. Todos os experimentos do Neptune são sincronizados com o servidor, e nenhum outro \n",
        "          experimento poderá ser adicionado a este objeto após a chamada a este método.\n",
        "        \"\"\"\n",
        "        if self.__class__.se_geracao_rastro:         \n",
        "          self.run_neptune.stop()\n",
        "\n",
        "### Definindo parâmetros para o rastro\n",
        "\n",
        "\n",
        "NeptuneRastroRun.inicia_contexto(os.environ['NEPTUNE_PROJECT'], tag_contexto_rastro,  os.environ['NEPTUNE_API_TOKEN'])\n",
        "#NeptuneRastroRun.desativa_geracao_rastro()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers.integrations import NeptuneCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhMngMsq6ZIV"
      },
      "source": [
        "# Infra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD6YYInN6G1u"
      },
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hUm-kyDBEHEL"
      },
      "outputs": [],
      "source": [
        "PATH_LOCAL_DATA = '../..'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b3f4qXjB6HMm"
      },
      "outputs": [],
      "source": [
        "PATH_TRAIN_MODEL_LOCAL = f\"{PATH_LOCAL_DATA}/model/train/ptt5-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3KdgJPyrEBzr"
      },
      "outputs": [],
      "source": [
        "# path_data = '/content/drive/MyDrive/treinamento/202301_IA368DD/indir/data/train_data_juris_tcu_index_bm25.csv'\n",
        "\n",
        "# PATH_TRAIN_DATA_ZIP = f\"{PATH_LOCAL_DATA}/data/train_data_juris_tcu_index.zip\"\n",
        "PATH_TRAIN_DATA = f\"{PATH_LOCAL_DATA}/data/train_juris_tcu_index/train_data_juris_tcu_index.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrF4PpSvlrcU",
        "outputId": "d385afdd-3dfc-4874-80e0-d397880266fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.exists(PATH_TRAIN_MODEL_LOCAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrF4PpSvlrcU",
        "outputId": "d385afdd-3dfc-4874-80e0-d397880266fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.exists(PATH_TRAIN_DATA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNOEKGtZ55uU"
      },
      "source": [
        "## Função de verificação de memória"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2C0W0RMyx2yu"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_pq59lAHke",
        "outputId": "ce6828a6-8ce4-405f-f534-ebaf820f9d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Jul  5 19:28:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.116.03   Driver Version: 525.116.03   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
            "|  0%   48C    P8    26W / 370W |     58MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1223      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell                9MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9XgIWvkkH-kn"
      },
      "outputs": [],
      "source": [
        "def mostra_memoria(lista_mem=['cpu']):\n",
        "  \"\"\"\n",
        "  Esta função exibe informações de memória da CPU e/ou GPU, conforme parâmetros fornecidos.\n",
        "\n",
        "  Parâmetros:\n",
        "  -----------\n",
        "  lista_mem : list, opcional\n",
        "      Lista com strings 'cpu' e/ou 'gpu'.\n",
        "      'cpu' - exibe informações de memória da CPU.\n",
        "      'gpu' - exibe informações de memória da GPU (se disponível).\n",
        "      O valor padrão é ['cpu'].\n",
        "\n",
        "  Saída:\n",
        "  -------\n",
        "  A função não retorna nada, apenas exibe as informações na tela.\n",
        "\n",
        "  Exemplo de uso:\n",
        "  ---------------\n",
        "  Para exibir informações de memória da CPU:\n",
        "      mostra_memoria(['cpu'])\n",
        "\n",
        "  Para exibir informações de memória da CPU e GPU:\n",
        "      mostra_memoria(['cpu', 'gpu'])\n",
        "\n",
        "  Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "  \"\"\"\n",
        "  if 'cpu' in lista_mem:\n",
        "    vm = virtual_memory()\n",
        "    ram={}\n",
        "    ram['total']=round(vm.total / 1e9,2)\n",
        "    ram['available']=round(virtual_memory().available / 1e9,2)\n",
        "    # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
        "    ram['used']=round(virtual_memory().used / 1e9,2)\n",
        "    ram['free']=round(virtual_memory().free / 1e9,2)\n",
        "    ram['active']=round(virtual_memory().active / 1e9,2)\n",
        "    ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
        "    ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
        "    ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
        "    print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n",
        "    print('/nGPU')\n",
        "    gpu_info = !nvidia-smi\n",
        "  if 'gpu' in lista_mem:\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dri9iiMAvCT",
        "outputId": "174c2c6f-da21-4936-9fcc-4ecd0151966a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime RAM in gb: \n",
            " total 67.35\n",
            " available 55.33\n",
            " used 10.97\n",
            " free 19.13\n",
            " cached 35.27\n",
            " buffers 1.97\n",
            "/nGPU\n",
            "Wed Jul  5 19:28:13 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.116.03   Driver Version: 525.116.03   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
            "|  0%   49C    P8    37W / 370W |     58MiB / 24576MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1223      G   /usr/lib/xorg/Xorg                 46MiB |\n",
            "|    0   N/A  N/A      1366      G   /usr/bin/gnome-shell                9MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "mostra_memoria(['cpu','gpu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpPSgRGQ5wJv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aGDjEcJ_bawK"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MV8a69JaiEn9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TfI-DLjRbTzF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CB1vojvvlt0v"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f2b7k0tOuv_L"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZYqAt-NimJH5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    MT5Tokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import Dataset\n",
        "from dataclasses import dataclass, field\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v2gtkEPhA0t"
      },
      "source": [
        "## Preparando para debug e display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wQ5pmlOHxHhk"
      },
      "outputs": [],
      "source": [
        "def config_display():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de display do Pandas.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configurando formato saída Pandas\n",
        "  # define o número máximo de colunas que serão exibidas\n",
        "  pd.options.display.max_columns = None\n",
        "\n",
        "  # define a largura máxima de uma linha\n",
        "  pd.options.display.width = 1000\n",
        "\n",
        "  # define o número máximo de linhas que serão exibidas\n",
        "  pd.options.display.max_rows = 100\n",
        "\n",
        "  # define o número máximo de caracteres por coluna\n",
        "  pd.options.display.max_colwidth = 50\n",
        "\n",
        "  # se deve exibir o número de linhas e colunas de um DataFrame.\n",
        "  pd.options.display.show_dimensions = True\n",
        "\n",
        "  # número de dígitos após a vírgula decimal a serem exibidos para floats.\n",
        "  pd.options.display.precision = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "b2tDy72ATNHs"
      },
      "outputs": [],
      "source": [
        "def config_debug():\n",
        "  \"\"\"\n",
        "  Esta função configura as opções de debug do PyTorch e dos pacotes\n",
        "  transformers e datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define opções de impressão de tensores para o modo científico\n",
        "  torch.set_printoptions(sci_mode=True)\n",
        "  \"\"\"\n",
        "    Significa que valores muito grandes ou muito pequenos são mostrados em notação científica.\n",
        "    Por exemplo, em vez de imprimir o número 0.0000012345 como 0.0000012345,\n",
        "    ele seria impresso como 1.2345e-06. Isso é útil em situações em que os valores dos tensores\n",
        "    envolvidos nas operações são muito grandes ou pequenos, e a notação científica permite\n",
        "    uma melhor compreensão dos números envolvidos.\n",
        "  \"\"\"\n",
        "\n",
        "  # Habilita detecção de anomalias no autograd do PyTorch\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "  \"\"\"\n",
        "    Permite identificar operações que podem causar problemas de estabilidade numérica,\n",
        "    como gradientes explodindo ou desaparecendo. Quando essa opção é ativada,\n",
        "    o PyTorch verifica se há operações que geram valores NaN ou infinitos nos tensores\n",
        "    envolvidos no cálculo do gradiente. Se for detectado um valor anômalo, o PyTorch\n",
        "    interrompe a execução e gera uma exceção, permitindo que o erro seja corrigido\n",
        "    antes que se torne um problema maior.\n",
        "\n",
        "    É importante notar que a detecção de anomalias pode ter um impacto significativo\n",
        "    no desempenho, especialmente em modelos grandes e complexos. Por esse motivo,\n",
        "    ela deve ser usada com cautela e apenas para depuração.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configura variável de ambiente para habilitar a execução síncrona (bloqueante) das chamadas da API do CUDA.\n",
        "  os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "  \"\"\"\n",
        "    o Python aguarda o término da execução de uma chamada da API do CUDA antes de executar a próxima chamada.\n",
        "    Isso é útil para depurar erros no código que envolve operações na GPU, pois permite que o erro seja capturado\n",
        "    no momento em que ocorre, e não depois de uma sequência de operações que pode tornar a origem do erro mais difícil de determinar.\n",
        "    No entanto, é importante lembrar que esse modo de execução é significativamente mais lento do que a execução assíncrona,\n",
        "    que é o comportamento padrão do CUDA. Por isso, é recomendado utilizar esse comando apenas em situações de depuração\n",
        "    e removê-lo após a solução do problema.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define o nível de verbosity do pacote transformers para info\n",
        "  # transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "    Define o nível de detalhamento das mensagens de log geradas pela biblioteca Hugging Face Transformers\n",
        "    para o nível info. Isso significa que a biblioteca irá imprimir mensagens de log informativas sobre\n",
        "    o andamento da execução, tais como tempo de execução, tamanho de batches, etc.\n",
        "\n",
        "    Essas informações podem ser úteis para entender o que está acontecendo durante a execução da tarefa\n",
        "    e auxiliar no processo de debug. É importante notar que, em alguns casos, a quantidade de informações\n",
        "    geradas pode ser muito grande, o que pode afetar o desempenho do sistema e dificultar a visualização\n",
        "    das informações relevantes. Por isso, é importante ajustar o nível de detalhamento de acordo com a\n",
        "    necessidade de cada tarefa.\n",
        "\n",
        "    Caso queira reduzir a quantidade de mensagens, comentar a linha acima e\n",
        "      descomentar as duas linhas abaixo, para definir o nível de verbosity como error ou warning\n",
        "\n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_warning()\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Define o modo verbose do xmode, que é utilizado no debug\n",
        "  # %xmode Verbose\n",
        "\n",
        "  \"\"\"\n",
        "    Comando usado no Jupyter Notebook para controlar o modo de exibição das informações de exceções.\n",
        "    O modo verbose é um modo detalhado que exibe informações adicionais ao imprimir as exceções.\n",
        "    Ele inclui as informações de pilha de chamadas completa e valores de variáveis locais e globais\n",
        "    no momento da exceção. Isso pode ser útil para depurar e encontrar a causa de exceções em seu código.\n",
        "    Ao usar %xmode Verbose, as informações de exceção serão impressas com mais detalhes e informações adicionais serão incluídas.\n",
        "\n",
        "    Caso queira desabilitar o modo verbose e utilizar o modo plain,\n",
        "    comentar a linha acima e descomentar a linha abaixo:\n",
        "    %xmode Plain\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "    Dica:\n",
        "    1.  pdb (Python Debugger)\n",
        "      Quando ocorre uma exceção em uma parte do código, o programa para a execução e exibe uma mensagem de erro\n",
        "      com informações sobre a exceção, como a linha do código em que ocorreu o erro e o tipo da exceção.\n",
        "\n",
        "      Se você estiver depurando o código e quiser examinar o estado das variáveis ​​e executar outras operações\n",
        "      no momento em que a exceção ocorreu, pode usar o pdb (Python Debugger). Para isso, é preciso colocar o comando %debug\n",
        "      logo após ocorrer a exceção. Isso fará com que o programa pare na linha em que ocorreu a exceção e abra o pdb,\n",
        "      permitindo que você explore o estado das variáveis, examine a pilha de chamadas e execute outras operações para depurar o código.\n",
        "\n",
        "\n",
        "    2. ipdb\n",
        "      O ipdb é um depurador interativo para o Python que oferece recursos mais avançados do que o pdb,\n",
        "      incluindo a capacidade de navegar pelo código fonte enquanto depura.\n",
        "\n",
        "      Você pode começar a depurar seu código inserindo o comando ipdb.set_trace() em qualquer lugar do\n",
        "      seu código onde deseja pausar a execução e começar a depurar. Quando a execução chegar nessa linha,\n",
        "      o depurador entrará em ação, permitindo que você examine o estado atual do seu programa e execute\n",
        "      comandos para investigar o comportamento.\n",
        "\n",
        "      Durante a depuração, você pode usar comandos:\n",
        "        next (para executar a próxima linha de código),\n",
        "        step (para entrar em uma função chamada na próxima linha de código)\n",
        "        continue (para continuar a execução normalmente até o próximo ponto de interrupção).\n",
        "\n",
        "      Ao contrário do pdb, o ipdb é um depurador interativo que permite navegar pelo código fonte em que\n",
        "      está trabalhando enquanto depura, permitindo que você inspecione variáveis, defina pontos de interrupção\n",
        "      adicionais e até mesmo execute expressões Python no contexto do seu programa.\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Tb4aqtcExR84"
      },
      "outputs": [],
      "source": [
        "config_display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-5Bq4043fkfh"
      },
      "outputs": [],
      "source": [
        "config_debug()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GYGL4MV_yhQ",
        "outputId": "005fc71c-7b47-4acb-baa8-db818de19b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: /home/borela/fontes/ind-ir/code/train\n"
          ]
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "print(\"Current directory:\", current_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPlzXHONq9BC"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fEod2-Fjl6fB"
      },
      "outputs": [],
      "source": [
        "TOKEN_FALSE = '▁não'\n",
        "TOKEN_TRUE = '▁sim'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wc5FqYFQrNrE"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'unicamp-dl/ptt5-base-pt-msmarco-100k-v2'\n",
        "# 'unicamp-dl/mt5-3B-mmarco-en-pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMrTcbJN55ue"
      },
      "source": [
        "# Carga dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "r0igqAcT55ue"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(PATH_TRAIN_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGRYa-JT55ue",
        "outputId": "1c815909-6770-4143-9f85-fcce23e28fe2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(402738, 7)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape\n",
        "# lim 100(111852, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ3S90BBTrKJ"
      },
      "source": [
        "Verificando correção do arquivo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm7ss2BfTlOu",
        "outputId": "4d2c9497-e1c8-4a82-af23-3ba55bc4b79d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUERY_ID      0\n",
            "DOC_ID        0\n",
            "RELEVANCE     0\n",
            "SCORE         0\n",
            "TYPE          0\n",
            "DOC_TEXT      0\n",
            "QUERY_TEXT    0\n",
            "Length: 7, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "kxK8j_na55uh",
        "outputId": "48c933cd-ea22-4df7-c448-a2f3dd194fa4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QUERY_TEXT</th>\n",
              "      <th>DOC_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>402738.0000000</td>\n",
              "      <td>402738.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>322.8252313</td>\n",
              "      <td>830.6957451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>165.8299958</td>\n",
              "      <td>398.1844365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>41.0000000</td>\n",
              "      <td>86.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>217.0000000</td>\n",
              "      <td>572.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>294.0000000</td>\n",
              "      <td>759.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>391.0000000</td>\n",
              "      <td>1020.0000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4212.0000000</td>\n",
              "      <td>3739.0000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           QUERY_TEXT        DOC_TEXT\n",
              "count  402738.0000000  402738.0000000\n",
              "mean      322.8252313     830.6957451\n",
              "std       165.8299958     398.1844365\n",
              "min        41.0000000      86.0000000\n",
              "25%       217.0000000     572.0000000\n",
              "50%       294.0000000     759.0000000\n",
              "75%       391.0000000    1020.0000000\n",
              "max      4212.0000000    3739.0000000\n",
              "\n",
              "[8 rows x 2 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[['QUERY_TEXT','DOC_TEXT']].applymap(len).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBvMP8ma55uh"
      },
      "source": [
        "Para cada positivo, tem 5 negativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH49RPhY55uh",
        "outputId": "68651bd7-dabd-4cce-dfae-b90cbe863842"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    402738.0000000\n",
              "mean          0.1666667\n",
              "std           0.3726785\n",
              "min           0.0000000\n",
              "25%           0.0000000\n",
              "50%           0.0000000\n",
              "75%           0.0000000\n",
              "max           1.0000000\n",
              "Name: RELEVANCE, Length: 8, dtype: float64"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['RELEVANCE'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "9hx3q_TOy6QB",
        "outputId": "81137dcc-1021-4000-c161-395660ce6a6d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QUERY_ID</th>\n",
              "      <th>DOC_ID</th>\n",
              "      <th>RELEVANCE</th>\n",
              "      <th>SCORE</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>DOC_TEXT</th>\n",
              "      <th>QUERY_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151655</td>\n",
              "      <td>1943</td>\n",
              "      <td>1</td>\n",
              "      <td>0.897</td>\n",
              "      <td>TEMA</td>\n",
              "      <td>O termo é \"Agente público\".\\nAgente público te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>151655</td>\n",
              "      <td>15441</td>\n",
              "      <td>0</td>\n",
              "      <td>0.732</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>151655</td>\n",
              "      <td>6373</td>\n",
              "      <td>0</td>\n",
              "      <td>0.717</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Exercício financeiro anterior\".\\nEx...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151655</td>\n",
              "      <td>6973</td>\n",
              "      <td>0</td>\n",
              "      <td>0.680</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"CJF\".\\nCJF é classificado como uma ...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>151655</td>\n",
              "      <td>7201</td>\n",
              "      <td>0</td>\n",
              "      <td>0.751</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Embratur\".\\nEmbratur é classificado...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   QUERY_ID  DOC_ID  RELEVANCE  SCORE                               TYPE                                           DOC_TEXT                                         QUERY_TEXT\n",
              "0    151655    1943          1  0.897                               TEMA  O termo é \"Agente público\".\\nAgente público te...  O dever de observância à hierarquia militar nã...\n",
              "1    151655   15441          0  0.732  relevant:TEMA, not relevant:TOTAL  O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...  O dever de observância à hierarquia militar nã...\n",
              "2    151655    6373          0  0.717  relevant:TEMA, not relevant:TOTAL  O termo é \"Exercício financeiro anterior\".\\nEx...  O dever de observância à hierarquia militar nã...\n",
              "3    151655    6973          0  0.680  relevant:TEMA, not relevant:TOTAL  O termo é \"CJF\".\\nCJF é classificado como uma ...  O dever de observância à hierarquia militar nã...\n",
              "4    151655    7201          0  0.751  relevant:TEMA, not relevant:TOTAL  O termo é \"Embratur\".\\nEmbratur é classificado...  O dever de observância à hierarquia militar nã...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "wN-H7_d54tPY"
      },
      "outputs": [],
      "source": [
        "df[\"label\"] = [TOKEN_FALSE if relevance == 0 else TOKEN_TRUE for relevance in df[\"RELEVANCE\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "aEHO3ZjL42tA",
        "outputId": "4c00de02-3d7d-4c6d-e585-9bb7d7c9087f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QUERY_ID</th>\n",
              "      <th>DOC_ID</th>\n",
              "      <th>RELEVANCE</th>\n",
              "      <th>SCORE</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>DOC_TEXT</th>\n",
              "      <th>QUERY_TEXT</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151655</td>\n",
              "      <td>1943</td>\n",
              "      <td>1</td>\n",
              "      <td>0.897</td>\n",
              "      <td>TEMA</td>\n",
              "      <td>O termo é \"Agente público\".\\nAgente público te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁sim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>151655</td>\n",
              "      <td>15441</td>\n",
              "      <td>0</td>\n",
              "      <td>0.732</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>151655</td>\n",
              "      <td>6373</td>\n",
              "      <td>0</td>\n",
              "      <td>0.717</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Exercício financeiro anterior\".\\nEx...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151655</td>\n",
              "      <td>6973</td>\n",
              "      <td>0</td>\n",
              "      <td>0.680</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"CJF\".\\nCJF é classificado como uma ...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>151655</td>\n",
              "      <td>7201</td>\n",
              "      <td>0</td>\n",
              "      <td>0.751</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Embratur\".\\nEmbratur é classificado...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   QUERY_ID  DOC_ID  RELEVANCE  SCORE                               TYPE                                           DOC_TEXT                                         QUERY_TEXT label\n",
              "0    151655    1943          1  0.897                               TEMA  O termo é \"Agente público\".\\nAgente público te...  O dever de observância à hierarquia militar nã...  ▁sim\n",
              "1    151655   15441          0  0.732  relevant:TEMA, not relevant:TOTAL  O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "2    151655    6373          0  0.717  relevant:TEMA, not relevant:TOTAL  O termo é \"Exercício financeiro anterior\".\\nEx...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "3    151655    6973          0  0.680  relevant:TEMA, not relevant:TOTAL  O termo é \"CJF\".\\nCJF é classificado como uma ...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "4    151655    7201          0  0.751  relevant:TEMA, not relevant:TOTAL  O termo é \"Embratur\".\\nEmbratur é classificado...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YwZra1Tu2j92"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'DOC_TEXT': 'text', 'QUERY_TEXT':'query'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "L6VTWWEI23LA",
        "outputId": "990a54d7-acbf-4633-e5ea-008dfc668e39"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QUERY_ID</th>\n",
              "      <th>DOC_ID</th>\n",
              "      <th>RELEVANCE</th>\n",
              "      <th>SCORE</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>text</th>\n",
              "      <th>query</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151655</td>\n",
              "      <td>1943</td>\n",
              "      <td>1</td>\n",
              "      <td>0.897</td>\n",
              "      <td>TEMA</td>\n",
              "      <td>O termo é \"Agente público\".\\nAgente público te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁sim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>151655</td>\n",
              "      <td>15441</td>\n",
              "      <td>0</td>\n",
              "      <td>0.732</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>151655</td>\n",
              "      <td>6373</td>\n",
              "      <td>0</td>\n",
              "      <td>0.717</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Exercício financeiro anterior\".\\nEx...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151655</td>\n",
              "      <td>6973</td>\n",
              "      <td>0</td>\n",
              "      <td>0.680</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"CJF\".\\nCJF é classificado como uma ...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>151655</td>\n",
              "      <td>7201</td>\n",
              "      <td>0</td>\n",
              "      <td>0.751</td>\n",
              "      <td>relevant:TEMA, not relevant:TOTAL</td>\n",
              "      <td>O termo é \"Embratur\".\\nEmbratur é classificado...</td>\n",
              "      <td>O dever de observância à hierarquia militar nã...</td>\n",
              "      <td>▁não</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   QUERY_ID  DOC_ID  RELEVANCE  SCORE                               TYPE                                               text                                              query label\n",
              "0    151655    1943          1  0.897                               TEMA  O termo é \"Agente público\".\\nAgente público te...  O dever de observância à hierarquia militar nã...  ▁sim\n",
              "1    151655   15441          0  0.732  relevant:TEMA, not relevant:TOTAL  O termo é \"Reforma-prêmio\".\\nReforma-prêmio te...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "2    151655    6373          0  0.717  relevant:TEMA, not relevant:TOTAL  O termo é \"Exercício financeiro anterior\".\\nEx...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "3    151655    6973          0  0.680  relevant:TEMA, not relevant:TOTAL  O termo é \"CJF\".\\nCJF é classificado como uma ...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "4    151655    7201          0  0.751  relevant:TEMA, not relevant:TOTAL  O termo é \"Embratur\".\\nEmbratur é classificado...  O dever de observância à hierarquia militar nã...  ▁não\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "byw95g1KzUP7"
      },
      "outputs": [],
      "source": [
        "df = df[['query', 'text', 'label']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF4iu3DMztPX",
        "outputId": "eb038f3c-16b2-4d9f-a137-42f741b02d60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(402738, 3)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itVdwg9tz8Rs",
        "outputId": "632d7cae-e688-4d32-8ae7-1f11933d7df5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "# ... código anterior ...\n",
        "\n",
        "# Liberar memória utilizando gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUDBUOWmyzrf"
      },
      "source": [
        "# Separating evaluation data and prepare dataset tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wH4Tx1cGy3HR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "s7qRU31pzNnT"
      },
      "outputs": [],
      "source": [
        "train_df, valid_df = train_test_split(df, test_size=0.01,\n",
        "                                      stratify=df['label'].values, random_state=123)\n",
        "# Definir os argumentos de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVFFx-bhzNj4",
        "outputId": "d5207702-eb8e-47b8-e5cd-269d0287c773"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((398710, 3), (4028, 3))"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.shape, valid_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "G6kwqr6Cz0X9",
        "outputId": "0c235ee1-cbb2-4b7d-a088-8de7865d7395"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>270246</th>\n",
              "      <td>Verificado sobrepreço em contrato de obra públ...</td>\n",
              "      <td>O termo é \"Fatura\".\\nFatura tem nota de escopo...</td>\n",
              "      <td>▁sim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106266</th>\n",
              "      <td>Faculta-se aos médicos do poder judiciário o e...</td>\n",
              "      <td>O termo é \"Médico\".\\nMédico tem definição: \"Aq...</td>\n",
              "      <td>▁sim</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    query                                               text label\n",
              "270246  Verificado sobrepreço em contrato de obra públ...  O termo é \"Fatura\".\\nFatura tem nota de escopo...  ▁sim\n",
              "106266  Faculta-se aos médicos do poder judiciário o e...  O termo é \"Médico\".\\nMédico tem definição: \"Aq...  ▁sim\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_df[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egkeJ5Lxz7O_",
        "outputId": "ff633ddd-6d46-4158-9fd1-1298b91f9023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array(['▁não', '▁sim'], dtype=object), array([332258,  66452])) \n",
            " (array(['▁não', '▁sim'], dtype=object), array([3357,  671]))\n"
          ]
        }
      ],
      "source": [
        "print(np.unique(train_df['label'], return_counts=True), '\\n', np.unique(valid_df['label'], return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RyDkS1BHz7M3"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df[[\"query\", \"text\", \"label\"]].reset_index(drop=True))\n",
        "valid_dataset = Dataset.from_pandas(valid_df[[\"query\", \"text\", \"label\"]].reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyhn6oozz7JZ",
        "outputId": "05ee54ad-2616-43e2-d86e-d955b9bfdeae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(398710, 4028)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset), len(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBMIhR4-1AsW",
        "outputId": "1d08ec30-04ec-4398-8f22-a0bf86f84686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'Verificado sobrepreço em contrato de obra pública, a Administração deve promover ajuste do valor apurado nas faturas vincendas. Não existindo saldo financeiro, deve providenciar a instauração da competente tomada de contas especial.',\n",
              " 'text': 'O termo é \"Fatura\".\\nFatura tem nota de escopo: \"É um documento onde está registrado um valor em debito com prazo de quitação da dívida seja parcelado ou à vista.\".\\nFatura tem termo relacionado: \"Duplicata\", \"Nota fiscal eletrônica\", \"Nota fiscal\" e \"Cartão de crédito\".\\nFatura tem tradução em espanhol: \"Factura\".\\nFatura tem tradução em inglês: \"Invoice\".',\n",
              " 'label': '▁sim'}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "zZVBeVB_0Onw"
      },
      "outputs": [],
      "source": [
        "del df, train_df, valid_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh-EzCpA0cqZ",
        "outputId": "70239a6d-0ab3-40aa-b7fb-01d96bdfe7a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ-rHK17f_42"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "0ULDoMS7rkXT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)okenizer_config.json: 100%|██████████| 1.98k/1.98k [00:00<00:00, 554kB/s]\n",
            "Downloading spiece.model: 100%|██████████| 756k/756k [00:00<00:00, 793kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 1.00MB/s]\n"
          ]
        }
      ],
      "source": [
        "# tokenizer = MT5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7JUJQQuW7bCA"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    queries_documents = [f\"Query: {query} Document: {text} Relevant:\" for query, text in zip(batch[\"query\"], batch[\"text\"])]\n",
        "    print(f\"Chamado tokenize len(queries_documents): {len(queries_documents)}\")\n",
        "    tokenized = tokenizer(\n",
        "        queries_documents,\n",
        "        padding=True, # \"max_length\",\n",
        "        truncation=True,\n",
        "        # return_tensors=\"pt\",\n",
        "        max_length= 512\n",
        "    )\n",
        "    # tokenized[\"labels\"] = [[label] for label in batch[\"label\"]]\n",
        "    # tokenized['label'] = [[token_false, token_true][int(pairs[\"label\"][i])]\n",
        "    tokenized[\"labels\"] = tokenizer(batch['label'])['input_ids']\n",
        "    # tokenized[\"labels\"] = [tokenizer.get_vocab()[token] for token in batch['label']]\n",
        "    # tokenized[\"labels\"] = [token_id_true if label == 'true' else token_id_false for label in batch[\"label\"]]\n",
        "    return tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108,
          "referenced_widgets": [
            "8363e87c191c4e27a55624d76592522e",
            "9ddd0ce214044ff4985779bc45a78075",
            "6ad3f253125f410bbedb811f3313b312",
            "80f9f8c628fc4eeeaead5bf16c880b94",
            "492c664c2bf84d98bd5372d20ed8f8d2",
            "4b258b1461cb4ebd95674bbe963da89b",
            "4d153ac235b643d9b9fb1f502f220845",
            "614aa774c3b44e02b377967e4ceea596",
            "cc8cb0b3f3ba4174bd7f4e3d5a6f8f46",
            "36a988374e504d9a997e8e48955d6bd2",
            "be972ca97d034c5fb5b616299d8dd29c"
          ]
        },
        "id": "YOaxhRfM3css",
        "outputId": "a061da2f-c993-4a46-92d1-4768017b44b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   0%|          | 0/4028 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  50%|████▉     | 2000/4028 [00:00<00:00, 3247.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 28\n",
            "CPU times: user 3.68 s, sys: 138 ms, total: 3.82 s\n",
            "Wall time: 1.06 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# valid_dataset.set_transform(tokenize)\n",
        "valid_dataset = valid_dataset.map(\n",
        "        tokenize,\n",
        "        remove_columns=('query', 'text', 'label'),\n",
        "        batched=True,\n",
        "        desc='Tokenizing',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7dec4e3e109040308c245ca52fc29e93",
            "4e5ea796f1384a3984a8f52720d3c445",
            "5c546b9f682444be932b17b2ff4c3409",
            "520b60a897d342648199509590bcb3c7",
            "f50b231ed5fa423e81400f13d72f77bf",
            "bc2c9da3baaa4d2b92aaf7a4ab74035e",
            "b849d25c533f4618afa11c027f3feaad",
            "6ce6ab38f4d4400cb7ba03d59d7a7b21",
            "1d1db4f6d75944ddbe4284da885e8038",
            "ee3f86de98ba49fda4fe387569bd85d8",
            "4e6e72fe001744c4830d22150b8e2b84"
          ]
        },
        "id": "S49luVi-5bFG",
        "outputId": "0ded5508-104b-4db0-c781-d9e1ecdd3b20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   0%|          | 1000/398710 [00:00<00:59, 6739.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   1%|          | 3000/398710 [00:00<00:56, 7036.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   1%|▏         | 5000/398710 [00:00<00:57, 6825.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   2%|▏         | 7000/398710 [00:01<00:58, 6728.33 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   2%|▏         | 9000/398710 [00:01<01:03, 6145.39 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   3%|▎         | 11000/398710 [00:01<00:59, 6563.53 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   3%|▎         | 13000/398710 [00:01<00:57, 6707.86 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   4%|▍         | 15000/398710 [00:02<00:55, 6853.75 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   4%|▍         | 17000/398710 [00:02<00:55, 6904.80 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   5%|▍         | 19000/398710 [00:02<00:59, 6405.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   5%|▌         | 21000/398710 [00:03<00:56, 6718.08 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   6%|▌         | 23000/398710 [00:03<00:55, 6810.92 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   6%|▋         | 25000/398710 [00:03<00:54, 6878.63 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   7%|▋         | 27000/398710 [00:04<00:55, 6718.76 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   7%|▋         | 28000/398710 [00:04<00:54, 6801.39 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   8%|▊         | 30000/398710 [00:04<00:58, 6289.07 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   8%|▊         | 32000/398710 [00:04<00:57, 6393.56 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   9%|▊         | 34000/398710 [00:05<00:54, 6724.92 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:   9%|▉         | 36000/398710 [00:05<00:53, 6782.30 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  10%|▉         | 38000/398710 [00:05<00:51, 6949.49 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  10%|█         | 40000/398710 [00:05<00:51, 6984.47 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  11%|█         | 42000/398710 [00:06<00:54, 6508.35 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  11%|█         | 44000/398710 [00:06<00:52, 6820.80 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  12%|█▏        | 46000/398710 [00:06<00:50, 7024.47 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  12%|█▏        | 48000/398710 [00:07<00:48, 7164.00 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  13%|█▎        | 50000/398710 [00:07<00:48, 7183.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  13%|█▎        | 52000/398710 [00:07<00:57, 6029.52 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  14%|█▎        | 54000/398710 [00:08<00:53, 6447.34 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  14%|█▍        | 56000/398710 [00:08<00:52, 6548.55 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  15%|█▍        | 58000/398710 [00:08<00:50, 6747.66 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  15%|█▌        | 60000/398710 [00:08<00:49, 6789.01 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  15%|█▌        | 61000/398710 [00:09<00:49, 6754.49 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  16%|█▌        | 63000/398710 [00:09<00:52, 6334.32 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  16%|█▋        | 65000/398710 [00:09<00:49, 6706.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  17%|█▋        | 67000/398710 [00:10<00:47, 6990.35 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  17%|█▋        | 69000/398710 [00:10<00:46, 7072.88 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  18%|█▊        | 71000/398710 [00:10<00:45, 7138.53 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  18%|█▊        | 73000/398710 [00:10<00:45, 7137.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  19%|█▉        | 75000/398710 [00:11<00:49, 6561.75 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  19%|█▉        | 77000/398710 [00:11<00:46, 6846.18 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  20%|█▉        | 79000/398710 [00:11<00:45, 6992.43 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  20%|██        | 81000/398710 [00:12<00:44, 7073.74 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  21%|██        | 83000/398710 [00:12<00:44, 7160.56 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  21%|██▏       | 85000/398710 [00:12<00:48, 6493.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  22%|██▏       | 87000/398710 [00:12<00:45, 6878.81 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  22%|██▏       | 89000/398710 [00:13<00:43, 7084.60 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  23%|██▎       | 91000/398710 [00:13<00:42, 7157.32 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  23%|██▎       | 93000/398710 [00:13<00:42, 7192.81 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  24%|██▍       | 95000/398710 [00:14<00:48, 6286.14 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  24%|██▍       | 97000/398710 [00:14<00:44, 6730.40 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  25%|██▍       | 99000/398710 [00:14<00:43, 6886.14 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  25%|██▌       | 101000/398710 [00:14<00:42, 6988.41 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  26%|██▌       | 103000/398710 [00:15<00:42, 7026.00 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  26%|██▋       | 105000/398710 [00:15<00:46, 6276.02 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  27%|██▋       | 107000/398710 [00:15<00:48, 6023.08 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  27%|██▋       | 109000/398710 [00:16<00:44, 6573.47 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  28%|██▊       | 111000/398710 [00:16<00:41, 6909.89 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  28%|██▊       | 113000/398710 [00:16<00:40, 7103.16 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  29%|██▉       | 115000/398710 [00:17<00:39, 7132.05 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  29%|██▉       | 117000/398710 [00:17<00:44, 6270.08 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  30%|██▉       | 119000/398710 [00:17<00:41, 6755.66 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  30%|███       | 121000/398710 [00:17<00:40, 6876.00 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  31%|███       | 123000/398710 [00:18<00:39, 7002.31 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  31%|███▏      | 125000/398710 [00:18<00:38, 7083.89 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  32%|███▏      | 127000/398710 [00:18<00:38, 7068.15 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  32%|███▏      | 129000/398710 [00:19<00:41, 6423.67 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  33%|███▎      | 131000/398710 [00:19<00:39, 6758.49 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  33%|███▎      | 133000/398710 [00:19<00:39, 6796.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  34%|███▍      | 135000/398710 [00:20<00:37, 6946.71 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  34%|███▍      | 137000/398710 [00:20<00:36, 7090.01 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  35%|███▍      | 139000/398710 [00:20<00:36, 7139.33 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  35%|███▌      | 141000/398710 [00:20<00:39, 6512.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  36%|███▌      | 143000/398710 [00:21<00:37, 6777.61 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  36%|███▋      | 145000/398710 [00:21<00:36, 6978.72 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  37%|███▋      | 147000/398710 [00:21<00:35, 7036.77 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  37%|███▋      | 149000/398710 [00:22<00:35, 7022.14 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  38%|███▊      | 151000/398710 [00:22<00:39, 6319.45 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  38%|███▊      | 153000/398710 [00:22<00:36, 6690.46 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  39%|███▉      | 155000/398710 [00:22<00:35, 6848.72 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  39%|███▉      | 157000/398710 [00:23<00:34, 7064.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  40%|███▉      | 159000/398710 [00:23<00:34, 6962.69 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  40%|████      | 160000/398710 [00:23<00:34, 6971.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  41%|████      | 162000/398710 [00:24<00:37, 6331.65 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  41%|████      | 164000/398710 [00:24<00:34, 6711.11 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  42%|████▏     | 166000/398710 [00:24<00:34, 6829.36 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  42%|████▏     | 168000/398710 [00:24<00:33, 6897.91 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  43%|████▎     | 170000/398710 [00:25<00:32, 7048.96 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  43%|████▎     | 172000/398710 [00:25<00:31, 7087.58 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  44%|████▎     | 174000/398710 [00:25<00:34, 6426.23 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  44%|████▍     | 176000/398710 [00:26<00:35, 6209.75 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  45%|████▍     | 178000/398710 [00:26<00:33, 6575.87 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  45%|████▌     | 180000/398710 [00:26<00:31, 6883.72 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  46%|████▌     | 182000/398710 [00:26<00:30, 7074.63 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  46%|████▌     | 184000/398710 [00:27<00:33, 6488.62 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  47%|████▋     | 186000/398710 [00:27<00:30, 6880.59 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  47%|████▋     | 188000/398710 [00:27<00:29, 7045.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  48%|████▊     | 190000/398710 [00:28<00:30, 6815.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  48%|████▊     | 192000/398710 [00:28<00:29, 6927.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  49%|████▊     | 194000/398710 [00:28<00:32, 6215.09 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  49%|████▉     | 196000/398710 [00:29<00:30, 6744.60 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  50%|████▉     | 198000/398710 [00:29<00:30, 6657.29 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  50%|█████     | 200000/398710 [00:29<00:28, 6902.22 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  51%|█████     | 202000/398710 [00:29<00:28, 6983.05 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  51%|█████     | 204000/398710 [00:30<00:27, 6998.85 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  52%|█████▏    | 206000/398710 [00:30<00:30, 6324.68 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  52%|█████▏    | 207000/398710 [00:30<00:29, 6590.46 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  52%|█████▏    | 209000/398710 [00:31<00:34, 5543.37 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  53%|█████▎    | 211000/398710 [00:31<00:29, 6258.78 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  53%|█████▎    | 213000/398710 [00:31<00:27, 6695.50 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  54%|█████▍    | 215000/398710 [00:31<00:26, 6932.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  54%|█████▍    | 217000/398710 [00:32<00:27, 6518.60 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  55%|█████▍    | 219000/398710 [00:32<00:26, 6855.88 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  55%|█████▌    | 221000/398710 [00:32<00:25, 7067.68 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  56%|█████▌    | 223000/398710 [00:33<00:25, 7017.22 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  56%|█████▋    | 225000/398710 [00:33<00:24, 7105.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  57%|█████▋    | 227000/398710 [00:33<00:27, 6234.10 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  57%|█████▋    | 229000/398710 [00:34<00:25, 6695.40 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  58%|█████▊    | 231000/398710 [00:34<00:24, 6898.84 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  58%|█████▊    | 233000/398710 [00:34<00:23, 6999.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  59%|█████▉    | 235000/398710 [00:34<00:22, 7145.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  59%|█████▉    | 237000/398710 [00:35<00:22, 7166.78 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  60%|█████▉    | 239000/398710 [00:35<00:25, 6319.56 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  60%|██████    | 241000/398710 [00:35<00:23, 6755.90 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  61%|██████    | 243000/398710 [00:36<00:22, 6949.57 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  61%|██████▏   | 245000/398710 [00:36<00:21, 7043.16 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  62%|██████▏   | 247000/398710 [00:36<00:21, 7156.70 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  62%|██████▏   | 249000/398710 [00:36<00:23, 6305.14 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  63%|██████▎   | 251000/398710 [00:37<00:21, 6729.24 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  63%|██████▎   | 253000/398710 [00:37<00:20, 6944.01 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  64%|██████▍   | 255000/398710 [00:37<00:20, 7092.61 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  64%|██████▍   | 257000/398710 [00:38<00:19, 7180.78 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  65%|██████▍   | 259000/398710 [00:38<00:19, 7136.68 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  65%|██████▌   | 261000/398710 [00:38<00:21, 6513.18 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  66%|██████▌   | 263000/398710 [00:38<00:19, 6856.93 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  66%|██████▋   | 265000/398710 [00:39<00:18, 7054.77 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  67%|██████▋   | 267000/398710 [00:39<00:18, 7153.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  67%|██████▋   | 269000/398710 [00:39<00:18, 7074.74 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  68%|██████▊   | 271000/398710 [00:40<00:17, 7147.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  68%|██████▊   | 273000/398710 [00:40<00:19, 6572.24 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  69%|██████▉   | 275000/398710 [00:40<00:17, 6924.55 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  69%|██████▉   | 277000/398710 [00:40<00:17, 7064.84 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  70%|██████▉   | 279000/398710 [00:41<00:16, 7091.46 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  70%|███████   | 281000/398710 [00:41<00:16, 6932.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  71%|███████   | 283000/398710 [00:41<00:18, 6155.65 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  71%|███████▏  | 285000/398710 [00:42<00:17, 6452.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  72%|███████▏  | 287000/398710 [00:42<00:16, 6684.51 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  72%|███████▏  | 289000/398710 [00:42<00:16, 6786.00 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  73%|███████▎  | 291000/398710 [00:43<00:15, 6961.35 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  73%|███████▎  | 292000/398710 [00:43<00:15, 7004.88 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  74%|███████▎  | 294000/398710 [00:43<00:16, 6376.87 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  74%|███████▍  | 296000/398710 [00:43<00:15, 6672.83 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  75%|███████▍  | 298000/398710 [00:44<00:14, 6949.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  75%|███████▌  | 300000/398710 [00:44<00:14, 7018.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  76%|███████▌  | 302000/398710 [00:44<00:13, 7047.52 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  76%|███████▌  | 304000/398710 [00:44<00:13, 7056.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  77%|███████▋  | 306000/398710 [00:45<00:14, 6416.98 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  77%|███████▋  | 308000/398710 [00:45<00:13, 6790.78 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  78%|███████▊  | 310000/398710 [00:45<00:12, 6963.70 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  78%|███████▊  | 312000/398710 [00:46<00:12, 6951.49 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  79%|███████▉  | 314000/398710 [00:46<00:12, 6937.77 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  79%|███████▉  | 316000/398710 [00:46<00:13, 6311.81 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  80%|███████▉  | 318000/398710 [00:47<00:12, 6607.48 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  80%|████████  | 320000/398710 [00:47<00:11, 6767.15 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  81%|████████  | 322000/398710 [00:47<00:11, 6952.45 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  81%|████████▏ | 324000/398710 [00:47<00:10, 6977.77 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  82%|████████▏ | 326000/398710 [00:48<00:11, 6152.07 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  82%|████████▏ | 328000/398710 [00:48<00:10, 6589.98 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  83%|████████▎ | 330000/398710 [00:48<00:10, 6804.43 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  83%|████████▎ | 332000/398710 [00:49<00:09, 6936.84 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  84%|████████▍ | 334000/398710 [00:49<00:09, 6950.52 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  84%|████████▍ | 336000/398710 [00:49<00:09, 6934.92 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  85%|████████▍ | 338000/398710 [00:50<00:09, 6252.01 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  85%|████████▌ | 340000/398710 [00:50<00:08, 6662.91 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  86%|████████▌ | 342000/398710 [00:50<00:09, 6194.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  86%|████████▋ | 344000/398710 [00:51<00:08, 6457.67 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  87%|████████▋ | 346000/398710 [00:51<00:07, 6732.96 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  87%|████████▋ | 347000/398710 [00:51<00:08, 6173.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  88%|████████▊ | 349000/398710 [00:51<00:08, 6008.09 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  88%|████████▊ | 351000/398710 [00:52<00:07, 6380.65 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  89%|████████▊ | 353000/398710 [00:52<00:06, 6585.35 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  89%|████████▉ | 355000/398710 [00:52<00:06, 6813.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  90%|████████▉ | 357000/398710 [00:53<00:06, 6778.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  90%|████████▉ | 358000/398710 [00:53<00:06, 6719.82 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  90%|█████████ | 360000/398710 [00:53<00:06, 6278.57 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  91%|█████████ | 362000/398710 [00:53<00:05, 6629.60 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  91%|█████████▏| 364000/398710 [00:54<00:05, 6788.79 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  92%|█████████▏| 366000/398710 [00:54<00:04, 6951.30 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  92%|█████████▏| 368000/398710 [00:54<00:04, 6898.70 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  93%|█████████▎| 370000/398710 [00:54<00:04, 7018.46 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  93%|█████████▎| 372000/398710 [00:55<00:04, 6483.09 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  94%|█████████▍| 374000/398710 [00:55<00:03, 6786.85 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  94%|█████████▍| 376000/398710 [00:55<00:03, 6977.84 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  95%|█████████▍| 378000/398710 [00:56<00:02, 7069.54 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  95%|█████████▌| 380000/398710 [00:56<00:02, 7121.19 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  96%|█████████▌| 382000/398710 [00:56<00:02, 6428.51 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  96%|█████████▋| 384000/398710 [00:57<00:02, 6744.67 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  97%|█████████▋| 386000/398710 [00:57<00:01, 6924.36 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  97%|█████████▋| 388000/398710 [00:57<00:01, 7023.97 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  98%|█████████▊| 390000/398710 [00:57<00:01, 7101.03 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  98%|█████████▊| 391000/398710 [00:58<00:01, 7026.50 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  99%|█████████▊| 393000/398710 [00:58<00:00, 6318.44 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing:  99%|█████████▉| 395000/398710 [00:58<00:00, 6467.16 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing: 100%|█████████▉| 397000/398710 [00:59<00:00, 6497.92 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 1000\n",
            "Chamado tokenize len(queries_documents): 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chamado tokenize len(queries_documents): 710\n",
            "CPU times: user 5min 27s, sys: 2.66 s, total: 5min 30s\n",
            "Wall time: 59.3 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# train_dataset.set_transform(tokenize)\n",
        "train_dataset = train_dataset.map(\n",
        "        tokenize,\n",
        "        remove_columns=('query', 'text', 'label'),\n",
        "        batched=True,\n",
        "        desc='Tokenizing',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFHMQrDr5XKD",
        "outputId": "796d3b62-4620-4e3e-b9ab-e1d701c659dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [2094, 540, 46, 1231, 6916, 81, 1044, 1164, 12, 1170, 4, 516, 1450, 3, 7, 5271, 697, 3072, 16630, 10, 941, 7, 8644, 53, 117, 1593, 1083, 6, 14591, 4575, 178, 5, 852, 18113, 17832, 6551, 3, 697, 17668, 33, 7, 14232, 1343, 93, 11, 18294, 4931, 4, 12558, 918, 5, 745, 797, 1806, 46, 28, 762, 21, 15, 3528, 1083, 64, 931, 1083, 87, 2649, 4, 19414, 46, 15, 3921, 16, 4064, 83, 141, 7578, 16, 941, 12, 4, 719, 123, 18, 4178, 4, 3366, 2424, 11, 9578, 474, 11084, 53, 52, 48, 1355, 5, 64, 931, 1083, 87, 762, 5993, 46, 15, 5787, 4152, 121, 49, 15, 1931, 121, 11408, 5441, 49, 15, 1931, 121, 11408, 27, 8, 15, 3022, 1380, 4, 8025, 64, 931, 1083, 87, 4101, 12, 1830, 46, 15, 3528, 167, 1083, 64, 931, 1083, 87, 4101, 12, 792, 46, 15, 2028, 601, 3249, 64, 294, 8985, 5572, 46, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2593, 1]}\n"
          ]
        }
      ],
      "source": [
        "print(valid_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uipa1K06isE"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hig02oOb580I"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t9-bawPr57P4",
        "outputId": "9028f41c-babd-4a9c-e0ed-73f657251698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'../../model/train/ptt5-base'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PATH_TRAIN_MODEL_LOCAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "bGJOtzX4FhnF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "num_step_alert = 200\n",
        "training_args = Seq2SeqTrainingArguments(output_dir=PATH_TRAIN_MODEL_LOCAL)\n",
        "# Needed to make the Trainer work with an on-the-fly transformation on the dataset\n",
        "# training_args.remove_unused_columns = False\n",
        "training_args.output_dir = PATH_TRAIN_MODEL_LOCAL\n",
        "training_args.warmup_steps=400 # Alterar!\n",
        "training_args.num_train_epochs=4.0 # Alterar!\n",
        "training_args.logging_steps=num_step_alert # Alterar!\n",
        "training_args.save_strategy=\"steps\"\n",
        "training_args.save_steps=num_step_alert\n",
        "training_args.save_total_limit=10\n",
        "training_args.learning_rate=5e-5\n",
        "training_args.per_device_train_batch_size=16 # t4: 8, a100-40: 32\n",
        "training_args.gradient_accumulation_steps=4 # t4: 4, a100-40: 2\n",
        "#training_args._n_gpu = 1\n",
        "# training_args.bf16 = True # se for usar a100, 3090, 4090 -> usar\n",
        "training_args.ignore_data_skip = True\n",
        "training_args.load_best_model_at_end = True\n",
        "training_args.evaluation_strategy='steps'\n",
        "training_args.eval_steps=num_step_alert\n",
        "training_args.do_eval = True\n",
        "# training_args.optim='adamw_hf' #default\n",
        "training_args.gradient_checkpointing = False # True\n",
        "# se precisar economizar gpu\n",
        "# training_args.optim='adamw_bnb_8bit'\n",
        "# training_args.gradient_checkpointing = True\n",
        "training_args.report_to=\"neptune\",\n",
        "# training_args.report_to = 'None'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFMX-y4ZkLF",
        "outputId": "df911339-88b1-4f97-d33d-4f64cd203bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mMQw6ra63Xk",
        "outputId": "98e651ba-25d0-419c-fe39-143a4db50d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=200,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=True,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../model/train/ptt5-base/runs/Jul05_19-58-20_borela-wks,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=200,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=4.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=../../model/train/ptt5-base,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=('neptune',),\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../model/train/ptt5-base,\n",
            "save_on_each_node=False,\n",
            "save_steps=200,\n",
            "save_strategy=steps,\n",
            "save_total_limit=10,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=400,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
          ]
        }
      ],
      "source": [
        "print(training_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "nBRTbzhSW7hF"
      },
      "outputs": [],
      "source": [
        "# from transformers.integrations import NeptuneCallback\n",
        "# rastro_neptune = NeptuneRastroRun(hparam, parm_lista_tag= tag_contexto_rastro)\n",
        "# neptune_callback = NeptuneCallback(run=rastro_neptune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# se local\n",
        "nome_caminho_modelo = \"/home/borela/fontes/relevar-busca/modelo/\" + MODEL_NAME\n",
        "assert os.path.exists(nome_caminho_modelo), f\"Path para {MODEL_NAME} não existe!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJt67cC8u7XO",
        "outputId": "f1825556-6a3f-48c9-a678-370de859034a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)lve/main/config.json: 100%|██████████| 635/635 [00:00<00:00, 230kB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 892M/892M [01:01<00:00, 14.6MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3.88 s, sys: 2.27 s, total: 6.15 s\n",
            "Wall time: 1min 5s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(nome_caminho_modelo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "rrK0NBIV2GjX"
      },
      "outputs": [],
      "source": [
        "trainer_cls = Seq2SeqTrainer\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "Srq3KA_ZlWPD"
      },
      "outputs": [],
      "source": [
        "# Limpa o cache da memória da GPU\n",
        "# del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWdCbovj25Sh",
        "outputId": "33a0353a-a926-4470-c532-458a6c425b73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoik0K8sX3Jg"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "XhSrqbj-tgFm",
        "outputId": "29003b19-31f0-4154-c0e9-d5cc4fcb9af3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Exception: </span>Parar aqui reinício!\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mException: \u001b[0mParar aqui reinício!\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "raise Exception('Parar aqui reinício!')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dicas para integrar com Neptune \n",
        "https://docs.neptune.ai/integrations/transformers/#__tabbed_2_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "jqrhQxd3E55t"
      },
      "outputs": [],
      "source": [
        "trainer = trainer_cls(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n",
            "\u001b[0;32m----> 1\u001b[0m \u001b[39mdel\u001b[39;00m trainer\n",
            "\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ],
      "source": [
        "del trainer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "reiniciando checkpoint 5200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "QOqMyEd_neHN",
        "outputId": "3bb99a99-33be-4315-fd83-79cbb3624050"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading model from ../../model/train/ptt5-base/checkpoint-5200.\n",
            "You are resuming training from a checkpoint trained with 4.30.2 of Transformers but your current version is 4.25.1. This is not recommended and could yield to errors or unwanted behaviors.\n",
            "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 398710\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 24920\n",
            "  Number of trainable parameters = 222903552\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 0\n",
            "  Continuing training from global step 5200\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-108\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/24920 [00:00<?, ?it/s]Didn't manage to set back the RNG states of the GPU because of the following error:\n",
            " RNG state is wrong size\n",
            "This won't yield the same results as if the training had not been interrupted.\n",
            " 22%|██▏       | 5400/24920 [14:04<22:45:30,  4.20s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0386, 'learning_rate': 3.9804241435562806e-05, 'epoch': 0.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                       \n",
            " 22%|██▏       | 5400/24920 [14:59<22:45:30,  4.20s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-5400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-5400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03830071911215782, 'eval_runtime': 55.5829, 'eval_samples_per_second': 72.468, 'eval_steps_per_second': 9.068, 'epoch': 0.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-5400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-5400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-5400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-5400/spiece.model\n",
            " 22%|██▏       | 5600/24920 [29:02<22:07:17,  4.12s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0397, 'learning_rate': 3.9396411092985316e-05, 'epoch': 0.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                       \n",
            " 22%|██▏       | 5600/24920 [29:56<22:07:17,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-5600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-5600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.039101142436265945, 'eval_runtime': 54.7467, 'eval_samples_per_second': 73.575, 'eval_steps_per_second': 9.206, 'epoch': 0.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-5600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-5600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-5600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-5600/spiece.model\n",
            " 23%|██▎       | 5800/24920 [43:35<21:40:49,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0359, 'learning_rate': 3.898858075040783e-05, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                       \n",
            " 23%|██▎       | 5800/24920 [44:30<21:40:49,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-5800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-5800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03769339248538017, 'eval_runtime': 54.5549, 'eval_samples_per_second': 73.834, 'eval_steps_per_second': 9.238, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-5800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-5800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-5800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-5800/spiece.model\n",
            " 24%|██▍       | 6000/24920 [58:09<21:29:50,  4.09s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0352, 'learning_rate': 3.858075040783034e-05, 'epoch': 0.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                       \n",
            " 24%|██▍       | 6000/24920 [59:04<21:29:50,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.037992507219314575, 'eval_runtime': 54.6378, 'eval_samples_per_second': 73.722, 'eval_steps_per_second': 9.224, 'epoch': 0.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6000/spiece.model\n",
            " 25%|██▍       | 6200/24920 [1:12:45<21:15:49,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0411, 'learning_rate': 3.817292006525285e-05, 'epoch': 0.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 25%|██▍       | 6200/24920 [1:13:40<21:15:49,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035734109580516815, 'eval_runtime': 54.4601, 'eval_samples_per_second': 73.962, 'eval_steps_per_second': 9.254, 'epoch': 0.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6200/spiece.model\n",
            " 26%|██▌       | 6400/24920 [1:27:19<21:08:14,  4.11s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0402, 'learning_rate': 3.776508972267537e-05, 'epoch': 0.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 26%|██▌       | 6400/24920 [1:28:13<21:08:14,  4.11s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03558366373181343, 'eval_runtime': 54.5072, 'eval_samples_per_second': 73.899, 'eval_steps_per_second': 9.246, 'epoch': 0.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6400/spiece.model\n",
            " 26%|██▋       | 6600/24920 [1:41:52<20:49:54,  4.09s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.036, 'learning_rate': 3.735725938009788e-05, 'epoch': 0.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 26%|██▋       | 6600/24920 [1:42:47<20:49:54,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035693056881427765, 'eval_runtime': 54.5174, 'eval_samples_per_second': 73.885, 'eval_steps_per_second': 9.245, 'epoch': 0.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6600/spiece.model\n",
            " 27%|██▋       | 6800/24920 [1:56:25<20:32:25,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0396, 'learning_rate': 3.69494290375204e-05, 'epoch': 0.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 27%|██▋       | 6800/24920 [1:57:19<20:32:25,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-6800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-6800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0364278145134449, 'eval_runtime': 54.4275, 'eval_samples_per_second': 74.007, 'eval_steps_per_second': 9.26, 'epoch': 0.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-6800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-6800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-6800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-6800/spiece.model\n",
            " 28%|██▊       | 7000/24920 [2:10:59<20:18:37,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0383, 'learning_rate': 3.654159869494291e-05, 'epoch': 0.29}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 28%|██▊       | 7000/24920 [2:11:54<20:18:37,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035444226115942, 'eval_runtime': 54.4906, 'eval_samples_per_second': 73.921, 'eval_steps_per_second': 9.249, 'epoch': 0.29}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7000/spiece.model\n",
            " 29%|██▉       | 7200/24920 [2:25:32<20:06:02,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.04, 'learning_rate': 3.613376835236542e-05, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 29%|██▉       | 7200/24920 [2:26:26<20:06:02,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035171572118997574, 'eval_runtime': 54.4213, 'eval_samples_per_second': 74.015, 'eval_steps_per_second': 9.261, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-5200] due to args.save_total_limit\n",
            " 30%|██▉       | 7400/24920 [2:40:06<19:44:41,  4.06s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0389, 'learning_rate': 3.572593800978793e-05, 'epoch': 0.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 30%|██▉       | 7400/24920 [2:41:01<19:44:41,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03583730384707451, 'eval_runtime': 54.7785, 'eval_samples_per_second': 73.532, 'eval_steps_per_second': 9.201, 'epoch': 0.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-5400] due to args.save_total_limit\n",
            " 30%|███       | 7600/24920 [2:54:43<19:49:03,  4.12s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0346, 'learning_rate': 3.5318107667210443e-05, 'epoch': 0.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 30%|███       | 7600/24920 [2:55:37<19:49:03,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03525320068001747, 'eval_runtime': 54.8676, 'eval_samples_per_second': 73.413, 'eval_steps_per_second': 9.186, 'epoch': 0.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-5600] due to args.save_total_limit\n",
            " 31%|███▏      | 7800/24920 [3:09:15<19:23:44,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.039, 'learning_rate': 3.4910277324632953e-05, 'epoch': 0.42}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 31%|███▏      | 7800/24920 [3:10:10<19:23:44,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-7800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-7800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03660828247666359, 'eval_runtime': 54.6418, 'eval_samples_per_second': 73.716, 'eval_steps_per_second': 9.224, 'epoch': 0.42}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-7800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-7800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-7800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-7800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-5800] due to args.save_total_limit\n",
            " 32%|███▏      | 8000/24920 [3:23:46<19:09:10,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0363, 'learning_rate': 3.4502446982055463e-05, 'epoch': 0.45}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 32%|███▏      | 8000/24920 [3:24:41<19:09:10,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03499669209122658, 'eval_runtime': 55.0438, 'eval_samples_per_second': 73.178, 'eval_steps_per_second': 9.156, 'epoch': 0.45}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6000] due to args.save_total_limit\n",
            " 33%|███▎      | 8200/24920 [3:38:19<18:56:42,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0359, 'learning_rate': 3.409461663947798e-05, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 33%|███▎      | 8200/24920 [3:39:14<18:56:42,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035376258194446564, 'eval_runtime': 54.9293, 'eval_samples_per_second': 73.331, 'eval_steps_per_second': 9.175, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6200] due to args.save_total_limit\n",
            " 34%|███▎      | 8400/24920 [3:52:53<18:41:51,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.036, 'learning_rate': 3.368678629690049e-05, 'epoch': 0.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 34%|███▎      | 8400/24920 [3:53:47<18:41:51,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035488445311784744, 'eval_runtime': 54.5872, 'eval_samples_per_second': 73.79, 'eval_steps_per_second': 9.233, 'epoch': 0.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6400] due to args.save_total_limit\n",
            " 35%|███▍      | 8600/24920 [4:07:26<18:24:36,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0373, 'learning_rate': 3.327895595432301e-05, 'epoch': 0.55}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 35%|███▍      | 8600/24920 [4:08:20<18:24:36,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03550172969698906, 'eval_runtime': 54.471, 'eval_samples_per_second': 73.948, 'eval_steps_per_second': 9.253, 'epoch': 0.55}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6600] due to args.save_total_limit\n",
            " 35%|███▌      | 8800/24920 [4:21:58<18:16:40,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0355, 'learning_rate': 3.287112561174552e-05, 'epoch': 0.58}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 35%|███▌      | 8800/24920 [4:22:52<18:16:40,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-8800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-8800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034991305321455, 'eval_runtime': 54.6821, 'eval_samples_per_second': 73.662, 'eval_steps_per_second': 9.217, 'epoch': 0.58}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-8800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-8800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-8800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-8800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-6800] due to args.save_total_limit\n",
            " 36%|███▌      | 9000/24920 [4:36:32<17:59:09,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0372, 'learning_rate': 3.246329526916803e-05, 'epoch': 0.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 36%|███▌      | 9000/24920 [4:37:26<17:59:09,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034312039613723755, 'eval_runtime': 54.6616, 'eval_samples_per_second': 73.69, 'eval_steps_per_second': 9.22, 'epoch': 0.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7000] due to args.save_total_limit\n",
            " 37%|███▋      | 9200/24920 [4:51:04<17:53:47,  4.10s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0342, 'learning_rate': 3.205546492659054e-05, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 37%|███▋      | 9200/24920 [4:51:59<17:53:47,  4.10s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035302210599184036, 'eval_runtime': 54.5633, 'eval_samples_per_second': 73.823, 'eval_steps_per_second': 9.237, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7200] due to args.save_total_limit\n",
            " 38%|███▊      | 9400/24920 [5:05:35<17:31:30,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0345, 'learning_rate': 3.1647634584013054e-05, 'epoch': 0.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 38%|███▊      | 9400/24920 [5:06:29<17:31:30,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03438310697674751, 'eval_runtime': 54.5577, 'eval_samples_per_second': 73.83, 'eval_steps_per_second': 9.238, 'epoch': 0.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7400] due to args.save_total_limit\n",
            " 39%|███▊      | 9600/24920 [5:20:06<17:17:21,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.037, 'learning_rate': 3.1239804241435564e-05, 'epoch': 0.71}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 39%|███▊      | 9600/24920 [5:21:01<17:17:21,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0335809662938118, 'eval_runtime': 54.6769, 'eval_samples_per_second': 73.669, 'eval_steps_per_second': 9.218, 'epoch': 0.71}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7600] due to args.save_total_limit\n",
            " 39%|███▉      | 9800/24920 [5:34:38<17:09:55,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0359, 'learning_rate': 3.0831973898858074e-05, 'epoch': 0.74}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 39%|███▉      | 9800/24920 [5:35:33<17:09:55,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-9800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-9800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034582629799842834, 'eval_runtime': 54.5553, 'eval_samples_per_second': 73.833, 'eval_steps_per_second': 9.238, 'epoch': 0.74}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-9800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-9800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-9800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-9800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-7800] due to args.save_total_limit\n",
            " 40%|████      | 10000/24920 [5:49:09<16:52:39,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0335, 'learning_rate': 3.0424143556280587e-05, 'epoch': 0.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 40%|████      | 10000/24920 [5:50:03<16:52:39,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.039332326501607895, 'eval_runtime': 54.3935, 'eval_samples_per_second': 74.053, 'eval_steps_per_second': 9.266, 'epoch': 0.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8000] due to args.save_total_limit\n",
            " 41%|████      | 10200/24920 [6:03:39<16:37:30,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0339, 'learning_rate': 3.00163132137031e-05, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 41%|████      | 10200/24920 [6:04:33<16:37:30,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.036113590002059937, 'eval_runtime': 54.3368, 'eval_samples_per_second': 74.13, 'eval_steps_per_second': 9.275, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8200] due to args.save_total_limit\n",
            " 42%|████▏     | 10400/24920 [6:18:08<16:22:16,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0341, 'learning_rate': 2.9608482871125614e-05, 'epoch': 0.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 42%|████▏     | 10400/24920 [6:19:02<16:22:16,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033041030168533325, 'eval_runtime': 54.4255, 'eval_samples_per_second': 74.009, 'eval_steps_per_second': 9.26, 'epoch': 0.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8400] due to args.save_total_limit\n",
            " 43%|████▎     | 10600/24920 [6:32:37<16:10:12,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0381, 'learning_rate': 2.9200652528548127e-05, 'epoch': 0.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 43%|████▎     | 10600/24920 [6:33:31<16:10:12,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034012772142887115, 'eval_runtime': 54.3142, 'eval_samples_per_second': 74.161, 'eval_steps_per_second': 9.279, 'epoch': 0.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8600] due to args.save_total_limit\n",
            " 43%|████▎     | 10800/24920 [6:47:10<15:56:53,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0335, 'learning_rate': 2.8792822185970637e-05, 'epoch': 0.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 43%|████▎     | 10800/24920 [6:48:05<15:56:53,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-10800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-10800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032748669385910034, 'eval_runtime': 54.6942, 'eval_samples_per_second': 73.646, 'eval_steps_per_second': 9.215, 'epoch': 0.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-10800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-10800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-10800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-10800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-8800] due to args.save_total_limit\n",
            " 44%|████▍     | 11000/24920 [7:01:42<15:45:30,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0341, 'learning_rate': 2.838499184339315e-05, 'epoch': 0.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 44%|████▍     | 11000/24920 [7:02:37<15:45:30,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034486666321754456, 'eval_runtime': 54.7649, 'eval_samples_per_second': 73.551, 'eval_steps_per_second': 9.203, 'epoch': 0.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9000] due to args.save_total_limit\n",
            " 45%|████▍     | 11200/24920 [7:16:19<15:36:55,  4.10s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0348, 'learning_rate': 2.7977161500815664e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 45%|████▍     | 11200/24920 [7:17:14<15:36:55,  4.10s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033023323863744736, 'eval_runtime': 54.5971, 'eval_samples_per_second': 73.777, 'eval_steps_per_second': 9.231, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9200] due to args.save_total_limit\n",
            " 46%|████▌     | 11400/24920 [7:30:51<15:14:23,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0337, 'learning_rate': 2.7569331158238177e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 46%|████▌     | 11400/24920 [7:31:46<15:14:23,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03410288691520691, 'eval_runtime': 54.4564, 'eval_samples_per_second': 73.967, 'eval_steps_per_second': 9.255, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9400] due to args.save_total_limit\n",
            " 47%|████▋     | 11600/24920 [7:45:20<15:00:06,  4.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0269, 'learning_rate': 2.7161500815660684e-05, 'epoch': 1.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 47%|████▋     | 11600/24920 [7:46:15<15:00:06,  4.05s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03432347625494003, 'eval_runtime': 54.4664, 'eval_samples_per_second': 73.954, 'eval_steps_per_second': 9.253, 'epoch': 1.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9600] due to args.save_total_limit\n",
            " 47%|████▋     | 11800/24920 [7:59:49<14:50:44,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0283, 'learning_rate': 2.6753670473083197e-05, 'epoch': 1.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 47%|████▋     | 11800/24920 [8:00:44<14:50:44,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-11800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-11800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03402471914887428, 'eval_runtime': 54.5164, 'eval_samples_per_second': 73.886, 'eval_steps_per_second': 9.245, 'epoch': 1.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-11800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-11800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-11800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-11800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-9800] due to args.save_total_limit\n",
            " 48%|████▊     | 12000/24920 [8:14:20<14:41:43,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0289, 'learning_rate': 2.634584013050571e-05, 'epoch': 1.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 48%|████▊     | 12000/24920 [8:15:14<14:41:43,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03309272229671478, 'eval_runtime': 54.8533, 'eval_samples_per_second': 73.432, 'eval_steps_per_second': 9.188, 'epoch': 1.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10000] due to args.save_total_limit\n",
            " 49%|████▉     | 12200/24920 [8:28:50<14:23:40,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0274, 'learning_rate': 2.5938009787928224e-05, 'epoch': 1.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 49%|████▉     | 12200/24920 [8:29:45<14:23:40,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0332350879907608, 'eval_runtime': 55.0246, 'eval_samples_per_second': 73.204, 'eval_steps_per_second': 9.16, 'epoch': 1.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10200] due to args.save_total_limit\n",
            " 50%|████▉     | 12400/24920 [8:43:21<14:06:10,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0286, 'learning_rate': 2.5530179445350734e-05, 'epoch': 1.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 50%|████▉     | 12400/24920 [8:44:16<14:06:10,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03304194658994675, 'eval_runtime': 54.4162, 'eval_samples_per_second': 74.022, 'eval_steps_per_second': 9.262, 'epoch': 1.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10400] due to args.save_total_limit\n",
            " 51%|█████     | 12600/24920 [8:57:51<13:53:01,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.028, 'learning_rate': 2.5122349102773248e-05, 'epoch': 1.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 51%|█████     | 12600/24920 [8:58:45<13:53:01,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03348330035805702, 'eval_runtime': 54.3818, 'eval_samples_per_second': 74.069, 'eval_steps_per_second': 9.268, 'epoch': 1.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10600] due to args.save_total_limit\n",
            " 51%|█████▏    | 12800/24920 [9:12:19<13:39:41,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0271, 'learning_rate': 2.471451876019576e-05, 'epoch': 1.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 51%|█████▏    | 12800/24920 [9:13:13<13:39:41,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-12800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-12800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03277859091758728, 'eval_runtime': 54.3456, 'eval_samples_per_second': 74.118, 'eval_steps_per_second': 9.274, 'epoch': 1.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-12800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-12800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-12800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-12800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-10800] due to args.save_total_limit\n",
            " 52%|█████▏    | 13000/24920 [9:26:47<13:24:52,  4.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0268, 'learning_rate': 2.4306688417618274e-05, 'epoch': 1.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 52%|█████▏    | 13000/24920 [9:27:41<13:24:52,  4.05s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03482929989695549, 'eval_runtime': 54.8141, 'eval_samples_per_second': 73.485, 'eval_steps_per_second': 9.195, 'epoch': 1.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11000] due to args.save_total_limit\n",
            " 53%|█████▎    | 13200/24920 [9:41:15<13:17:04,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0265, 'learning_rate': 2.3898858075040784e-05, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 53%|█████▎    | 13200/24920 [9:42:09<13:17:04,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03329039737582207, 'eval_runtime': 54.4179, 'eval_samples_per_second': 74.02, 'eval_steps_per_second': 9.262, 'epoch': 1.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11200] due to args.save_total_limit\n",
            " 54%|█████▍    | 13400/24920 [9:55:43<13:03:05,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.03, 'learning_rate': 2.3491027732463298e-05, 'epoch': 1.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 54%|█████▍    | 13400/24920 [9:56:37<13:03:05,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03396589681506157, 'eval_runtime': 54.3845, 'eval_samples_per_second': 74.065, 'eval_steps_per_second': 9.267, 'epoch': 1.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11400] due to args.save_total_limit\n",
            " 55%|█████▍    | 13600/24920 [10:10:13<12:51:07,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0256, 'learning_rate': 2.3083197389885808e-05, 'epoch': 1.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 55%|█████▍    | 13600/24920 [10:11:08<12:51:07,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03387042135000229, 'eval_runtime': 54.3974, 'eval_samples_per_second': 74.048, 'eval_steps_per_second': 9.265, 'epoch': 1.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11600] due to args.save_total_limit\n",
            " 55%|█████▌    | 13800/24920 [10:24:50<12:37:54,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0284, 'learning_rate': 2.267536704730832e-05, 'epoch': 1.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 55%|█████▌    | 13800/24920 [10:25:46<12:37:54,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-13800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-13800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03275327757000923, 'eval_runtime': 55.4019, 'eval_samples_per_second': 72.705, 'eval_steps_per_second': 9.097, 'epoch': 1.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-13800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-13800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-13800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-13800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-11800] due to args.save_total_limit\n",
            " 56%|█████▌    | 14000/24920 [10:39:24<12:22:00,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0291, 'learning_rate': 2.226753670473083e-05, 'epoch': 1.41}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 56%|█████▌    | 14000/24920 [10:40:19<12:22:00,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03327794745564461, 'eval_runtime': 54.6509, 'eval_samples_per_second': 73.704, 'eval_steps_per_second': 9.222, 'epoch': 1.41}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12000] due to args.save_total_limit\n",
            " 57%|█████▋    | 14200/24920 [10:53:53<12:05:35,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0289, 'learning_rate': 2.1859706362153344e-05, 'epoch': 1.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 57%|█████▋    | 14200/24920 [10:54:48<12:05:35,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033764783293008804, 'eval_runtime': 54.3898, 'eval_samples_per_second': 74.058, 'eval_steps_per_second': 9.266, 'epoch': 1.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12200] due to args.save_total_limit\n",
            " 58%|█████▊    | 14400/24920 [11:08:22<11:52:35,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0257, 'learning_rate': 2.1451876019575858e-05, 'epoch': 1.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 58%|█████▊    | 14400/24920 [11:09:16<11:52:35,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033985648304224014, 'eval_runtime': 54.2261, 'eval_samples_per_second': 74.282, 'eval_steps_per_second': 9.294, 'epoch': 1.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12400] due to args.save_total_limit\n",
            " 59%|█████▊    | 14600/24920 [11:22:58<11:42:49,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0268, 'learning_rate': 2.104404567699837e-05, 'epoch': 1.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 59%|█████▊    | 14600/24920 [11:23:52<11:42:49,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03413856402039528, 'eval_runtime': 54.5704, 'eval_samples_per_second': 73.813, 'eval_steps_per_second': 9.236, 'epoch': 1.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12600] due to args.save_total_limit\n",
            " 59%|█████▉    | 14800/24920 [11:37:25<11:22:40,  4.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0299, 'learning_rate': 2.0636215334420885e-05, 'epoch': 1.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 59%|█████▉    | 14800/24920 [11:38:19<11:22:40,  4.05s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-14800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-14800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034166935831308365, 'eval_runtime': 54.2621, 'eval_samples_per_second': 74.232, 'eval_steps_per_second': 9.288, 'epoch': 1.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-14800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-14800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-14800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-14800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-12800] due to args.save_total_limit\n",
            " 60%|██████    | 15000/24920 [11:51:53<11:09:25,  4.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0264, 'learning_rate': 2.0228384991843395e-05, 'epoch': 1.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 60%|██████    | 15000/24920 [11:52:48<11:09:25,  4.05s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033432092517614365, 'eval_runtime': 54.432, 'eval_samples_per_second': 74.001, 'eval_steps_per_second': 9.259, 'epoch': 1.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13000] due to args.save_total_limit\n",
            " 61%|██████    | 15200/24920 [12:06:23<10:56:55,  4.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0253, 'learning_rate': 1.9820554649265908e-05, 'epoch': 1.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 61%|██████    | 15200/24920 [12:07:18<10:56:55,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0337190218269825, 'eval_runtime': 54.46, 'eval_samples_per_second': 73.963, 'eval_steps_per_second': 9.254, 'epoch': 1.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13200] due to args.save_total_limit\n",
            " 62%|██████▏   | 15400/24920 [12:20:52<10:47:07,  4.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0292, 'learning_rate': 1.9412724306688418e-05, 'epoch': 1.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 62%|██████▏   | 15400/24920 [12:21:46<10:47:07,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03311581164598465, 'eval_runtime': 54.3622, 'eval_samples_per_second': 74.096, 'eval_steps_per_second': 9.271, 'epoch': 1.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13400] due to args.save_total_limit\n",
            " 63%|██████▎   | 15600/24920 [12:35:23<10:35:42,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0258, 'learning_rate': 1.900489396411093e-05, 'epoch': 1.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 63%|██████▎   | 15600/24920 [12:36:18<10:35:42,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03332404047250748, 'eval_runtime': 54.5929, 'eval_samples_per_second': 73.783, 'eval_steps_per_second': 9.232, 'epoch': 1.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13600] due to args.save_total_limit\n",
            " 63%|██████▎   | 15800/24920 [12:49:55<10:20:55,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0292, 'learning_rate': 1.859706362153344e-05, 'epoch': 1.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 63%|██████▎   | 15800/24920 [12:50:50<10:20:55,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-15800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-15800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03233485668897629, 'eval_runtime': 54.4815, 'eval_samples_per_second': 73.933, 'eval_steps_per_second': 9.251, 'epoch': 1.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-15800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-15800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-15800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-15800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-13800] due to args.save_total_limit\n",
            " 64%|██████▍   | 16000/24920 [13:04:31<10:07:22,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0262, 'learning_rate': 1.8189233278955955e-05, 'epoch': 1.73}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \n",
            " 64%|██████▍   | 16000/24920 [13:05:26<10:07:22,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03301709517836571, 'eval_runtime': 54.7592, 'eval_samples_per_second': 73.558, 'eval_steps_per_second': 9.204, 'epoch': 1.73}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14000] due to args.save_total_limit\n",
            " 65%|██████▌   | 16200/24920 [13:19:05<9:51:54,  4.07s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0297, 'learning_rate': 1.7781402936378465e-05, 'epoch': 1.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 65%|██████▌   | 16200/24920 [13:20:00<9:51:54,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03242029994726181, 'eval_runtime': 54.8458, 'eval_samples_per_second': 73.442, 'eval_steps_per_second': 9.189, 'epoch': 1.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14200] due to args.save_total_limit\n",
            " 66%|██████▌   | 16400/24920 [13:33:37<9:36:12,  4.06s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0264, 'learning_rate': 1.7373572593800978e-05, 'epoch': 1.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 66%|██████▌   | 16400/24920 [13:34:32<9:36:12,  4.06s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0320003479719162, 'eval_runtime': 54.7364, 'eval_samples_per_second': 73.589, 'eval_steps_per_second': 9.208, 'epoch': 1.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14400] due to args.save_total_limit\n",
            " 67%|██████▋   | 16600/24920 [13:48:08<9:24:21,  4.07s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0257, 'learning_rate': 1.6965742251223495e-05, 'epoch': 1.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 67%|██████▋   | 16600/24920 [13:49:03<9:24:21,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.031586699187755585, 'eval_runtime': 54.4731, 'eval_samples_per_second': 73.945, 'eval_steps_per_second': 9.252, 'epoch': 1.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14600] due to args.save_total_limit\n",
            " 67%|██████▋   | 16800/24920 [14:02:39<9:13:00,  4.09s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0257, 'learning_rate': 1.6557911908646005e-05, 'epoch': 1.86}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 67%|██████▋   | 16800/24920 [14:03:33<9:13:00,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-16800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-16800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03272559493780136, 'eval_runtime': 54.5957, 'eval_samples_per_second': 73.779, 'eval_steps_per_second': 9.232, 'epoch': 1.86}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-16800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-16800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-16800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-16800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-14800] due to args.save_total_limit\n",
            " 68%|██████▊   | 17000/24920 [14:17:11<8:59:42,  4.09s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0274, 'learning_rate': 1.6150081566068518e-05, 'epoch': 1.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 68%|██████▊   | 17000/24920 [14:18:06<8:59:42,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03192828223109245, 'eval_runtime': 54.6653, 'eval_samples_per_second': 73.685, 'eval_steps_per_second': 9.22, 'epoch': 1.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15000] due to args.save_total_limit\n",
            " 69%|██████▉   | 17191/24920 [14:31:12<8:48:49,  4.11s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: HTTPServiceUnavailable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 17192/24920 [14:31:17<8:50:05,  4.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Communication with Neptune restored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 17200/24920 [14:31:50<8:53:33,  4.15s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0255, 'learning_rate': 1.5742251223491028e-05, 'epoch': 1.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 69%|██████▉   | 17200/24920 [14:32:45<8:53:33,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033229388296604156, 'eval_runtime': 55.1296, 'eval_samples_per_second': 73.064, 'eval_steps_per_second': 9.142, 'epoch': 1.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15200] due to args.save_total_limit\n",
            " 70%|██████▉   | 17400/24920 [14:46:40<8:41:46,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0273, 'learning_rate': 1.533442088091354e-05, 'epoch': 1.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 70%|██████▉   | 17400/24920 [14:47:35<8:41:46,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032098811119794846, 'eval_runtime': 55.16, 'eval_samples_per_second': 73.024, 'eval_steps_per_second': 9.137, 'epoch': 1.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15400] due to args.save_total_limit\n",
            " 71%|███████   | 17600/24920 [15:01:31<8:28:36,  4.17s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0288, 'learning_rate': 1.4926590538336052e-05, 'epoch': 1.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 71%|███████   | 17600/24920 [15:02:26<8:28:36,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032441623508930206, 'eval_runtime': 55.211, 'eval_samples_per_second': 72.957, 'eval_steps_per_second': 9.129, 'epoch': 1.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15600] due to args.save_total_limit\n",
            " 71%|███████▏  | 17800/24920 [15:16:21<8:18:48,  4.20s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.024, 'learning_rate': 1.4518760195758565e-05, 'epoch': 2.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 71%|███████▏  | 17800/24920 [15:17:17<8:18:48,  4.20s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-17800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-17800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03280528634786606, 'eval_runtime': 55.4043, 'eval_samples_per_second': 72.702, 'eval_steps_per_second': 9.097, 'epoch': 2.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-17800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-17800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-17800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-17800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-15800] due to args.save_total_limit\n",
            " 72%|███████▏  | 18000/24920 [15:31:11<8:02:44,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0231, 'learning_rate': 1.4110929853181077e-05, 'epoch': 2.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 72%|███████▏  | 18000/24920 [15:32:06<8:02:44,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-18000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-18000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032688163220882416, 'eval_runtime': 55.4264, 'eval_samples_per_second': 72.673, 'eval_steps_per_second': 9.093, 'epoch': 2.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-18000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-18000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-16000] due to args.save_total_limit\n",
            " 73%|███████▎  | 18200/24920 [15:46:01<7:39:11,  4.10s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0212, 'learning_rate': 1.370309951060359e-05, 'epoch': 2.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 73%|███████▎  | 18200/24920 [15:46:57<7:39:11,  4.10s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-18200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-18200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033261191099882126, 'eval_runtime': 55.2131, 'eval_samples_per_second': 72.954, 'eval_steps_per_second': 9.128, 'epoch': 2.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-18200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-18200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-18200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-18200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-16200] due to args.save_total_limit\n",
            " 74%|███████▍  | 18400/24920 [16:00:47<7:32:00,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0201, 'learning_rate': 1.32952691680261e-05, 'epoch': 2.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 74%|███████▍  | 18400/24920 [16:01:43<7:32:00,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-18400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-18400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0328025184571743, 'eval_runtime': 55.8932, 'eval_samples_per_second': 72.066, 'eval_steps_per_second': 9.017, 'epoch': 2.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-18400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-18400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-18400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-18400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-16400] due to args.save_total_limit\n",
            " 75%|███████▍  | 18600/24920 [16:15:46<7:25:14,  4.23s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0204, 'learning_rate': 1.2887438825448613e-05, 'epoch': 2.15}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 75%|███████▍  | 18600/24920 [16:16:41<7:25:14,  4.23s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-18600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-18600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0336274690926075, 'eval_runtime': 55.5273, 'eval_samples_per_second': 72.541, 'eval_steps_per_second': 9.077, 'epoch': 2.15}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-18600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-18600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-18600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-18600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-16600] due to args.save_total_limit\n",
            " 75%|███████▌  | 18800/24920 [16:30:45<7:10:38,  4.22s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0197, 'learning_rate': 1.2479608482871127e-05, 'epoch': 2.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 75%|███████▌  | 18800/24920 [16:31:40<7:10:38,  4.22s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-18800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-18800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034743838012218475, 'eval_runtime': 55.474, 'eval_samples_per_second': 72.611, 'eval_steps_per_second': 9.085, 'epoch': 2.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-18800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-18800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-18800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-18800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-16800] due to args.save_total_limit\n",
            " 76%|███████▌  | 19000/24920 [16:45:48<6:52:16,  4.18s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0253, 'learning_rate': 1.2071778140293638e-05, 'epoch': 2.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 76%|███████▌  | 19000/24920 [16:46:43<6:52:16,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-19000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-19000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033184681087732315, 'eval_runtime': 55.406, 'eval_samples_per_second': 72.7, 'eval_steps_per_second': 9.096, 'epoch': 2.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-19000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-19000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-17000] due to args.save_total_limit\n",
            " 77%|███████▋  | 19200/24920 [17:00:41<6:36:14,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.022, 'learning_rate': 1.166394779771615e-05, 'epoch': 2.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 77%|███████▋  | 19200/24920 [17:01:37<6:36:14,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-19200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-19200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03394577279686928, 'eval_runtime': 55.1714, 'eval_samples_per_second': 73.009, 'eval_steps_per_second': 9.135, 'epoch': 2.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-19200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-19200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-19200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-19200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-17200] due to args.save_total_limit\n",
            " 78%|███████▊  | 19400/24920 [17:15:30<6:16:49,  4.10s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0219, 'learning_rate': 1.1256117455138662e-05, 'epoch': 2.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 78%|███████▊  | 19400/24920 [17:16:26<6:16:49,  4.10s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-19400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-19400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03350279852747917, 'eval_runtime': 55.5103, 'eval_samples_per_second': 72.563, 'eval_steps_per_second': 9.079, 'epoch': 2.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-19400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-19400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-19400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-19400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-17400] due to args.save_total_limit\n",
            " 79%|███████▊  | 19600/24920 [17:30:17<6:05:36,  4.12s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0216, 'learning_rate': 1.0848287112561175e-05, 'epoch': 2.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 79%|███████▊  | 19600/24920 [17:31:12<6:05:36,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-19600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-19600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034070245921611786, 'eval_runtime': 55.0177, 'eval_samples_per_second': 73.213, 'eval_steps_per_second': 9.161, 'epoch': 2.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-19600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-19600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-19600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-19600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-17600] due to args.save_total_limit\n",
            " 79%|███████▉  | 19800/24920 [17:45:05<5:51:57,  4.12s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0215, 'learning_rate': 1.0440456769983689e-05, 'epoch': 2.34}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 79%|███████▉  | 19800/24920 [17:46:00<5:51:57,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-19800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-19800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03342704102396965, 'eval_runtime': 55.2773, 'eval_samples_per_second': 72.869, 'eval_steps_per_second': 9.118, 'epoch': 2.34}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-19800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-19800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-19800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-19800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-17800] due to args.save_total_limit\n",
            " 80%|████████  | 20000/24920 [17:59:51<5:40:50,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0206, 'learning_rate': 1.00326264274062e-05, 'epoch': 2.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 80%|████████  | 20000/24920 [18:00:46<5:40:50,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-20000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-20000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0346258319914341, 'eval_runtime': 55.3466, 'eval_samples_per_second': 72.778, 'eval_steps_per_second': 9.106, 'epoch': 2.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-20000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-20000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-18000] due to args.save_total_limit\n",
            " 81%|████████  | 20200/24920 [18:14:38<5:24:38,  4.13s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0213, 'learning_rate': 9.624796084828712e-06, 'epoch': 2.41}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 81%|████████  | 20200/24920 [18:15:33<5:24:38,  4.13s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-20200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-20200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03318621590733528, 'eval_runtime': 55.2009, 'eval_samples_per_second': 72.97, 'eval_steps_per_second': 9.13, 'epoch': 2.41}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-20200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-20200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-20200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-20200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-18200] due to args.save_total_limit\n",
            " 82%|████████▏ | 20400/24920 [18:29:25<5:13:16,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.023, 'learning_rate': 9.216965742251224e-06, 'epoch': 2.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 82%|████████▏ | 20400/24920 [18:30:21<5:13:16,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-20400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-20400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032678015530109406, 'eval_runtime': 55.2482, 'eval_samples_per_second': 72.907, 'eval_steps_per_second': 9.122, 'epoch': 2.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-20400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-20400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-20400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-20400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-18400] due to args.save_total_limit\n",
            " 83%|████████▎ | 20600/24920 [18:44:15<4:57:02,  4.13s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0233, 'learning_rate': 8.809135399673735e-06, 'epoch': 2.47}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 83%|████████▎ | 20600/24920 [18:45:10<4:57:02,  4.13s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-20600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-20600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03318605571985245, 'eval_runtime': 54.9828, 'eval_samples_per_second': 73.259, 'eval_steps_per_second': 9.167, 'epoch': 2.47}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-20600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-20600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-20600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-20600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-18600] due to args.save_total_limit\n",
            " 83%|████████▎ | 20800/24920 [18:59:00<4:45:18,  4.15s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.019, 'learning_rate': 8.401305057096249e-06, 'epoch': 2.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 83%|████████▎ | 20800/24920 [18:59:55<4:45:18,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-20800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-20800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033951032906770706, 'eval_runtime': 55.295, 'eval_samples_per_second': 72.846, 'eval_steps_per_second': 9.115, 'epoch': 2.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-20800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-20800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-20800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-20800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-18800] due to args.save_total_limit\n",
            " 84%|████████▍ | 21000/24920 [19:13:47<4:30:22,  4.14s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0217, 'learning_rate': 7.99347471451876e-06, 'epoch': 2.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 84%|████████▍ | 21000/24920 [19:14:42<4:30:22,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-21000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-21000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03282683715224266, 'eval_runtime': 55.2857, 'eval_samples_per_second': 72.858, 'eval_steps_per_second': 9.116, 'epoch': 2.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-21000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-21000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-21000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-21000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-19000] due to args.save_total_limit\n",
            " 85%|████████▌ | 21200/24920 [19:28:34<4:14:41,  4.11s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.02, 'learning_rate': 7.585644371941272e-06, 'epoch': 2.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 85%|████████▌ | 21200/24920 [19:29:30<4:14:41,  4.11s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-21200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-21200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033493563532829285, 'eval_runtime': 55.219, 'eval_samples_per_second': 72.946, 'eval_steps_per_second': 9.127, 'epoch': 2.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-21200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-21200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-21200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-21200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-19200] due to args.save_total_limit\n",
            " 86%|████████▌ | 21400/24920 [19:43:20<4:01:46,  4.12s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0206, 'learning_rate': 7.177814029363785e-06, 'epoch': 2.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 86%|████████▌ | 21400/24920 [19:44:15<4:01:46,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-21400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-21400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03364516794681549, 'eval_runtime': 55.3632, 'eval_samples_per_second': 72.756, 'eval_steps_per_second': 9.104, 'epoch': 2.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-21400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-21400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-21400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-21400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-19400] due to args.save_total_limit\n",
            " 87%|████████▋ | 21600/24920 [19:58:04<3:48:25,  4.13s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0235, 'learning_rate': 6.769983686786298e-06, 'epoch': 2.63}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 87%|████████▋ | 21600/24920 [19:58:59<3:48:25,  4.13s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-21600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-21600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03274868428707123, 'eval_runtime': 54.9239, 'eval_samples_per_second': 73.338, 'eval_steps_per_second': 9.176, 'epoch': 2.63}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-21600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-21600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-21600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-21600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-19600] due to args.save_total_limit\n",
            " 87%|████████▋ | 21800/24920 [20:12:52<3:37:05,  4.17s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0205, 'learning_rate': 6.36215334420881e-06, 'epoch': 2.66}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 87%|████████▋ | 21800/24920 [20:13:48<3:37:05,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-21800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-21800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03275454416871071, 'eval_runtime': 55.3855, 'eval_samples_per_second': 72.727, 'eval_steps_per_second': 9.1, 'epoch': 2.66}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-21800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-21800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-21800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-21800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-19800] due to args.save_total_limit\n",
            " 88%|████████▊ | 22000/24920 [20:27:39<3:21:30,  4.14s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0209, 'learning_rate': 5.954323001631321e-06, 'epoch': 2.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 88%|████████▊ | 22000/24920 [20:28:35<3:21:30,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-22000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-22000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03274182602763176, 'eval_runtime': 55.367, 'eval_samples_per_second': 72.751, 'eval_steps_per_second': 9.103, 'epoch': 2.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-22000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-22000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-22000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-22000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-20000] due to args.save_total_limit\n",
            " 89%|████████▉ | 22200/24920 [20:42:25<3:07:37,  4.14s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0241, 'learning_rate': 5.546492659053834e-06, 'epoch': 2.73}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 89%|████████▉ | 22200/24920 [20:43:20<3:07:37,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-22200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-22200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03203582763671875, 'eval_runtime': 55.0769, 'eval_samples_per_second': 73.134, 'eval_steps_per_second': 9.151, 'epoch': 2.73}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-22200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-22200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-22200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-22200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-20200] due to args.save_total_limit\n",
            " 90%|████████▉ | 22400/24920 [20:57:17<2:53:34,  4.13s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0198, 'learning_rate': 5.1386623164763465e-06, 'epoch': 2.76}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 90%|████████▉ | 22400/24920 [20:58:13<2:53:34,  4.13s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-22400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-22400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03270697593688965, 'eval_runtime': 55.4259, 'eval_samples_per_second': 72.674, 'eval_steps_per_second': 9.093, 'epoch': 2.76}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-22400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-22400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-22400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-22400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-20400] due to args.save_total_limit\n",
            " 91%|█████████ | 22600/24920 [21:12:06<2:40:07,  4.14s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0207, 'learning_rate': 4.730831973898858e-06, 'epoch': 2.79}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 91%|█████████ | 22600/24920 [21:13:01<2:40:07,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-22600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-22600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03286290913820267, 'eval_runtime': 55.3105, 'eval_samples_per_second': 72.825, 'eval_steps_per_second': 9.112, 'epoch': 2.79}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-22600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-22600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-22600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-22600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-20600] due to args.save_total_limit\n",
            " 91%|█████████▏| 22800/24920 [21:26:56<2:28:04,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0203, 'learning_rate': 4.323001631321371e-06, 'epoch': 2.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 91%|█████████▏| 22800/24920 [21:27:51<2:28:04,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-22800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-22800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03315269574522972, 'eval_runtime': 55.1991, 'eval_samples_per_second': 72.972, 'eval_steps_per_second': 9.131, 'epoch': 2.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-22800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-22800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-22800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-22800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-20800] due to args.save_total_limit\n",
            " 92%|█████████▏| 23000/24920 [21:41:44<2:13:05,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0225, 'learning_rate': 3.915171288743882e-06, 'epoch': 2.86}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 92%|█████████▏| 23000/24920 [21:42:39<2:13:05,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-23000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-23000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03251825273036957, 'eval_runtime': 55.1543, 'eval_samples_per_second': 73.031, 'eval_steps_per_second': 9.138, 'epoch': 2.86}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-23000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-23000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-23000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-23000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-21000] due to args.save_total_limit\n",
            " 93%|█████████▎| 23200/24920 [21:56:33<1:59:05,  4.15s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0218, 'learning_rate': 3.5073409461663953e-06, 'epoch': 2.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 93%|█████████▎| 23200/24920 [21:57:29<1:59:05,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-23200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-23200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032475244253873825, 'eval_runtime': 55.3439, 'eval_samples_per_second': 72.781, 'eval_steps_per_second': 9.107, 'epoch': 2.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-23200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-23200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-23200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-23200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-21200] due to args.save_total_limit\n",
            " 94%|█████████▍| 23400/24920 [22:11:21<1:45:16,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0213, 'learning_rate': 3.0995106035889074e-06, 'epoch': 2.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 94%|█████████▍| 23400/24920 [22:12:17<1:45:16,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-23400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-23400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032613594084978104, 'eval_runtime': 55.5445, 'eval_samples_per_second': 72.518, 'eval_steps_per_second': 9.074, 'epoch': 2.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-23400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-23400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-23400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-23400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-21400] due to args.save_total_limit\n",
            " 95%|█████████▍| 23600/24920 [22:26:09<1:30:43,  4.12s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.021, 'learning_rate': 2.691680261011419e-06, 'epoch': 2.95}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 95%|█████████▍| 23600/24920 [22:27:04<1:30:43,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-23600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-23600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03257031366229057, 'eval_runtime': 55.3641, 'eval_samples_per_second': 72.755, 'eval_steps_per_second': 9.103, 'epoch': 2.95}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-23600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-23600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-23600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-23600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-21600] due to args.save_total_limit\n",
            " 96%|█████████▌| 23800/24920 [22:40:55<1:17:15,  4.14s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0209, 'learning_rate': 2.2838499184339317e-06, 'epoch': 2.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 96%|█████████▌| 23800/24920 [22:41:50<1:17:15,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-23800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-23800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03263464197516441, 'eval_runtime': 55.0953, 'eval_samples_per_second': 73.11, 'eval_steps_per_second': 9.148, 'epoch': 2.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-23800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-23800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-23800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-23800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-21800] due to args.save_total_limit\n",
            " 96%|█████████▋| 24000/24920 [22:55:42<1:03:28,  4.14s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0226, 'learning_rate': 1.8760195758564438e-06, 'epoch': 3.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 96%|█████████▋| 24000/24920 [22:56:37<1:03:28,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-24000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-24000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03244469314813614, 'eval_runtime': 55.0384, 'eval_samples_per_second': 73.185, 'eval_steps_per_second': 9.157, 'epoch': 3.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-24000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-24000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-24000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-24000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-22000] due to args.save_total_limit\n",
            " 97%|█████████▋| 24200/24920 [23:10:29<49:23,  4.12s/it]  ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0191, 'learning_rate': 1.468189233278956e-06, 'epoch': 3.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            " 97%|█████████▋| 24200/24920 [23:11:24<49:23,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-24200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-24200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03270672634243965, 'eval_runtime': 55.2636, 'eval_samples_per_second': 72.887, 'eval_steps_per_second': 9.12, 'epoch': 3.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-24200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-24200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-24200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-24200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-22200] due to args.save_total_limit\n",
            " 98%|█████████▊| 24400/24920 [23:25:19<35:59,  4.15s/it]  ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0178, 'learning_rate': 1.0603588907014684e-06, 'epoch': 3.08}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            " 98%|█████████▊| 24400/24920 [23:26:14<35:59,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-24400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-24400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032966915518045425, 'eval_runtime': 55.065, 'eval_samples_per_second': 73.15, 'eval_steps_per_second': 9.153, 'epoch': 3.08}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-24400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-24400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-24400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-24400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-22400] due to args.save_total_limit\n",
            " 99%|█████████▊| 24600/24920 [23:40:08<22:04,  4.14s/it]  ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0192, 'learning_rate': 6.525285481239805e-07, 'epoch': 3.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            " 99%|█████████▊| 24600/24920 [23:41:03<22:04,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-24600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-24600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033010199666023254, 'eval_runtime': 55.115, 'eval_samples_per_second': 73.084, 'eval_steps_per_second': 9.145, 'epoch': 3.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-24600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-24600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-24600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-24600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-22600] due to args.save_total_limit\n",
            "100%|█████████▉| 24800/24920 [23:54:54<08:18,  4.15s/it]  ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0191, 'learning_rate': 2.4469820554649265e-07, 'epoch': 3.15}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            "100%|█████████▉| 24800/24920 [23:55:50<08:18,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-24800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-24800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033007100224494934, 'eval_runtime': 55.4151, 'eval_samples_per_second': 72.688, 'eval_steps_per_second': 9.095, 'epoch': 3.15}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-24800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-24800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-24800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-24800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-22800] due to args.save_total_limit\n",
            "100%|██████████| 24920/24920 [24:04:11<00:00,  4.15s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 24920/24920 [24:04:11<00:00,  4.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 86652.3247, 'train_samples_per_second': 18.405, 'train_steps_per_second': 0.288, 'train_loss': 0.022168252383916374, 'epoch': 3.17}\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 15 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 15 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-108\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24920/24920 [24:04:11<00:00,  3.48s/it]\n"
          ]
        }
      ],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "TABp6GNzneEa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ../../model/train/ptt5-indir-version/config.json\n",
            "Model weights saved in ../../model/train/ptt5-indir-version/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained('../../model/train/ptt5-indir-version')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=24920, training_loss=0.022168252383916374, metrics={'train_runtime': 86652.3247, 'train_samples_per_second': 18.405, 'train_steps_per_second': 0.288, 'train_loss': 0.022168252383916374, 'epoch': 3.17})"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "bGJOtzX4FhnF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "num_step_alert = 200\n",
        "training_args = Seq2SeqTrainingArguments(output_dir=PATH_TRAIN_MODEL_LOCAL)\n",
        "# Needed to make the Trainer work with an on-the-fly transformation on the dataset\n",
        "# training_args.remove_unused_columns = False\n",
        "training_args.output_dir = PATH_TRAIN_MODEL_LOCAL\n",
        "training_args.warmup_steps=400 # Alterar!\n",
        "training_args.num_train_epochs=6.0 # Alterar!\n",
        "training_args.logging_steps=num_step_alert # Alterar!\n",
        "training_args.save_strategy=\"steps\"\n",
        "training_args.save_steps=num_step_alert\n",
        "training_args.save_total_limit=20\n",
        "training_args.learning_rate=5e-5\n",
        "training_args.per_device_train_batch_size=16 # t4: 8, a100-40: 32\n",
        "training_args.gradient_accumulation_steps=4 # t4: 4, a100-40: 2\n",
        "#training_args._n_gpu = 1\n",
        "# training_args.bf16 = True # se for usar a100, 3090, 4090 -> usar\n",
        "training_args.ignore_data_skip = True\n",
        "training_args.load_best_model_at_end = True\n",
        "training_args.evaluation_strategy='steps'\n",
        "training_args.eval_steps=num_step_alert\n",
        "training_args.do_eval = True\n",
        "# training_args.optim='adamw_hf' #default\n",
        "training_args.gradient_checkpointing = False # True\n",
        "# se precisar economizar gpu\n",
        "# training_args.optim='adamw_bnb_8bit'\n",
        "# training_args.gradient_checkpointing = True\n",
        "training_args.report_to=\"neptune\",\n",
        "# training_args.report_to = 'None'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mMQw6ra63Xk",
        "outputId": "98e651ba-25d0-419c-fe39-143a4db50d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=200,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=True,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../model/train/ptt5-base/runs/Jul07_09-40-07_borela-wks,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=200,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=6.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=../../model/train/ptt5-base,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=('neptune',),\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../model/train/ptt5-base,\n",
            "save_on_each_node=False,\n",
            "save_steps=200,\n",
            "save_strategy=steps,\n",
            "save_total_limit=20,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=400,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(training_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "jqrhQxd3E55t"
      },
      "outputs": [],
      "source": [
        "trainer = trainer_cls(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "khUa05-9e5RC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading model from ../../model/train/ptt5-base/checkpoint-24800.\n",
            "/home/borela/miniconda3/envs/treinapython39/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 398710\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 37380\n",
            "  Number of trainable parameters = 222903552\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 3\n",
            "  Continuing training from global step 24800\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-109\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 25000/37380 [13:44<14:15:30,  4.15s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0182, 'learning_rate': 1.6738777717685236e-05, 'epoch': 3.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            " 67%|██████▋   | 25000/37380 [14:39<14:15:30,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-25000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-25000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03435084596276283, 'eval_runtime': 55.4408, 'eval_samples_per_second': 72.654, 'eval_steps_per_second': 9.091, 'epoch': 3.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-25000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-25000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-25000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-25000/spiece.model\n",
            " 67%|██████▋   | 25200/37380 [45:52<13:46:00,  4.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0187, 'learning_rate': 1.6468361276365603e-05, 'epoch': 3.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            " 67%|██████▋   | 25200/37380 [46:47<13:46:00,  4.07s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-25200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-25200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03584064170718193, 'eval_runtime': 54.993, 'eval_samples_per_second': 73.246, 'eval_steps_per_second': 9.165, 'epoch': 3.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-25200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-25200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-25200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-25200/spiece.model\n",
            " 68%|██████▊   | 25400/37380 [1:18:16<13:55:26,  4.18s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0189, 'learning_rate': 1.619794483504597e-05, 'epoch': 3.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 68%|██████▊   | 25400/37380 [1:19:11<13:55:26,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-25400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-25400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03574325889348984, 'eval_runtime': 55.056, 'eval_samples_per_second': 73.162, 'eval_steps_per_second': 9.154, 'epoch': 3.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-25400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-25400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-25400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-25400/spiece.model\n",
            " 68%|██████▊   | 25600/37380 [1:33:05<13:34:23,  4.15s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0182, 'learning_rate': 1.592752839372634e-05, 'epoch': 3.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 68%|██████▊   | 25600/37380 [1:34:00<13:34:23,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-25600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-25600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0346958227455616, 'eval_runtime': 55.2803, 'eval_samples_per_second': 72.865, 'eval_steps_per_second': 9.117, 'epoch': 3.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-25600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-25600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-25600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-25600/spiece.model\n",
            " 69%|██████▉   | 25800/37380 [1:47:58<13:09:18,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0216, 'learning_rate': 1.565711195240671e-05, 'epoch': 3.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 69%|██████▉   | 25800/37380 [1:48:52<13:09:18,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-25800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-25800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03292287513613701, 'eval_runtime': 54.461, 'eval_samples_per_second': 73.961, 'eval_steps_per_second': 9.254, 'epoch': 3.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-25800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-25800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-25800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-25800/spiece.model\n",
            " 70%|██████▉   | 26000/37380 [2:02:38<13:04:46,  4.14s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0225, 'learning_rate': 1.5386695511087072e-05, 'epoch': 3.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 70%|██████▉   | 26000/37380 [2:03:33<13:04:46,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-26000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-26000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03302432596683502, 'eval_runtime': 54.8012, 'eval_samples_per_second': 73.502, 'eval_steps_per_second': 9.197, 'epoch': 3.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-26000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-26000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-26000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-26000/spiece.model\n",
            " 70%|███████   | 26200/37380 [2:17:16<12:42:46,  4.09s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.019, 'learning_rate': 1.5116279069767441e-05, 'epoch': 3.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 70%|███████   | 26200/37380 [2:18:11<12:42:46,  4.09s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-26200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-26200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034505702555179596, 'eval_runtime': 54.7859, 'eval_samples_per_second': 73.523, 'eval_steps_per_second': 9.199, 'epoch': 3.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-26200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-26200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-26200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-26200/spiece.model\n",
            " 71%|███████   | 26400/37380 [2:32:00<12:47:09,  4.19s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0221, 'learning_rate': 1.484586262844781e-05, 'epoch': 3.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 71%|███████   | 26400/37380 [2:32:55<12:47:09,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-26400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-26400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034426845610141754, 'eval_runtime': 55.5634, 'eval_samples_per_second': 72.494, 'eval_steps_per_second': 9.071, 'epoch': 3.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-26400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-26400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-26400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-26400/spiece.model\n",
            " 71%|███████   | 26600/37380 [2:46:55<12:31:05,  4.18s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0208, 'learning_rate': 1.4575446187128177e-05, 'epoch': 3.29}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 71%|███████   | 26600/37380 [2:47:50<12:31:05,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-26600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-26600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034394122660160065, 'eval_runtime': 55.3994, 'eval_samples_per_second': 72.708, 'eval_steps_per_second': 9.098, 'epoch': 3.29}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-26600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-26600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-26600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-26600/spiece.model\n",
            " 72%|███████▏  | 26800/37380 [3:01:49<12:15:05,  4.17s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.02, 'learning_rate': 1.4305029745808546e-05, 'epoch': 3.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 72%|███████▏  | 26800/37380 [3:02:45<12:15:05,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-26800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-26800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0343005396425724, 'eval_runtime': 55.5902, 'eval_samples_per_second': 72.459, 'eval_steps_per_second': 9.066, 'epoch': 3.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-26800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-26800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-26800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-26800/spiece.model\n",
            " 72%|███████▏  | 27000/37380 [3:16:44<12:03:10,  4.18s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0212, 'learning_rate': 1.4034613304488914e-05, 'epoch': 3.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 72%|███████▏  | 27000/37380 [3:17:39<12:03:10,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-27000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-27000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03411152958869934, 'eval_runtime': 55.5097, 'eval_samples_per_second': 72.564, 'eval_steps_per_second': 9.079, 'epoch': 3.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-27000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-27000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-27000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-27000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-23000] due to args.save_total_limit\n",
            " 73%|███████▎  | 27200/37380 [3:31:39<11:59:35,  4.24s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0185, 'learning_rate': 1.3764196863169283e-05, 'epoch': 3.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 73%|███████▎  | 27200/37380 [3:32:35<11:59:35,  4.24s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-27200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-27200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03429172933101654, 'eval_runtime': 55.2816, 'eval_samples_per_second': 72.863, 'eval_steps_per_second': 9.117, 'epoch': 3.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-27200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-27200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-27200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-27200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-23200] due to args.save_total_limit\n",
            " 73%|███████▎  | 27400/37380 [3:46:33<11:31:44,  4.16s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0211, 'learning_rate': 1.349378042184965e-05, 'epoch': 3.42}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 73%|███████▎  | 27400/37380 [3:47:29<11:31:44,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-27400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-27400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0341457761824131, 'eval_runtime': 55.3048, 'eval_samples_per_second': 72.833, 'eval_steps_per_second': 9.113, 'epoch': 3.42}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-27400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-27400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-27400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-27400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-23400] due to args.save_total_limit\n",
            " 74%|███████▍  | 27600/37380 [4:01:27<11:20:32,  4.18s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0208, 'learning_rate': 1.3223363980530019e-05, 'epoch': 3.45}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 74%|███████▍  | 27600/37380 [4:02:22<11:20:32,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-27600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-27600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033053621649742126, 'eval_runtime': 55.5817, 'eval_samples_per_second': 72.47, 'eval_steps_per_second': 9.068, 'epoch': 3.45}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-27600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-27600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-27600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-27600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-23600] due to args.save_total_limit\n",
            " 74%|███████▍  | 27800/37380 [4:16:21<11:05:39,  4.17s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0195, 'learning_rate': 1.2952947539210383e-05, 'epoch': 3.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 74%|███████▍  | 27800/37380 [4:17:16<11:05:39,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-27800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-27800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033555351197719574, 'eval_runtime': 55.3398, 'eval_samples_per_second': 72.787, 'eval_steps_per_second': 9.107, 'epoch': 3.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-27800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-27800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-27800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-27800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-23800] due to args.save_total_limit\n",
            " 75%|███████▍  | 28000/37380 [4:31:14<10:53:19,  4.18s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0196, 'learning_rate': 1.2682531097890751e-05, 'epoch': 3.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 75%|███████▍  | 28000/37380 [4:32:09<10:53:19,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-28000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-28000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03426952287554741, 'eval_runtime': 55.2655, 'eval_samples_per_second': 72.885, 'eval_steps_per_second': 9.12, 'epoch': 3.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-28000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-28000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-28000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-28000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-24000] due to args.save_total_limit\n",
            " 75%|███████▌  | 28200/37380 [4:46:10<10:47:58,  4.24s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0205, 'learning_rate': 1.241211465657112e-05, 'epoch': 3.55}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 75%|███████▌  | 28200/37380 [4:47:06<10:47:58,  4.24s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-28200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-28200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03372732177376747, 'eval_runtime': 55.4543, 'eval_samples_per_second': 72.636, 'eval_steps_per_second': 9.089, 'epoch': 3.55}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-28200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-28200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-28200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-28200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-24200] due to args.save_total_limit\n",
            " 76%|███████▌  | 28400/37380 [5:01:04<10:25:33,  4.18s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0207, 'learning_rate': 1.2141698215251488e-05, 'epoch': 3.58}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 76%|███████▌  | 28400/37380 [5:01:59<10:25:33,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-28400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-28400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03371589258313179, 'eval_runtime': 55.2955, 'eval_samples_per_second': 72.845, 'eval_steps_per_second': 9.115, 'epoch': 3.58}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-28400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-28400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-28400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-28400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-24400] due to args.save_total_limit\n",
            " 77%|███████▋  | 28600/37380 [5:15:58<10:12:48,  4.19s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0195, 'learning_rate': 1.1871281773931855e-05, 'epoch': 3.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 77%|███████▋  | 28600/37380 [5:16:53<10:12:48,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-28600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-28600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03311342000961304, 'eval_runtime': 55.415, 'eval_samples_per_second': 72.688, 'eval_steps_per_second': 9.095, 'epoch': 3.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-28600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-28600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-28600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-28600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-24600] due to args.save_total_limit\n",
            " 77%|███████▋  | 28800/37380 [5:30:52<9:59:55,  4.20s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0199, 'learning_rate': 1.1600865332612224e-05, 'epoch': 3.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 77%|███████▋  | 28800/37380 [5:31:47<9:59:55,  4.20s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-28800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-28800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032547686249017715, 'eval_runtime': 55.4309, 'eval_samples_per_second': 72.667, 'eval_steps_per_second': 9.092, 'epoch': 3.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-28800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-28800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-28800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-28800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-24800] due to args.save_total_limit\n",
            " 78%|███████▊  | 29000/37380 [5:45:48<9:45:45,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0196, 'learning_rate': 1.1330448891292591e-05, 'epoch': 3.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 78%|███████▊  | 29000/37380 [5:46:43<9:45:45,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-29000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-29000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03298616781830788, 'eval_runtime': 55.4767, 'eval_samples_per_second': 72.607, 'eval_steps_per_second': 9.085, 'epoch': 3.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-29000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-29000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-29000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-29000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-25000] due to args.save_total_limit\n",
            " 78%|███████▊  | 29200/37380 [6:00:43<9:27:26,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0214, 'learning_rate': 1.1060032449972958e-05, 'epoch': 3.71}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 78%|███████▊  | 29200/37380 [6:01:38<9:27:26,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-29200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-29200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03255263715982437, 'eval_runtime': 55.3995, 'eval_samples_per_second': 72.708, 'eval_steps_per_second': 9.098, 'epoch': 3.71}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-29200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-29200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-29200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-29200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-25200] due to args.save_total_limit\n",
            " 79%|███████▊  | 29400/37380 [6:15:36<9:18:09,  4.20s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0201, 'learning_rate': 1.0789616008653325e-05, 'epoch': 3.74}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 79%|███████▊  | 29400/37380 [6:16:31<9:18:09,  4.20s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-29400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-29400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032849110662937164, 'eval_runtime': 55.506, 'eval_samples_per_second': 72.569, 'eval_steps_per_second': 9.08, 'epoch': 3.74}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-29400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-29400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-29400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-29400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-25400] due to args.save_total_limit\n",
            " 79%|███████▉  | 29600/37380 [6:30:33<9:03:23,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0196, 'learning_rate': 1.0519199567333694e-05, 'epoch': 3.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 79%|███████▉  | 29600/37380 [6:31:29<9:03:23,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-29600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-29600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033511437475681305, 'eval_runtime': 55.4356, 'eval_samples_per_second': 72.661, 'eval_steps_per_second': 9.092, 'epoch': 3.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-29600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-29600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-29600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-29600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-25600] due to args.save_total_limit\n",
            " 80%|███████▉  | 29800/37380 [6:45:26<8:47:55,  4.18s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0205, 'learning_rate': 1.0248783126014062e-05, 'epoch': 3.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 80%|███████▉  | 29800/37380 [6:46:22<8:47:55,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-29800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-29800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033625781536102295, 'eval_runtime': 55.5439, 'eval_samples_per_second': 72.519, 'eval_steps_per_second': 9.074, 'epoch': 3.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-29800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-29800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-29800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-29800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-25800] due to args.save_total_limit\n",
            " 80%|████████  | 30000/37380 [7:00:21<8:29:38,  4.14s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0201, 'learning_rate': 9.97836668469443e-06, 'epoch': 3.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 80%|████████  | 30000/37380 [7:01:16<8:29:38,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-30000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-30000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03320852667093277, 'eval_runtime': 55.3709, 'eval_samples_per_second': 72.746, 'eval_steps_per_second': 9.102, 'epoch': 3.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-30000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-30000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-30000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-30000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-26000] due to args.save_total_limit\n",
            " 81%|████████  | 30200/37380 [7:15:15<8:18:25,  4.17s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0221, 'learning_rate': 9.707950243374798e-06, 'epoch': 3.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 81%|████████  | 30200/37380 [7:16:10<8:18:25,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-30200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-30200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.032312966883182526, 'eval_runtime': 55.409, 'eval_samples_per_second': 72.696, 'eval_steps_per_second': 9.096, 'epoch': 3.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-30200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-30200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-30200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-30200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-26200] due to args.save_total_limit\n",
            " 81%|████████▏ | 30400/37380 [7:30:10<8:09:54,  4.21s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0202, 'learning_rate': 9.437533802055165e-06, 'epoch': 3.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 81%|████████▏ | 30400/37380 [7:31:05<8:09:54,  4.21s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-30400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-30400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03236682340502739, 'eval_runtime': 55.4142, 'eval_samples_per_second': 72.689, 'eval_steps_per_second': 9.095, 'epoch': 3.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-30400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-30400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-30400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-30400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-26400] due to args.save_total_limit\n",
            " 82%|████████▏ | 30600/37380 [7:45:05<7:53:38,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0207, 'learning_rate': 9.167117360735534e-06, 'epoch': 3.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 82%|████████▏ | 30600/37380 [7:46:00<7:53:38,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-30600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-30600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03250203654170036, 'eval_runtime': 55.3833, 'eval_samples_per_second': 72.73, 'eval_steps_per_second': 9.1, 'epoch': 3.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-30600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-30600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-30600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-30600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-26600] due to args.save_total_limit\n",
            " 82%|████████▏ | 30800/37380 [8:00:04<7:47:41,  4.26s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0209, 'learning_rate': 8.896700919415901e-06, 'epoch': 3.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 82%|████████▏ | 30800/37380 [8:00:59<7:47:41,  4.26s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-30800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-30800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03224475309252739, 'eval_runtime': 55.683, 'eval_samples_per_second': 72.338, 'eval_steps_per_second': 9.051, 'epoch': 3.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-30800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-30800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-30800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-30800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-26800] due to args.save_total_limit\n",
            " 83%|████████▎ | 31000/37380 [8:15:00<7:17:01,  4.11s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0187, 'learning_rate': 8.62628447809627e-06, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 83%|████████▎ | 31000/37380 [8:15:56<7:17:01,  4.11s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-31000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-31000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033378809690475464, 'eval_runtime': 55.3686, 'eval_samples_per_second': 72.749, 'eval_steps_per_second': 9.103, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-31000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-31000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-31000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-31000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-27000] due to args.save_total_limit\n",
            " 83%|████████▎ | 31200/37380 [8:29:42<7:00:31,  4.08s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0162, 'learning_rate': 8.355868036776636e-06, 'epoch': 4.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 83%|████████▎ | 31200/37380 [8:30:36<7:00:31,  4.08s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-31200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-31200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03468750789761543, 'eval_runtime': 54.4594, 'eval_samples_per_second': 73.963, 'eval_steps_per_second': 9.255, 'epoch': 4.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-31200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-31200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-31200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-31200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-27200] due to args.save_total_limit\n",
            " 84%|████████▍ | 31400/37380 [8:47:12<6:48:56,  4.10s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0165, 'learning_rate': 8.085451595457005e-06, 'epoch': 4.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n",
            "                                                         \n",
            " 84%|████████▍ | 31400/37380 [8:48:08<6:48:56,  4.10s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-31400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-31400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035022228956222534, 'eval_runtime': 55.3112, 'eval_samples_per_second': 72.824, 'eval_steps_per_second': 9.112, 'epoch': 4.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-31400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-31400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-31400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-31400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-27400] due to args.save_total_limit\n",
            " 85%|████████▍ | 31600/37380 [9:04:12<6:40:59,  4.16s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0167, 'learning_rate': 7.815035154137372e-06, 'epoch': 4.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n",
            "                                                         \n",
            " 85%|████████▍ | 31600/37380 [9:05:08<6:40:59,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-31600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-31600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03544702008366585, 'eval_runtime': 55.0779, 'eval_samples_per_second': 73.133, 'eval_steps_per_second': 9.151, 'epoch': 4.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-31600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-31600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-31600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-31600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-27600] due to args.save_total_limit\n",
            " 85%|████████▌ | 31800/37380 [9:19:06<6:25:31,  4.15s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0157, 'learning_rate': 7.54461871281774e-06, 'epoch': 4.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 85%|████████▌ | 31800/37380 [9:20:02<6:25:31,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-31800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-31800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03496887907385826, 'eval_runtime': 55.417, 'eval_samples_per_second': 72.685, 'eval_steps_per_second': 9.095, 'epoch': 4.12}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-31800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-31800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-31800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-31800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-27800] due to args.save_total_limit\n",
            " 86%|████████▌ | 32000/37380 [9:33:57<6:12:21,  4.15s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0165, 'learning_rate': 7.274202271498108e-06, 'epoch': 4.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 86%|████████▌ | 32000/37380 [9:34:53<6:12:21,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-32000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-32000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03463713452219963, 'eval_runtime': 55.1023, 'eval_samples_per_second': 73.1, 'eval_steps_per_second': 9.147, 'epoch': 4.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-32000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-32000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-32000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-32000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-28000] due to args.save_total_limit\n",
            " 86%|████████▌ | 32200/37380 [9:48:51<6:01:21,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0169, 'learning_rate': 7.003785830178475e-06, 'epoch': 4.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \n",
            " 86%|████████▌ | 32200/37380 [9:49:46<6:01:21,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-32200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-32200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03459534794092178, 'eval_runtime': 55.415, 'eval_samples_per_second': 72.688, 'eval_steps_per_second': 9.095, 'epoch': 4.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-32200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-32200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-32200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-32200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-28200] due to args.save_total_limit\n",
            " 87%|████████▋ | 32400/37380 [10:03:44<5:46:01,  4.17s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.016, 'learning_rate': 6.733369388858843e-06, 'epoch': 4.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 87%|████████▋ | 32400/37380 [10:04:39<5:46:01,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-32400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-32400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035855039954185486, 'eval_runtime': 55.3782, 'eval_samples_per_second': 72.736, 'eval_steps_per_second': 9.101, 'epoch': 4.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-32400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-32400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-32400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-32400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-28400] due to args.save_total_limit\n",
            " 87%|████████▋ | 32600/37380 [10:18:35<5:31:12,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0167, 'learning_rate': 6.4629529475392105e-06, 'epoch': 4.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 87%|████████▋ | 32600/37380 [10:19:31<5:31:12,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-32600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-32600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.035321012139320374, 'eval_runtime': 55.3158, 'eval_samples_per_second': 72.818, 'eval_steps_per_second': 9.111, 'epoch': 4.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-32600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-32600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-32600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-32600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-28600] due to args.save_total_limit\n",
            " 88%|████████▊ | 32800/37380 [10:33:28<5:19:12,  4.18s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0147, 'learning_rate': 6.1925365062195785e-06, 'epoch': 4.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 88%|████████▊ | 32800/37380 [10:34:23<5:19:12,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-32800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-32800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03554528206586838, 'eval_runtime': 55.3452, 'eval_samples_per_second': 72.78, 'eval_steps_per_second': 9.106, 'epoch': 4.28}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-32800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-32800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-32800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-32800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-28800] due to args.save_total_limit\n",
            " 88%|████████▊ | 33000/37380 [10:48:18<5:03:08,  4.15s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0181, 'learning_rate': 5.922120064899947e-06, 'epoch': 4.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 88%|████████▊ | 33000/37380 [10:49:14<5:03:08,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-33000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-33000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0350835956633091, 'eval_runtime': 55.2393, 'eval_samples_per_second': 72.919, 'eval_steps_per_second': 9.124, 'epoch': 4.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-33000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-33000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-33000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-33000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-29000] due to args.save_total_limit\n",
            " 89%|████████▉ | 33200/37380 [11:03:12<4:49:35,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0158, 'learning_rate': 5.651703623580314e-06, 'epoch': 4.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 89%|████████▉ | 33200/37380 [11:04:07<4:49:35,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-33200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-33200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034998662769794464, 'eval_runtime': 55.3424, 'eval_samples_per_second': 72.783, 'eval_steps_per_second': 9.107, 'epoch': 4.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-33200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-33200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-33200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-33200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-29200] due to args.save_total_limit\n",
            " 89%|████████▉ | 33400/37380 [11:18:02<4:39:02,  4.21s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0178, 'learning_rate': 5.381287182260682e-06, 'epoch': 4.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 89%|████████▉ | 33400/37380 [11:18:57<4:39:02,  4.21s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-33400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-33400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03397810459136963, 'eval_runtime': 55.5399, 'eval_samples_per_second': 72.524, 'eval_steps_per_second': 9.075, 'epoch': 4.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-33400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-33400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-33400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-33400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-29400] due to args.save_total_limit\n",
            " 90%|████████▉ | 33600/37380 [11:32:50<4:22:51,  4.17s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0175, 'learning_rate': 5.11087074094105e-06, 'epoch': 4.41}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 90%|████████▉ | 33600/37380 [11:33:45<4:22:51,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-33600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-33600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03373459726572037, 'eval_runtime': 55.1911, 'eval_samples_per_second': 72.983, 'eval_steps_per_second': 9.132, 'epoch': 4.41}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-33600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-33600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-33600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-33600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-29600] due to args.save_total_limit\n",
            " 90%|█████████ | 33800/37380 [11:47:41<4:06:06,  4.12s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0175, 'learning_rate': 4.840454299621417e-06, 'epoch': 4.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 90%|█████████ | 33800/37380 [11:48:37<4:06:06,  4.12s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-33800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-33800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03356345742940903, 'eval_runtime': 55.163, 'eval_samples_per_second': 73.02, 'eval_steps_per_second': 9.137, 'epoch': 4.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-33800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-33800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-33800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-33800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-29800] due to args.save_total_limit\n",
            " 91%|█████████ | 34000/37380 [12:02:33<3:56:01,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0152, 'learning_rate': 4.570037858301785e-06, 'epoch': 4.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 91%|█████████ | 34000/37380 [12:03:28<3:56:01,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-34000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-34000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03426697477698326, 'eval_runtime': 55.1776, 'eval_samples_per_second': 73.001, 'eval_steps_per_second': 9.134, 'epoch': 4.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-34000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-34000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-34000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-34000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-30000] due to args.save_total_limit\n",
            " 91%|█████████▏| 34200/37380 [12:17:26<3:40:49,  4.17s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0157, 'learning_rate': 4.2996214169821526e-06, 'epoch': 4.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 91%|█████████▏| 34200/37380 [12:18:21<3:40:49,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-34200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-34200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034476831555366516, 'eval_runtime': 55.094, 'eval_samples_per_second': 73.111, 'eval_steps_per_second': 9.148, 'epoch': 4.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-34200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-34200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-34200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-34200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-30200] due to args.save_total_limit\n",
            " 92%|█████████▏| 34400/37380 [12:32:18<3:25:26,  4.14s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0183, 'learning_rate': 4.029204975662521e-06, 'epoch': 4.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 92%|█████████▏| 34400/37380 [12:33:14<3:25:26,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-34400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-34400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03425212576985359, 'eval_runtime': 55.5392, 'eval_samples_per_second': 72.525, 'eval_steps_per_second': 9.075, 'epoch': 4.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-34400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-34400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-34400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-34400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-30400] due to args.save_total_limit\n",
            " 93%|█████████▎| 34600/37380 [12:47:09<3:12:38,  4.16s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0151, 'learning_rate': 3.758788534342888e-06, 'epoch': 4.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 93%|█████████▎| 34600/37380 [12:48:05<3:12:38,  4.16s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-34600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-34600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03455273061990738, 'eval_runtime': 55.3052, 'eval_samples_per_second': 72.832, 'eval_steps_per_second': 9.113, 'epoch': 4.57}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-34600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-34600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-34600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-34600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-30600] due to args.save_total_limit\n",
            " 93%|█████████▎| 34800/37380 [13:01:59<2:58:17,  4.15s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0158, 'learning_rate': 3.488372093023256e-06, 'epoch': 4.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 93%|█████████▎| 34800/37380 [13:02:54<2:58:17,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-34800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-34800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03455011174082756, 'eval_runtime': 55.5137, 'eval_samples_per_second': 72.559, 'eval_steps_per_second': 9.079, 'epoch': 4.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-34800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-34800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-34800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-34800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-30800] due to args.save_total_limit\n",
            " 94%|█████████▎| 35000/37380 [13:16:46<2:44:15,  4.14s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0184, 'learning_rate': 3.217955651703624e-06, 'epoch': 4.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 94%|█████████▎| 35000/37380 [13:17:42<2:44:15,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-35000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-35000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03427380695939064, 'eval_runtime': 55.5387, 'eval_samples_per_second': 72.526, 'eval_steps_per_second': 9.075, 'epoch': 4.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-35000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-35000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-35000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-35000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-31000] due to args.save_total_limit\n",
            " 94%|█████████▍| 35200/37380 [13:31:38<2:30:40,  4.15s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0153, 'learning_rate': 2.9475392103839913e-06, 'epoch': 4.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 94%|█████████▍| 35200/37380 [13:32:33<2:30:40,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-35200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-35200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.034559741616249084, 'eval_runtime': 55.4916, 'eval_samples_per_second': 72.588, 'eval_steps_per_second': 9.082, 'epoch': 4.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-35200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-35200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-35200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-35200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-31200] due to args.save_total_limit\n",
            " 95%|█████████▍| 35400/37380 [13:46:31<2:18:10,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0201, 'learning_rate': 2.6771227690643593e-06, 'epoch': 4.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 95%|█████████▍| 35400/37380 [13:47:26<2:18:10,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-35400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-35400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033719126135110855, 'eval_runtime': 55.1741, 'eval_samples_per_second': 73.005, 'eval_steps_per_second': 9.135, 'epoch': 4.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-35400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-35400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-35400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-35400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-31400] due to args.save_total_limit\n",
            " 95%|█████████▌| 35600/37380 [14:01:19<2:02:25,  4.13s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0172, 'learning_rate': 2.406706327744727e-06, 'epoch': 4.73}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 95%|█████████▌| 35600/37380 [14:02:14<2:02:25,  4.13s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-35600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-35600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03357253968715668, 'eval_runtime': 55.3597, 'eval_samples_per_second': 72.76, 'eval_steps_per_second': 9.104, 'epoch': 4.73}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-35600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-35600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-35600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-35600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-31600] due to args.save_total_limit\n",
            " 96%|█████████▌| 35800/37380 [14:16:09<1:50:17,  4.19s/it] ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0198, 'learning_rate': 2.1362898864250946e-06, 'epoch': 4.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 96%|█████████▌| 35800/37380 [14:17:04<1:50:17,  4.19s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-35800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-35800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033345792442560196, 'eval_runtime': 55.3595, 'eval_samples_per_second': 72.761, 'eval_steps_per_second': 9.104, 'epoch': 4.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-35800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-35800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-35800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-35800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-31800] due to args.save_total_limit\n",
            " 96%|█████████▋| 36000/37380 [14:31:01<1:35:31,  4.15s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0166, 'learning_rate': 1.8658734451054625e-06, 'epoch': 4.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 96%|█████████▋| 36000/37380 [14:31:57<1:35:31,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-36000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-36000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0335436575114727, 'eval_runtime': 55.3112, 'eval_samples_per_second': 72.824, 'eval_steps_per_second': 9.112, 'epoch': 4.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-36000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-36000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-36000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-36000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-32000] due to args.save_total_limit\n",
            " 97%|█████████▋| 36200/37380 [14:45:52<1:22:33,  4.20s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0163, 'learning_rate': 1.5954570037858302e-06, 'epoch': 4.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 97%|█████████▋| 36200/37380 [14:46:47<1:22:33,  4.20s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-36200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-36200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03342711925506592, 'eval_runtime': 55.442, 'eval_samples_per_second': 72.653, 'eval_steps_per_second': 9.091, 'epoch': 4.83}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-36200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-36200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-36200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-36200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-32200] due to args.save_total_limit\n",
            " 97%|█████████▋| 36400/37380 [15:00:41<1:08:07,  4.17s/it]***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0158, 'learning_rate': 1.325040562466198e-06, 'epoch': 4.86}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \n",
            " 97%|█████████▋| 36400/37380 [15:01:36<1:08:07,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-36400\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-36400/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03371485322713852, 'eval_runtime': 55.2507, 'eval_samples_per_second': 72.904, 'eval_steps_per_second': 9.122, 'epoch': 4.86}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-36400/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-36400/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-36400/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-36400/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-32400] due to args.save_total_limit\n",
            " 98%|█████████▊| 36600/37380 [15:15:30<53:50,  4.14s/it]  ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0187, 'learning_rate': 1.0546241211465657e-06, 'epoch': 4.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            " 98%|█████████▊| 36600/37380 [15:16:25<53:50,  4.14s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-36600\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-36600/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03363970294594765, 'eval_runtime': 55.2924, 'eval_samples_per_second': 72.849, 'eval_steps_per_second': 9.115, 'epoch': 4.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-36600/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-36600/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-36600/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-36600/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-32600] due to args.save_total_limit\n",
            " 98%|█████████▊| 36800/37380 [15:30:22<40:23,  4.18s/it]  ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0155, 'learning_rate': 7.842076798269334e-07, 'epoch': 4.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            " 98%|█████████▊| 36800/37380 [15:31:17<40:23,  4.18s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-36800\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-36800/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033717378973960876, 'eval_runtime': 55.2173, 'eval_samples_per_second': 72.948, 'eval_steps_per_second': 9.128, 'epoch': 4.93}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-36800/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-36800/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-36800/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-36800/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-32800] due to args.save_total_limit\n",
            " 99%|█████████▉| 37000/37380 [15:45:11<26:23,  4.17s/it]  ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0187, 'learning_rate': 5.137912385073012e-07, 'epoch': 4.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            " 99%|█████████▉| 37000/37380 [15:46:07<26:23,  4.17s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-37000\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-37000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.033612579107284546, 'eval_runtime': 55.3379, 'eval_samples_per_second': 72.789, 'eval_steps_per_second': 9.108, 'epoch': 4.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-37000/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-37000/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-37000/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-37000/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-33000] due to args.save_total_limit\n",
            "100%|█████████▉| 37200/37380 [16:00:02<12:26,  4.15s/it]  ***** Running Evaluation *****\n",
            "  Num examples = 4028\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0179, 'learning_rate': 2.43374797187669e-07, 'epoch': 4.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \n",
            "100%|█████████▉| 37200/37380 [16:00:57<12:26,  4.15s/it]Saving model checkpoint to ../../model/train/ptt5-base/checkpoint-37200\n",
            "Configuration saved in ../../model/train/ptt5-base/checkpoint-37200/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03358309343457222, 'eval_runtime': 55.645, 'eval_samples_per_second': 72.387, 'eval_steps_per_second': 9.057, 'epoch': 4.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ../../model/train/ptt5-base/checkpoint-37200/pytorch_model.bin\n",
            "tokenizer config file saved in ../../model/train/ptt5-base/checkpoint-37200/tokenizer_config.json\n",
            "Special tokens file saved in ../../model/train/ptt5-base/checkpoint-37200/special_tokens_map.json\n",
            "Copy vocab file to ../../model/train/ptt5-base/checkpoint-37200/spiece.model\n",
            "Deleting older checkpoint [../../model/train/ptt5-base/checkpoint-33200] due to args.save_total_limit\n",
            "100%|██████████| 37380/37380 [16:13:27<00:00,  4.19s/it]  \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 37380/37380 [16:13:27<00:00,  4.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 58408.934, 'train_samples_per_second': 40.957, 'train_steps_per_second': 0.64, 'train_loss': 0.006224599051692385, 'epoch': 5.02}\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 15 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 15 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/marcusborela/IA386DD/e/IAD-109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 37380/37380 [16:13:27<00:00,  1.56s/it]\n"
          ]
        }
      ],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enw1dGpIe5KO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "TABp6GNzneEa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ../../model/train/ptt5-indir-version/indir-600-pcte/config.json\n",
            "Model weights saved in ../../model/train/ptt5-indir-version/indir-600-pcte/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained('../../model/train/ptt5-indir-version/indir-600-pcte')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=37380, training_loss=0.006224599051692385, metrics={'train_runtime': 58408.934, 'train_samples_per_second': 40.957, 'train_steps_per_second': 0.64, 'train_loss': 0.006224599051692385, 'epoch': 5.02})"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jKfDvjb3BcU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCkjjXJq-aM-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CTpjx8V-aGb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcjNusQs3BZP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1qE7f2L3BVn"
      },
      "outputs": [],
      "source": [
        "EXECUÇÕES ANTERIORES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAP47M6XSVW-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhejNQYISWKJ"
      },
      "outputs": [],
      "source": [
        "print(train_metrics )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC0aZ7jrSWG3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMFVAAj7SWDH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtHXA95tSWAF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTa9O1SMSWra"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc-0ySvYFyxe"
      },
      "source": [
        "Não sei se a mudança do batch size (32x2) 64 para 32 (8x4) impactou passar por dados duas vezes!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU3xzNdiETy4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ytqhIl_ULr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av7Mc8VxETvW"
      },
      "outputs": [],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuooOEKxETr8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oumFThfiETpG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ_RROtmETl0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnK4bRPMETjn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsl2RdzcETf8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ3J3DfrFHj5"
      },
      "source": [
        "Abaixo execucao lim50: a100/40gb (erro mount drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L10lhQsHkiHl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_metrics = trainer.train(resume_from_checkpoint=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkNe4_3okiEd"
      },
      "outputs": [],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX_4MBFFkiBq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DDvR-7xS_7_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_metrics = trainer.train(resume_from_checkpoint=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1fz-U5b8bp3"
      },
      "outputs": [],
      "source": [
        "train_metrics = trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tLtOiY_cvBs"
      },
      "outputs": [],
      "source": [
        "train_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuCwnEea9Rxp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvTA3D2pc2w7"
      },
      "outputs": [],
      "source": [
        "huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk415BxBfguC"
      },
      "outputs": [],
      "source": [
        "pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEh5XDoLgX7I"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EucZwwegcGe"
      },
      "outputs": [],
      "source": [
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR3TjrK7gg8x"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(\n",
        "    model_id=\"ptt5-base-pt-msmarco-100k-v2-indir-lim100\",\n",
        "    repo_name=\"marcusborela\"\n",
        "    # use_auth_token=\"\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "e431eb1d856c426fade2a694f8536bd46c4e9c4bd47cb4afd3fb4d2c61122b03"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d1db4f6d75944ddbe4284da885e8038": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36a988374e504d9a997e8e48955d6bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492c664c2bf84d98bd5372d20ed8f8d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "4b258b1461cb4ebd95674bbe963da89b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d153ac235b643d9b9fb1f502f220845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e5ea796f1384a3984a8f52720d3c445": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc2c9da3baaa4d2b92aaf7a4ab74035e",
            "placeholder": "​",
            "style": "IPY_MODEL_b849d25c533f4618afa11c027f3feaad",
            "value": "Tokenizing: 100%"
          }
        },
        "4e6e72fe001744c4830d22150b8e2b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "520b60a897d342648199509590bcb3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee3f86de98ba49fda4fe387569bd85d8",
            "placeholder": "​",
            "style": "IPY_MODEL_4e6e72fe001744c4830d22150b8e2b84",
            "value": " 400724/400724 [06:46&lt;00:00, 880.84 examples/s]"
          }
        },
        "5c546b9f682444be932b17b2ff4c3409": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ce6ab38f4d4400cb7ba03d59d7a7b21",
            "max": 400724,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d1db4f6d75944ddbe4284da885e8038",
            "value": 400724
          }
        },
        "614aa774c3b44e02b377967e4ceea596": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad3f253125f410bbedb811f3313b312": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614aa774c3b44e02b377967e4ceea596",
            "max": 2014,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc8cb0b3f3ba4174bd7f4e3d5a6f8f46",
            "value": 2014
          }
        },
        "6ce6ab38f4d4400cb7ba03d59d7a7b21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dec4e3e109040308c245ca52fc29e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e5ea796f1384a3984a8f52720d3c445",
              "IPY_MODEL_5c546b9f682444be932b17b2ff4c3409",
              "IPY_MODEL_520b60a897d342648199509590bcb3c7"
            ],
            "layout": "IPY_MODEL_f50b231ed5fa423e81400f13d72f77bf"
          }
        },
        "80f9f8c628fc4eeeaead5bf16c880b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36a988374e504d9a997e8e48955d6bd2",
            "placeholder": "​",
            "style": "IPY_MODEL_be972ca97d034c5fb5b616299d8dd29c",
            "value": " 2000/2014 [00:01&lt;00:00, 1120.74 examples/s]"
          }
        },
        "8363e87c191c4e27a55624d76592522e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ddd0ce214044ff4985779bc45a78075",
              "IPY_MODEL_6ad3f253125f410bbedb811f3313b312",
              "IPY_MODEL_80f9f8c628fc4eeeaead5bf16c880b94"
            ],
            "layout": "IPY_MODEL_492c664c2bf84d98bd5372d20ed8f8d2"
          }
        },
        "9ddd0ce214044ff4985779bc45a78075": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b258b1461cb4ebd95674bbe963da89b",
            "placeholder": "​",
            "style": "IPY_MODEL_4d153ac235b643d9b9fb1f502f220845",
            "value": "Tokenizing:  99%"
          }
        },
        "b849d25c533f4618afa11c027f3feaad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc2c9da3baaa4d2b92aaf7a4ab74035e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be972ca97d034c5fb5b616299d8dd29c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc8cb0b3f3ba4174bd7f4e3d5a6f8f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee3f86de98ba49fda4fe387569bd85d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f50b231ed5fa423e81400f13d72f77bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
